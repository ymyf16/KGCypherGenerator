## ðŸ’¡BEACON: Benchmarking-optimizing, strongly-typed Evolutionary Algorithm-based Cypher Generator
### Overview
This repository aims at the following:
- Automatic random cypher query generator that is always syntactically correct, with strict and precise sorting of node and relationship types, and
- EA-based benchmarking cypher query questions generation, which will be useful for standardizing the benchmarking process for AI knowledge retrieval from the general Knowledge Graph based databases.

The project is still under active development on optimizing the evolutionary algorithm. 

### Pre-reqs
#### Docker, Mgconsole, Knowledge Graph Export and Import
0. For a quick start on how to install docker and compose files for Memgraph, check the [Memgraph Documentation](https://memgraph.com/docs/getting-started).

1. Connect to mgconsole:
```
# use `docker ps` to get the Memgraph container id for memgraph-mage
docker exec -ti <your-container-id> mgconsole
```
2. Regardless of the graph database client of your knowledge graph, you can export your data into a `.cypherl` file.

4. Import it via Docker mgconsole:

    a. Open a new terminal and check the Docker container ID by runningÂ `docker ps`.
  
    b. Copy theÂ `knowledge_graph_name.cypherl`Â [to your Docker container](https://memgraph.com/docs/getting-started/first-steps-with-docker#copy-files-from-and-to-a-docker-container).
  
    c. Run the following command:
```
docker exec -i <your-container-id> mgconsole < knowledge_graph_name.cypherl
```

For more information aboutÂ `mgconsole`Â options, you can run:
```
docker exec -i CONTAINER_ID mgconsole --help
```

Or check the available configurations from the [Mgconsole Documentation Page](https://memgraph.com/docs/getting-started/cli).

#### Getting the schema and properties csv
BEACON takes a nested dictionary as input. The first layer has node labels as keys and their corresponding property labels as values. In the second layer, the keys are each property labels, and the values are the specific property values. To get the nested dictionary, a simple way is to export the graph schema from the knowledge graph to create the first layer, then extract specific properties as csv using the following Cypher query:

```
MATCH (a)
WHERE LABELS(a) IS NOT NULL and a.your_property_name IS NOT NULL
RETURN LABELS(a)[0] as label, a.your_property_name as property_name
```
You can then download or export into the csv format file. If you are using Memgraph Lab, you can simply click the Download button as below:

<img width="700" alt="image" src="https://github.com/user-attachments/assets/8681b107-f037-4d13-a90d-5342da0da06c">

The input for BEACON should be somewhat like this:
```
sample_dict = {
    'BiologicalProcess': {
        'regulation_labels': 
            'regulation of nervous system development',

        'development_labels': [
            'tube morphogenesis',
            'organ development'
        ]
    },
    'CellComponent': {
        'structure_labels': [
            'mitochondrial membrane',
            'cell cortex'
        ],
        'function_labels': [
            'protein binding',
            'ion channel activity'
        ]
    }
}
```

Here, we use the [AlzKB knowledge graph](https://github.com/EpistasisLab/AlzKB/tree/master) as an example. We selectively export only the common_names property and geneSymbol property for Gene nodes since they contain the most important information of AlzKB. In practice, the nested dictionary input allows you to flexibly adjust your input.

```
from kgpre import *

common_names = pd.read_csv('./example/memgraph-query-results-export.csv', index_col=False)
grouped_names = group_labels(common_names, 'label', 'commonName') #your df, node label column name, property label column name
alzkb_nested_dict = {}
for key in grouped_names.keys():
    sub_dict = {}
    if key == 'Gene':
        sub_dict['commonName']= grouped_names[key]
        sub_dict['geneSymbol']= geneSymbol
    else:
        sub_dict['commonName']= grouped_names[key]
    alzkb_nested_dict[key] = sub_dict
```

### BEACON Features
- **kgpre.py** contains all the required functions for generating Cypher queries and conducting EA:

  	- QueryManager class: Manages functions needed to generate random Cypher queries customizable according to your specific knowledge graph. See Sample Usage below for details.
  	- DepthManager class: Tracks the "depth" of a query by counting the **number of Relationships** a query contains, which also considers the **number of hops** if exist. This is the base of determining the diversity of the final query list.
  	- EvolutionaryAlgorithm class: Manages evolutionary-algorithm related functions with initialization, crossover, mutation, *evaluation (still developing)*, output functions available. See Sample Usage below for details.
  	  
- **multiprocessing.py** contains functions related to multithreading, which interacts with mgconsole from command-line to process queries in batches and return results for further evaluation.
    - If you haven't already, since the bash file contains timeout command to limit execution time of individual queries, you need to first install the timeout module. For MacOS users, use `brew install coreutils`, then `alias timeout=gtimeout`


### Sample Usage
Using AlzKB knowledge graph as example, we can first generate a customizable size of Cypher queries like the following:

```
from kgpre import *
from multiproc import *

dm = DepthManager.getInstance()
qm = QueryManager(dm=dm)

# Import the AlzKB knowledge graph
qm.import_grouped_info(alzkb_nested_dict)
qm.import_relationships(relationships)  

# Initialize EA
ea = EvolutionaryAlgorithm(qm=qm, depth_manager=dm, initial_population_size=10,max_depth=4, max_generation=3, min_population_size=10,max_population_size=100)
ea.reset_ea()
ea.initialize_population()

# To get final result of query list, simply implement this line of code below:
final = ea.Evolve()
```
All the current generation of queries will be stored in the ea.tree_population as `TreeNode` type. `TreeNode` type contains children as a list of childs with specific types (Node, Relationship, Condition, Clause) -- also where the EA is strongly-typed. And the specific value in str type within each child can be retrieved as `child.value`. 

The scoring of individual queries is based primarily on the Validity of queries (by checking results returned from mgconsole) and the Coverage Metric score of queries weighted as below:
```
weight_type_coverage = 0.7
weight_complexity = 0.2
weight_complexity_div = 0.1

score = (weight_type_coverage * coverage) + (weight_complexity * complexity) + (weight_complexity_div * complexity_diversity)
```


**The sample large-scale result is coming soon...**








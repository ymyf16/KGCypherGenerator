{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict = {\n",
    "    'BiologicalProcess': {\n",
    "        'regulation_labels': \n",
    "            'regulation of nervous system development',\n",
    "\n",
    "        'development_labels': [\n",
    "            'tube morphogenesis',\n",
    "            'organ development'\n",
    "        ]\n",
    "    },\n",
    "    'CellComponent': {\n",
    "        'structure_labels': [\n",
    "            'mitochondrial membrane',\n",
    "            'cell cortex'\n",
    "        ],\n",
    "        'function_labels': [\n",
    "            'protein binding',\n",
    "            'ion channel activity'\n",
    "        ]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cook labels and properties\n",
    "- Grep csv file containing node labels, relationships, property labels, and wanted properties from Memgraph\n",
    "- Create a nested dictionary with {node_labels:{sub_dictionary of properties}}, the sub_dictionary contains corresponding {property_labels: properties}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Open and load the graph schema json file\n",
    "with open('schema.json', 'r',encoding='utf-8-sig') as file:\n",
    "    schema = json.load(file)\n",
    "    \n",
    "# Extract nodes and edges from the schema\n",
    "#labels = [node['labels'][0] for node in schema[0]['nodes']]\n",
    "relationships = [relationship['type'] for relationship in schema[0]['relationships']]\n",
    "\n",
    "\n",
    "# Get detailed properties from the csv file\n",
    "common_names = pd.read_csv('memgraph-query-results-export.csv', index_col=False)\n",
    "\n",
    "def group_labels(df, label_col, name_col):\n",
    "    grouped = df.groupby(label_col)[name_col].apply(list).to_dict()\n",
    "    return grouped\n",
    "\n",
    "# Applying the function\n",
    "grouped_names = group_labels(common_names, 'label', 'commonName')\n",
    "\n",
    "#***************************************\n",
    "labels = list(grouped_names.keys())\n",
    "property_labels= [\"commonName\"] #will be generalized later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create alzkb nested dictionary\n",
    "- In the current test case, all node labels are used; only commonName properties are selected for all nodes except that geneSymbol for Gene node is also added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(grouped_names)\n",
    "# grouped_names.keys()\n",
    "# list(grouped_names.values())[0]\n",
    "\n",
    "geneSymbol_csv = pd.read_csv('geneSymbol.csv', index_col=False)\n",
    "# type(geneSymbol_csv)\n",
    "# type(geneSymbol_csv['g.geneSymbol'])\n",
    "\n",
    "geneSymbol = list(geneSymbol_csv['g.geneSymbol'])\n",
    "# geneSymbol\n",
    "\n",
    "geneSymbol_sub_dict = {}\n",
    "geneSymbol_sub_dict['geneSymbol'] = geneSymbol\n",
    "\n",
    "\n",
    "alzkb_nested_dict = {}\n",
    "for key in grouped_names.keys():\n",
    "    sub_dict = {}\n",
    "    if key == 'Gene':\n",
    "        sub_dict['commonName']= grouped_names[key]\n",
    "        sub_dict['geneSymbol']= geneSymbol\n",
    "    else:\n",
    "        sub_dict['commonName']= grouped_names[key]\n",
    "    alzkb_nested_dict[key] = sub_dict\n",
    "\n",
    "# alzkb_nested_dict['Gene'].keys()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepthManager:\n",
    "    _max_depth = 5  # Default maximum depth\n",
    "    _min_depth = 3\n",
    "\n",
    "    def __init__(self):\n",
    "        self.depth = 0  # Starting depth\n",
    "\n",
    "    @classmethod\n",
    "    def set_max_depth(cls, depth):\n",
    "        if depth > cls._min_depth:\n",
    "            cls._max_depth = depth\n",
    "        else:\n",
    "            print(\"Maximum depth cannot be smaller than the min depth! \\n The default max_depth is\", cls._max_depth)\n",
    "    \n",
    "    def depth_control(self, func):\n",
    "        def wrapper(*args, **kwargs):\n",
    "            if self.depth == self._max_depth:\n",
    "                print(\"Max depth reached\")\n",
    "                return None\n",
    "            result = func(*args, **kwargs)\n",
    "            self.depth += 1  # Increment depth after function call\n",
    "            return result\n",
    "        return wrapper\n",
    "    \n",
    "    def reset_depth(self):\n",
    "        self.depth = 0\n",
    " \n",
    "class Clause():\n",
    "    def __init__(self, value, children=None):\n",
    "        self.value = value\n",
    "        self.children = children if children is not None else []\n",
    "\n",
    "    def __str__(self):\n",
    "        if not self.children:\n",
    "            return str(self.value)\n",
    "        if self.value == \"RETURN\":\n",
    "            return f\"{self.value} {', '.join(str(child) for child in self.children)}\"\n",
    "        return f\"{self.value} {' '.join(str(child) for child in self.children)}\"\n",
    "    \n",
    "class Node:\n",
    "    \"\"\"\n",
    "    When called, will add connector to either nodes or relationships \n",
    "    \"\"\"\n",
    "    def __init__(self, value, children=None):\n",
    "        self.value = value\n",
    "        self.children = children if children is not None else []\n",
    "\n",
    "    def __str__(self):\n",
    "        if not self.children:\n",
    "            return str(self.value)\n",
    "        if self.value == '-':  \n",
    "            return ' '.join(str(child) for child in self.children)\n",
    "        return f\"{self.value}({', '.join(str(child) for child in self.children)})\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_manager = DepthManager()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeNode:\n",
    "   \n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "        self.children = []\n",
    "        # self.level = 0\n",
    "    \n",
    "\n",
    "    def add_child(self, node):\n",
    "        \"\"\"Add a TreeNode or value as a child.\"\"\"\n",
    "        # if not isinstance(node, TreeNode):\n",
    "            # node = TreeNode(node)  # Ensure all children are TreeNode instances\n",
    "        self.children.append(node)\n",
    "        # self.level += 1\n",
    "\n",
    "    def __str__(self):\n",
    "        # Use the helper method for generating the string with indentation\n",
    "        return self._str_recursive(level=0)\n",
    "\n",
    "    def _str_recursive(self,level):\n",
    "        # Create the string representation with indentation for current node\n",
    "        ret = \"\\t\" *level + str(self.value) + \"\\n\"  # Indent based on the current level\n",
    "        for child in self.children:\n",
    "            ret += child._str_recursive(level+1)\n",
    "        return ret\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'<TreeNode {self.value}>'\n",
    "    \n",
    "    # def reset_level(self):\n",
    "    #     self.level = 0 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryManager:\n",
    "    def __init__(self):\n",
    "        self.root = TreeNode(\"ROOT\")  # All parts will be children of root\n",
    "        self.current_node = self.root  # Current node context for adding parts\n",
    "        self.node_labels = []\n",
    "        self.relationships = []\n",
    "        self.grouped_info = {}\n",
    "        self.usable_labels = set()\n",
    "\n",
    "        \n",
    "        self.parts = []\n",
    "        self.selected_label_alias = {}\n",
    "        # self.parts = [] \n",
    "\n",
    "    def reset_per_gen(self):\n",
    "        self.root = TreeNode(\"ROOT\")\n",
    "        self.current_node = self.root  # Reset the tree for new generation\n",
    "        self.parts = []\n",
    "        self.selected_label_alias = {}\n",
    "        self.usable_labels.clear()\n",
    "\n",
    "    def import_grouped_info(self, input_group):\n",
    "        if input_group:\n",
    "            if type(input_group) is dict:\n",
    "                self.grouped_info = input_group\n",
    "                self.node_labels = list(self.grouped_info.keys())\n",
    "                # print(\"loaded node_labels:\",self.node_labels)\n",
    "            else:\n",
    "                print(\"input grouped info need to be dictionary type\")\n",
    "        else:\n",
    "            print(\"input grouped info cannot be empty\")\n",
    "    \n",
    "    def import_relationships(self, input_relationships):\n",
    "        if input_relationships:\n",
    "            self.relationships = input_relationships\n",
    "        else:\n",
    "            print(\"relationships cannot be empty\")\n",
    "    \n",
    "    def create_unique_alias(self, label):\n",
    "        \"\"\"Creates a unique alias for a node to prevent label overlap in queries.\"\"\"\n",
    "        base_alias = label.lower()\n",
    "        alias = base_alias\n",
    "        counter = 1\n",
    "        while alias in self.usable_labels:\n",
    "            alias = f\"{base_alias}{counter}\"\n",
    "            counter += 1\n",
    "        return alias\n",
    "\n",
    "    # def add_node(self):\n",
    "    #     \"\"\"Implementation to add a node under current context.\"\"\"\n",
    "    #     node_label = random.choice(self.node_labels)\n",
    "    #     possible_props = self.grouped_info[node_label]\n",
    "    #     property_label, properties_list = random.choice(list(possible_props.items()))\n",
    "    #     property_value = random.choice(properties_list)\n",
    "    #     properties_str = f\"{property_label}: '{property_value}'\"\n",
    "    #     node = TreeNode(f\"Node({node_label} {{{properties_str}}})\")\n",
    "    #     self.current_node.add_child(node)\n",
    "    #     self.usable_labels.add(alias)\n",
    "    #     return node\n",
    "    \n",
    "    def add_node(self):\n",
    "        \"\"\"Adds a node with random properties selected from grouped_info.\"\"\"\n",
    "        # node_label = ''\n",
    "        if self.node_labels:\n",
    "            node_label = random.choice(self.node_labels) \n",
    "            possible_props = self.grouped_info[node_label]\n",
    "            property_label, properties_list = random.choice(list(possible_props.items()))\n",
    "            alias = self.create_unique_alias(node_label)\n",
    "            self.selected_label_alias[alias]=node_label\n",
    "\n",
    "            property_value = random.choice(properties_list)\n",
    "            properties_str = f'''{property_label}: \"{property_value}\"''' if possible_props else ''\n",
    "            node_value = f\"{node_label} {{{properties_str}}}\"\n",
    "\n",
    "\n",
    "            node = Node(f\"({alias}:{node_value})\")\n",
    "            # self.current_node.add_child(node)\n",
    "\n",
    "            # self.nodes.append(node)\n",
    "            self.usable_labels.add(alias)  # Store label for possible RETURN clause usage\n",
    "            return node \n",
    "        print(\"No node labels available. Please import grouped info first.\")\n",
    "        return None\n",
    "\n",
    "    # def add_relationship(self, hop_p=0.5):\n",
    "    #     \"\"\"Implementation to add a relationship.\"\"\"\n",
    "    #     if random.random() < hop_p and len(self.nodes) > 1:\n",
    "    #         from_node, to_node = random.sample(self.nodes, 2)\n",
    "    #         relationship = TreeNode(f\"Relationship from {from_node.value} to {to_node.value}\")\n",
    "    #         self.current_node.add_child(relationship)\n",
    "    #         return relationship\n",
    "    #     return None\n",
    "    \n",
    "    @depth_manager.depth_control\n",
    "    def add_hop(self,hop_p=0.5):\n",
    "        \"\"\"\n",
    "        Randomly generate hops as condition to relationship based on a customizable possibility;\n",
    "        the default possibility is 0.5\n",
    "        \"\"\"\n",
    "        current_depth = depth_manager.depth\n",
    "        hop = random.randint(1,10) #TODO: see if this is reasonable\n",
    "        upper_hop = hop + random.randint(1,10)\n",
    "        exact_hop = f\"*{hop}\"\n",
    "        ceiling_hop = f\"*..{upper_hop}\"\n",
    "        floor_hop = f\"*{hop}..\"\n",
    "        interval_hop = f\"*{hop}..{upper_hop}\"\n",
    "        hop_choices = [exact_hop, ceiling_hop, floor_hop, interval_hop]\n",
    "        if random.random() > hop_p and current_depth < depth_manager._max_depth:\n",
    "            hop_choice = random.choice(hop_choices)\n",
    "            return hop_choice\n",
    "        else:\n",
    "            return ''\n",
    "\n",
    "\n",
    "    @depth_manager.depth_control\n",
    "    def add_relationship(self, hop_p=0.5):\n",
    "        \"\"\" Randomly generate a relationship between two nodes \"\"\"\n",
    "        current_depth = depth_manager.depth\n",
    "        rel_type = random.choice(self.relationships)\n",
    "        if current_depth>=3 and random.random() > 0.5: \n",
    "            direction1 = \"<-\"\n",
    "            direction2 = \"-\"\n",
    "        else:\n",
    "            direction1 = \"-\" \n",
    "            direction2 = \"->\"\n",
    "        hop_result = self.add_hop(hop_p) if self.add_hop(hop_p) else ''\n",
    "        relationship = Node(f\"{direction1} [:{rel_type}{hop_result}] {direction2}\")\n",
    "        # self.current_node.add_child(relationship)\n",
    "        # return relationship\n",
    "        return Node(\"-\", [relationship])\n",
    "        \n",
    "    # @depth_manager.depth_control\n",
    "    # def add_condition(self, where_p=0.5):\n",
    "    #     \"\"\"\n",
    "    #     Randomly generate WHERE clause based on a customizable possibility;\n",
    "    #     the default possibility where_p is 0.5\n",
    "    #     \"\"\"\n",
    "    #     current_depth = depth_manager.depth\n",
    "    #     if random.random() > where_p and current_depth < depth_manager._max_depth:\n",
    "    #         # node_label = random.choice(labels)\n",
    "    #         # label_lower = node_label.lower()\n",
    "    #         # property_label = random.choice(property_labels)\n",
    "    #         # node_label = random.choice(self.selected_node_label)\n",
    "    #         # alias = random.choice\n",
    "    #         alias, node_label = random.choice(list(self.selected_label_alias.items()))\n",
    "    #         print(alias, node_label)\n",
    "\n",
    "    #         possible_properties = self.grouped_info[node_label]\n",
    "    #         if possible_properties:\n",
    "    #             property_label, properties_list = random.choice(list(possible_properties.items()))\n",
    "    #             sample_prop_type = properties_list[0]\n",
    "    #             value = random.randint(20, 50) if isinstance(sample_prop_type, int) else random.choice(properties_list) \n",
    "    #         #TODO: customize the int part\n",
    "\n",
    "    #             operator = random.choice([\">\", \"<\", \"=\", \"<=\", \">=\"]) if isinstance(sample_prop_type, int) else '='\n",
    "    #             return Clause(\"WHERE\", [Clause(f\"{alias}.{property_label} {operator} '{value}'\", [])])\n",
    "    #         return None\n",
    "    #     return None\n",
    "    @staticmethod\n",
    "    def is_relationship(part):\n",
    "        \"\"\"\n",
    "        Determine if the given part of a query is a relationship based on containing \"[]\"\n",
    "        Ensures that part is a string before checking.\n",
    "        \"\"\"\n",
    "        # pattern = re.compile(r'\\[(.*?)\\]')\n",
    "        pattern = re.compile(r'(?:-\\s*\\[:.*?\\]\\s*->|<-\\s*\\[:.*?\\]\\s*-)')\n",
    "        trying = r\"-\\s*\\[:?([A-Za-z0-9_]+)?(\\*\\d*(\\.\\.\\d*)?)?\\]\\s*[-<>]?\"\n",
    "        # Ensure part is a string or bytes-like object\n",
    "        if isinstance(part, str):\n",
    "            # if pattern.search(part):\n",
    "            if re.search(trying, part):\n",
    "                return True\n",
    "            return False\n",
    "        print(\"input has to be str!\")\n",
    "        return\n",
    "\n",
    "    \n",
    "    def get_usable_labels(self):\n",
    "        return list(self.usable_labels)\n",
    "    \n",
    "    def add_return(self, return_num=None):\n",
    "        if return_num:\n",
    "            random_k = random.randint(1,return_num)\n",
    "            # choices = random.sample(self.usable_labels, return_num)\n",
    "        usable_labels = self.get_usable_labels()\n",
    "        # print(usable_labels)\n",
    "        random_k = random.randint(1,len(usable_labels))\n",
    "        choices = random.sample(self.usable_labels, random_k)\n",
    "        if choices:  # Check if the list is not empty\n",
    "            return Clause(\"RETURN\", choices)\n",
    "        return None\n",
    "    \n",
    "\n",
    "    def generate_query(self, flag=True, return_num=None, part_num=None, hop_p=0.5, where_p=0.5):\n",
    "        self.reset_per_gen()\n",
    "        depth_manager.reset_depth()\n",
    "\n",
    "        def alternate_functions(flag):\n",
    "            if flag:\n",
    "                return self.add_node(), not flag\n",
    "            else:\n",
    "                return self.add_relationship(hop_p), not flag\n",
    "\n",
    "        if part_num is None:\n",
    "            part_num = random.randint(1, depth_manager._max_depth-2)\n",
    "\n",
    "        # Keep adding nodes and relationships while depth is within limit\n",
    "        for _ in range(part_num+1):\n",
    "            part, flag = alternate_functions(flag)\n",
    "            if part is None:\n",
    "                break\n",
    "            self.parts.append(part)\n",
    "            self.current_node.add_child(TreeNode(part))\n",
    "\n",
    "        # Check if the last part is a relationship; if so, add a terminating node\n",
    "        if self.parts and self.is_relationship(str(self.parts[-1])): #ensure the input part is in string format\n",
    "            final_node = self.add_node()  # Generate a final node\n",
    "            if final_node:\n",
    "                self.parts.append(final_node)\n",
    "                print(\"final_node added:\", final_node)\n",
    "                self.current_node.add_child(TreeNode(final_node))\n",
    "\n",
    "\n",
    "        # Optionally add a WHERE clause if depth is still under max_depth\n",
    "        # condition = self.add_condition(where_p)\n",
    "        # if condition:\n",
    "        #     self.current_node.add_child(TreeNode(f\"WHERE {condition}\"))\n",
    "\n",
    "        # Add RETURN clause \n",
    "        ret = self.add_return(return_num)\n",
    "        if ret:\n",
    "            self.current_node.add_child(TreeNode(ret))\n",
    "\n",
    "        # print(\"Final Query Structure:\")\n",
    "        # print(self.current_node)  # Visualize the tree structure of the query\n",
    "        # self.current_node.reset_level()\n",
    "        return self.current_node\n",
    "    \n",
    "    ### FOR CROSSOVER RETURN ADJUSTMENT\n",
    "    def extract_node_alias(self, node_value_str):\n",
    "        pattern = r\"^\\(([^:]+):\"\n",
    "        match = re.search(pattern, node_value_str)\n",
    "        if match:\n",
    "            return match.group(1)  # Returns the first capturing group\n",
    "        # print(\"None found\")\n",
    "        return None\n",
    "\n",
    "    def collect_labels(self, tree):\n",
    "        \"\"\" Recursively collect labels from the tree that are usable in the RETURN clause. \"\"\"\n",
    "        if isinstance(tree, TreeNode) and isinstance(tree.children, list):\n",
    "            for child in tree.children:\n",
    "                # Extract label from the current node's value and add it to usable labels\n",
    "                child_value = str(child.value)\n",
    "                # print(child_value, type(child_value))\n",
    "                label = self.extract_node_alias(child_value)\n",
    "                if label:\n",
    "                    self.usable_labels.add(label)\n",
    "            # Recursively process each child\n",
    "                # self.collect_labels(child)\n",
    "                \n",
    "    \n",
    "    def adjust_return(self, tree):\n",
    "        \"\"\" Adjust the RETURN clause based on the labels collected from the tree. \"\"\"\n",
    "        if not isinstance(tree, TreeNode):\n",
    "            raise TypeError(\"Expected a tree that is TreeNode instance\")\n",
    "        # Clear existing labels and recollect from the new tree structure\n",
    "        self.usable_labels.clear()\n",
    "        self.collect_labels(tree)\n",
    "        \n",
    "        if self.usable_labels:\n",
    "            random_k = random.randint(1, len(self.usable_labels))\n",
    "            choices = random.sample(self.usable_labels, random_k)\n",
    "            new_return = Clause(\"RETURN\", choices)\n",
    "            \n",
    "            if tree.children and isinstance(tree.children[-1], TreeNode) and \"RETURN\" in str(tree.children[-1].value):\n",
    "                tree.children[-1] = TreeNode(new_return)  # Replace the last child with the new RETURN clause\n",
    "            else:\n",
    "                tree.add_child(TreeNode(new_return))  # Add new if no RETURN exists\n",
    "            # print(\"updated return:\",tree.children[-1])\n",
    "            return tree\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT\n",
      "\t(biologicalprocess:BiologicalProcess {regulation_labels: \"r\"})\n",
      "\t- [:rel2] ->\n",
      "\t(biologicalprocess1:BiologicalProcess {regulation_labels: \"m\"})\n",
      "\tRETURN biologicalprocess1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hx/mhvsgnnd2s3_cc4f_0smyv800000gn/T/ipykernel_64853/1052888239.py:188: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  choices = random.sample(self.usable_labels, random_k)\n"
     ]
    }
   ],
   "source": [
    "#Sample Usage\n",
    "qm = QueryManager()\n",
    "qm.import_grouped_info(test_dict)  # Assuming this is filled\n",
    "qm.import_relationships([\"rel1\", \"rel2\",\"rel3\"])  # Example relationships\n",
    "depth_manager.set_max_depth(9)\n",
    "depth_manager.reset_depth()\n",
    "result = qm.generate_query()\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Swapping and Crossover function implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def swap(query_manager1, query_manager2):\n",
    "    if not query_manager1.children or not query_manager2.children:\n",
    "        print(\"One of the trees does not have children to perform swapping.\")\n",
    "        return\n",
    "\n",
    "    # Select random subtree indices from both trees\n",
    "    index1 = random.randint(0, len(query_manager1.children) - 1)\n",
    "    index2 = random.randint(0, len(query_manager2.children) - 1)\n",
    "\n",
    "    # Swap the subtrees\n",
    "    query_manager1.children[index1], query_manager2.children[index2] = \\\n",
    "        query_manager2.children[index2], query_manager1.children[index1]\n",
    "\n",
    "    print(\"Swapping completed.\")\n",
    "\n",
    "\n",
    "def crossover(tree1, tree2):\n",
    "    qm = QueryManager()\n",
    "    if not tree1.children or not tree2.children:\n",
    "        print(\"One of the trees does not have children to perform crossover.\")\n",
    "        return\n",
    "\n",
    "    # Filter out indices that are not relationships\n",
    "    node_indices1 = [index for index, child in enumerate(tree1.children) if QueryManager.is_relationship(str(child))==False]\n",
    "    node_indices2 = [index for index, child in enumerate(tree2.children) if QueryManager.is_relationship(str(child))==False]\n",
    "\n",
    "    # Check if there are any nodes to perform crossover\n",
    "    if not node_indices1 or not node_indices2:\n",
    "        print(\"No nodes available for crossover in one or both trees.\")\n",
    "        return\n",
    "\n",
    "    # Select random node indices from the filtered lists\n",
    "    index1 = random.choice(node_indices1[:-1])\n",
    "    index2 = random.choice(node_indices2[:-1])\n",
    "\n",
    "    # Swap the subtrees at these indices\n",
    "    tree1.children[index1:], tree2.children[index2:] = \\\n",
    "        tree2.children[index2:], tree1.children[index1:]\n",
    "    # print(\"tree1 after crossover:\", tree1)\n",
    "    # print(\"tree2 after crossover:\", tree2)\n",
    "    # print(\"Before adjust, tree1 and tree2\", tree1.children[-1].value, tree2.children[-1].value)\n",
    "    \n",
    "    qm = QueryManager()\n",
    "    tree1 = qm.adjust_return(tree1)\n",
    "    tree2 = qm.adjust_return(tree2)\n",
    "\n",
    "    # print(\"After adjust, tree1 and tree2\", '\\n', tree1, '\\n',tree2)\n",
    "\n",
    "    print(\"Crossover and Return clause adjustment completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_node added: (disease:Disease {commonName: \"Alzheimer Disease, Familial, 3, with Spastic Paraparesis and Unusual Plaques\"})\n",
      "Crossover and Return clause adjustment completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hx/mhvsgnnd2s3_cc4f_0smyv800000gn/T/ipykernel_64853/1052888239.py:188: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  choices = random.sample(self.usable_labels, random_k)\n",
      "/var/folders/hx/mhvsgnnd2s3_cc4f_0smyv800000gn/T/ipykernel_64853/1052888239.py:272: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  choices = random.sample(self.usable_labels, random_k)\n"
     ]
    }
   ],
   "source": [
    "# Try if a single qm is enough -- YES!\n",
    "qm = QueryManager()\n",
    "\n",
    "qm.import_grouped_info(alzkb_nested_dict) \n",
    "qm.import_relationships(relationships)\n",
    "tree1 = qm.generate_query() \n",
    "tree2 = qm.generate_query()\n",
    "\n",
    "crossover(tree1, tree2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT\n",
      "\t(gene:Gene {commonName: \"-\"})\n",
      "\t<- [:GENEINPATHWAY*2] -\n",
      "\t(bodypart:BodyPart {commonName: \"mammary gland\"})\n",
      "\t<- [:GENEHASMOLECULARFUNCTION] -\n",
      "\t(disease:Disease {commonName: \"Alzheimer Disease, Familial, 3, with Spastic Paraparesis and Unusual Plaques\"})\n",
      "\tRETURN disease, gene\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tree1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT\n",
      "\t(biologicalprocess:BiologicalProcess {commonName: \"cell aging\"})\n",
      "\t- [:GENEHASMOLECULARFUNCTION*2..7] ->\n",
      "\t(drug:Drug {commonName: \"AMG-222\"})\n",
      "\t<- [:GENEREGULATESGENE] -\n",
      "\t(gene:Gene {commonName: \"-\"})\n",
      "\t- [:GENEASSOCIATESWITHDISEASE] ->\n",
      "\t(biologicalprocess:BiologicalProcess {commonName: \"regulation of lymphocyte activation\"})\n",
      "\t- [:SYMPTOMMANIFESTATIONOFDISEASE] ->\n",
      "\t(molecularfunction:MolecularFunction {commonName: \"inositol monophosphate 1-phosphatase activity\"})\n",
      "\tRETURN gene, biologicalprocess, molecularfunction\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tree2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building GP workflow and Connect to Memgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try processing the bad csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.read_csv('test_none.csv')\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def process_csv(input_filepath, output_filepath):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(input_filepath, header=None, names=['Type', 'Data'])\n",
    "    \n",
    "    # Ensure all data is treated as string\n",
    "    df['Data'] = df['Data'].astype(str)\n",
    "    \n",
    "    # Process the 'Data' to escape internal quotes and encapsulate it as a single string if it contains commas\n",
    "    df['Data'] = df['Data'].apply(lambda x: '\"' + x.replace('\"', '\"\"') + '\"' if ',' in x else x)\n",
    "\n",
    "    # Aggregating results for each type into one cell, separated by a semicolon\n",
    "    # This assumes the 'Type' column correctly identifies different queries\n",
    "    agg_df = df.groupby('Type')['Data'].agg(lambda x: '; '.join(x)).reset_index()\n",
    "\n",
    "    # Write the processed data back to a new CSV\n",
    "    agg_df.to_csv(output_filepath, index=False)\n",
    "\n",
    "\n",
    "# File paths\n",
    "input_csv = 'test_none.csv'\n",
    "output_csv = 'test_modified_output.csv'\n",
    "\n",
    "# Process the CSV\n",
    "process_csv(input_csv, output_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(:Drug {commonName: \"Basiliximab\", nodeID: \"26...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(:Drug {commonName: \"Muromonab\", nodeID: \"264\"...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(:Gene {commonName: \"TATA-box binding protein\"...</td>\n",
       "      <td>[:GENEPARTICIPATESINBIOLOGICALPROCESS]; [:GENE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>m</td>\n",
       "      <td>r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>n</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Type  \\\n",
       "0  (:Drug {commonName: \"Basiliximab\", nodeID: \"26...   \n",
       "1  (:Drug {commonName: \"Muromonab\", nodeID: \"264\"...   \n",
       "2  (:Gene {commonName: \"TATA-box binding protein\"...   \n",
       "3                                                  m   \n",
       "4                                                  n   \n",
       "\n",
       "                                                Data  \n",
       "0                                                NaN  \n",
       "1                                                NaN  \n",
       "2  [:GENEPARTICIPATESINBIOLOGICALPROCESS]; [:GENE...  \n",
       "3                                                  r  \n",
       "4                                                NaN  "
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"test_modified_output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(7691) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    }
   ],
   "source": [
    "!chmod +x run_queries.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "def process_csv(input_filepath, output_filepath):\n",
    "    # Open the raw CSV file for reading\n",
    "    with open(input_filepath, 'r', newline='', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file)\n",
    "        # Initialize a list to hold processed results\n",
    "        processed_results = []\n",
    "\n",
    "        # Temporary storage for aggregating related data\n",
    "        current_query = None\n",
    "        current_data = []\n",
    "\n",
    "        for row in reader:\n",
    "            if not row:\n",
    "                continue\n",
    "\n",
    "            # Extract query and data\n",
    "            if len(row) == 2:\n",
    "                query, data = row\n",
    "            elif len(row) > 2:  # Handles cases where commas split the data erroneously\n",
    "                query = row[0]\n",
    "                data = ','.join(row[1:])\n",
    "            else:\n",
    "                continue  # Skip empty or malformed rows\n",
    "\n",
    "            # Check if we've moved to a new query\n",
    "            if current_query is None or current_query == query:\n",
    "                current_query = query\n",
    "                current_data.append(data.replace('\"\"', '\"').strip('\"'))  # Normalize double quotes and strip outer quotes\n",
    "            else:\n",
    "                # Concatenate all collected data entries for the previous query\n",
    "                processed_results.append([current_query, '; '.join(current_data)])\n",
    "                current_query = query\n",
    "                current_data = [data.replace('\"\"', '\"').strip('\"')]\n",
    "\n",
    "        # Don't forget to add the last batch of collected data\n",
    "        if current_data:\n",
    "            processed_results.append([current_query, '; '.join(current_data)])\n",
    "\n",
    "    # Write processed data to the new CSV file\n",
    "    with open(output_filepath, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Query', 'Result'])  # Write header\n",
    "        for result in processed_results:\n",
    "            writer.writerow(result)\n",
    "\n",
    "# File paths\n",
    "input_csv = 'final_result.csv'\n",
    "output_csv = 'formatted_final_result.csv'\n",
    "\n",
    "# Process the CSV\n",
    "process_csv(input_csv, output_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'''\"MATCH (gene:Gene {commonName: \"-\"})&lt;- [:GE...</td>\n",
       "      <td>Familial</td>\n",
       "      <td>3</td>\n",
       "      <td>with Spastic Paraparesis and Unusual Plaques\"...</td>\n",
       "      <td>gene; \"'''</td>\n",
       "      <td>'''\"no_result\"'''</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MATCH (n) RETURN n LIMIT 2;</td>\n",
       "      <td>''' \"n\"</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(:Drug {commonName: \"Basiliximab\", nodeID: \"26...</td>\n",
       "      <td>nodeID: \"\"264\"\"</td>\n",
       "      <td>uri: \"\"http://jdr.bio/ontologies/alzkb.owl#dr...</td>\n",
       "      <td>xrefCasRN: \"\"140608-64-6\"\"</td>\n",
       "      <td>xrefDrugbank: \"\"DB00075\"\"})\"'''</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'''\"MATCH (m:Gene)-[r]-&gt;(Gene) RETURN m</td>\n",
       "      <td>r LIMIT 2;\"'''</td>\n",
       "      <td>''' \"m\"</td>\n",
       "      <td>r</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(:Gene {commonName: \"TATA-box binding protein\"...</td>\n",
       "      <td>[:GENEPARTICIPATESINBIOLOGICALPROCESS]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(:Gene {commonName: \"TATA-box binding protein\"...</td>\n",
       "      <td>[:GENEPARTICIPATESINBIOLOGICALPROCESS]'''</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  \\\n",
       "0  '''\"MATCH (gene:Gene {commonName: \"-\"})<- [:GE...   \n",
       "1                       MATCH (n) RETURN n LIMIT 2;    \n",
       "2  (:Drug {commonName: \"Basiliximab\", nodeID: \"26...   \n",
       "3            '''\"MATCH (m:Gene)-[r]->(Gene) RETURN m   \n",
       "4  (:Gene {commonName: \"TATA-box binding protein\"...   \n",
       "5  (:Gene {commonName: \"TATA-box binding protein\"...   \n",
       "\n",
       "                                           1  \\\n",
       "0                                   Familial   \n",
       "1                                   ''' \"n\"    \n",
       "2                            nodeID: \"\"264\"\"   \n",
       "3                             r LIMIT 2;\"'''   \n",
       "4     [:GENEPARTICIPATESINBIOLOGICALPROCESS]   \n",
       "5  [:GENEPARTICIPATESINBIOLOGICALPROCESS]'''   \n",
       "\n",
       "                                                   2  \\\n",
       "0                                                  3   \n",
       "1                                                NaN   \n",
       "2   uri: \"\"http://jdr.bio/ontologies/alzkb.owl#dr...   \n",
       "3                                            ''' \"m\"   \n",
       "4                                                NaN   \n",
       "5                                                NaN   \n",
       "\n",
       "                                                   3  \\\n",
       "0   with Spastic Paraparesis and Unusual Plaques\"...   \n",
       "1                                                NaN   \n",
       "2                         xrefCasRN: \"\"140608-64-6\"\"   \n",
       "3                                                  r   \n",
       "4                                                NaN   \n",
       "5                                                NaN   \n",
       "\n",
       "                                  4                  5  \n",
       "0                        gene; \"'''  '''\"no_result\"'''  \n",
       "1                               NaN                NaN  \n",
       "2   xrefDrugbank: \"\"DB00075\"\"})\"'''                NaN  \n",
       "3                               NaN                NaN  \n",
       "4                               NaN                NaN  \n",
       "5                               NaN                NaN  "
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"final_result_test_format.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(43435) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(43438) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1 executed and output saved to ./outputs/query1.csv\n",
      "Query 2 executed and output saved to ./outputs/query2.csv\n",
      "Query 3 executed and output saved to ./outputs/query3.csv\n",
      "All queries processed.\n"
     ]
    }
   ],
   "source": [
    "!chmod +x single_query_run.sh\n",
    "!./single_query_run.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_csv(folder):\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

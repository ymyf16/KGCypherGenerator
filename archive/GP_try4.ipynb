{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict = {\n",
    "    'BiologicalProcess': {\n",
    "        'regulation_labels': \n",
    "            'regulation of nervous system development',\n",
    "\n",
    "        'development_labels': [\n",
    "            'tube morphogenesis',\n",
    "            'organ development'\n",
    "        ]\n",
    "    },\n",
    "    'CellComponent': {\n",
    "        'structure_labels': [\n",
    "            'mitochondrial membrane',\n",
    "            'cell cortex'\n",
    "        ],\n",
    "        'function_labels': [\n",
    "            'protein binding',\n",
    "            'ion channel activity'\n",
    "        ]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cook labels and properties\n",
    "- Grep csv file containing node labels, relationships, property labels, and wanted properties from Memgraph\n",
    "- Create a nested dictionary with {node_labels:{sub_dictionary of properties}}, the sub_dictionary contains corresponding {property_labels: properties}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import json\n",
    "import pandas as pd\n",
    "import copy\n",
    "import hashlib\n",
    "import base64\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Open and load the graph schema json file\n",
    "with open('./example/schema.json', 'r',encoding='utf-8-sig') as file:\n",
    "    schema = json.load(file)\n",
    "    \n",
    "# Extract nodes and edges from the schema\n",
    "#labels = [node['labels'][0] for node in schema[0]['nodes']]\n",
    "relationships = [relationship['type'] for relationship in schema[0]['relationships']]\n",
    "\n",
    "\n",
    "# Get detailed properties from the csv file\n",
    "common_names = pd.read_csv('./example/memgraph-query-results-export.csv', index_col=False)\n",
    "\n",
    "def group_labels(df, label_col, name_col):\n",
    "    grouped = df.groupby(label_col)[name_col].apply(list).to_dict()\n",
    "    return grouped\n",
    "\n",
    "# Applying the function\n",
    "grouped_names = group_labels(common_names, 'label', 'commonName')\n",
    "\n",
    "#***************************************\n",
    "labels = list(grouped_names.keys())\n",
    "property_labels= [\"commonName\"] #will be generalized later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>commonName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Drug</td>\n",
       "      <td>Basiliximab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drug</td>\n",
       "      <td>Muromonab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Drug</td>\n",
       "      <td>Trastuzumab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Drug</td>\n",
       "      <td>Rituximab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Drug</td>\n",
       "      <td>Ibritumomab tiuxetan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label            commonName\n",
       "0  Drug           Basiliximab\n",
       "1  Drug             Muromonab\n",
       "2  Drug           Trastuzumab\n",
       "3  Drug             Rituximab\n",
       "4  Drug  Ibritumomab tiuxetan"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_names.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create alzkb nested dictionary\n",
    "- In the current test case, all node labels are used; only commonName properties are selected for all nodes except that geneSymbol for Gene node is also added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(grouped_names)\n",
    "# grouped_names.keys()\n",
    "# list(grouped_names.values())[0]\n",
    "\n",
    "geneSymbol_csv = pd.read_csv('./example/geneSymbol.csv', index_col=False)\n",
    "# type(geneSymbol_csv)\n",
    "# type(geneSymbol_csv['g.geneSymbol'])\n",
    "\n",
    "geneSymbol = list(geneSymbol_csv['g.geneSymbol'])\n",
    "# geneSymbol\n",
    "\n",
    "geneSymbol_sub_dict = {}\n",
    "geneSymbol_sub_dict['geneSymbol'] = geneSymbol\n",
    "\n",
    "\n",
    "alzkb_nested_dict = {}\n",
    "for key in grouped_names.keys():\n",
    "    sub_dict = {}\n",
    "    if key == 'Gene':\n",
    "        sub_dict['commonName']= grouped_names[key]\n",
    "        sub_dict['geneSymbol']= geneSymbol\n",
    "    else:\n",
    "        sub_dict['commonName']= grouped_names[key]\n",
    "    alzkb_nested_dict[key] = sub_dict\n",
    "\n",
    "# alzkb_nested_dict['Gene'].keys()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('alzkb_nested_dict_test.txt', 'w') as test: \n",
    "     test.write(json.dumps(alzkb_nested_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DepthManager, QueryManager, Nodes definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DepthManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDepthManager: #TRY TO MAKE DEPTHMANAGER SINGLETON \n",
    "    _max_depth = 5  # Default maximum depth\n",
    "    _min_depth = 3\n",
    "    _instance = None\n",
    "\n",
    "    @classmethod\n",
    "    def getInstance(cls):\n",
    "        if cls._instance is None:\n",
    "            cls._instance = cls()\n",
    "        return cls._instance\n",
    "\n",
    "    def __init__(self):\n",
    "        if TestDepthManager._instance is not None:\n",
    "            raise Exception(\"This is a singleton class. Use 'getInstance()'.\")\n",
    "        self.depth = 0  # Starting depth\n",
    "        self.depth_record = {}\n",
    "\n",
    "    @classmethod\n",
    "    def set_max_depth(cls, depth):\n",
    "        if depth > cls._min_depth:\n",
    "            cls._max_depth = depth\n",
    "        else:\n",
    "            print(\"Maximum depth cannot be smaller than the min depth! \\n The default max_depth is\", cls._max_depth)\n",
    "    \n",
    "    def reset_depth(self):\n",
    "        self.depth = 0\n",
    "    \n",
    "    # def reset_depth_record(self):\n",
    "    #     self.depth_record = {}\n",
    "    \n",
    "def depth_control(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        dm = TestDepthManager.getInstance()\n",
    "        if dm.depth == dm._max_depth:\n",
    "            print(\"Max depth reached\")\n",
    "            return None\n",
    "        result = func(*args, **kwargs)\n",
    "        dm.depth += 1  # Increment depth after function call\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "def class_depth_control(cls):\n",
    "    class WrappedClass(cls):  # Create a new class that wraps the original class\n",
    "        def __init__(self, *args, **kwargs):\n",
    "            dm = TestDepthManager.getInstance()\n",
    "            if dm.depth == dm._max_depth:\n",
    "                print(\"Max depth reached\")\n",
    "                return None\n",
    "            super().__init__(*args, **kwargs)\n",
    "            dm.depth += 1\n",
    "    return WrappedClass "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TreeNode:\n",
    "    depth = 0\n",
    "    score = 1 #defaultly assume tree as garbage query\n",
    "   \n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "        self.children = []\n",
    "        # self.level = 0\n",
    "\n",
    "    def add_child(self, node):\n",
    "        \"\"\"Add a TreeNode or value as a child.\"\"\"\n",
    "        # if not isinstance(node, TreeNode):\n",
    "            # node = TreeNode(node)  # Ensure all children are TreeNode instances\n",
    "        self.children.append(node)\n",
    "        # self.level += 1\n",
    "\n",
    "    def __str__(self):\n",
    "        # Use the helper method for generating the string with indentation\n",
    "        return self._str_recursive(level=0)\n",
    "\n",
    "    def _str_recursive(self,level):\n",
    "        # Create the string representation with indentation for current node\n",
    "        ret = \"\\t\" *level + str(self.value) + \"\\n\"  # Indent based on the current level\n",
    "        for child in self.children:\n",
    "            ret += child._str_recursive(level+1)\n",
    "        return ret\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'<TreeNode {self.value}>'\n",
    "    \n",
    "    def get_depth(self):\n",
    "        pass #because it needs later-defined class type\n",
    "\n",
    "    def to_querystr(self):\n",
    "        \"\"\"\n",
    "        convert the generate query tree into query string with ; separation to get ready for querying the Memgraph client\n",
    "        \"\"\"\n",
    "        child_compose = ''\n",
    "        final_query_str = 'MATCH'\n",
    "        for child in self.children:\n",
    "            if child.children:\n",
    "                for grandchild in child.children:\n",
    "                    child_compose = ' '+ str(grandchild.value)\n",
    "            final_query_str += ' ' + str(child.value) + child_compose\n",
    "        final_query_str += ';'\n",
    "        return final_query_str\n",
    "\n",
    "        \n",
    "class Clause(TreeNode):\n",
    "    def __init__(self, value, children=None):\n",
    "        super().__init__(value)\n",
    "        self.children = children if children is not None else []\n",
    "\n",
    "    def __str__(self):\n",
    "        if not self.children:\n",
    "            return str(self.value)\n",
    "        if self.value == \"RETURN\":\n",
    "            return f\"{self.value} {', '.join(str(child) for child in self.children)}\"\n",
    "        return f\"{self.value} {' '.join(str(child) for child in self.children)}\"\n",
    "    \n",
    "class Node(TreeNode):\n",
    "    \"\"\"\n",
    "    When called, will add connector to nodes and make nodes Node type\n",
    "    \"\"\" \n",
    "    def __init__(self, value, children=None):\n",
    "        super().__init__(value)\n",
    "        self.children = children if children is not None else []\n",
    " \n",
    "    def __str__(self):\n",
    "        if not self.children:\n",
    "            return str(self.value)\n",
    "        # if self.value == '-':  \n",
    "        #     return ' '.join(str(child) for child in self.children)\n",
    "        return f\"{self.value}({', '.join(str(child) for child in self.children)})\"\n",
    "\n",
    "# @class_depth_control\n",
    "class Relationship(TreeNode):\n",
    "    \"\"\"\n",
    "    When called, will add connector to relationships and make relationships Relationship type\n",
    "    \"\"\"\n",
    "    def __init__(self, value, hop_only=False):\n",
    "        super().__init__(value)\n",
    "        # self.children = children if children is not None else []\n",
    "        self.hop_only = True if hop_only else False\n",
    "        \n",
    "    @depth_control\n",
    "    def __str__(self):\n",
    "        # if not self.children:\n",
    "            # return str(self.value)\n",
    "        return f\"{self.value}\"\n",
    "        # return f\"{self.value} {' '.join(str(child) for child in self.children)}\"\n",
    "\n",
    "    def calculate_individual_depth(self):\n",
    "        # This method checks for both the presence of a relationship and additional depth from hops\n",
    "        base_depth = 1 if self.hop_only == False else 0 # Start with a depth of 1 for the relationship itself\n",
    "        # Look for hop patterns, each '*' adds one to the depth\n",
    "        hop_matches = re.findall(r'\\*', self.value.value)\n",
    "        return base_depth + len(hop_matches)  # Add one additional depth for each hop pattern\n",
    "\n",
    "def get_depth(self):\n",
    "    # This now maps through children, checking if they are Relationship instances, and sums their depths\n",
    "    updated_depth = 0\n",
    "    for child in self.children:\n",
    "        if isinstance(child, Relationship):\n",
    "            updated_depth += child.calculate_individual_depth()\n",
    "        elif isinstance(child,Condition):\n",
    "            updated_depth += 1\n",
    "        else:\n",
    "            updated_depth = updated_depth\n",
    "    # updated_depth = sum(child.calculate_individual_depth() if isinstance(child, Relationship) else 0 for child in self.children)\n",
    "    return updated_depth\n",
    "TreeNode.get_depth = get_depth\n",
    "\n",
    "\n",
    "class Condition(TreeNode):\n",
    "    def __init__(self, value, children=None):\n",
    "        super().__init__(value)\n",
    "        self.children = children if children is not None else []\n",
    "\n",
    "    def __str__(self):\n",
    "        if not self.children:\n",
    "            return str(self.value)\n",
    "        return f\"{self.value} {' '.join(str(child) for child in self.children)}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QueryManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DepthManagerInUse:\n",
    "\n",
    "class TestQueryManager:\n",
    "    def __init__(self, dm):\n",
    "        self.root = TreeNode(\"ROOT\")  # All parts will be children of root\n",
    "        self.current_node = self.root  # Current node context for adding parts\n",
    "        self.node_labels = []\n",
    "        self.relationships = []\n",
    "        self.grouped_info = {}\n",
    "        self.usable_labels = set()\n",
    "        # self.usable_props = set()\n",
    "\n",
    "        \n",
    "        self.parts = []\n",
    "        self.selected_label_alias = {}\n",
    "\n",
    "        self.query_str = ''\n",
    "        \n",
    "        self.final_depth = 0\n",
    "        # self.depth_manager = DepthManager()\n",
    "        # self.id = self.generate_id()\n",
    "        self.observed_depths = set()\n",
    "        self.dm = dm\n",
    "       \n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_id(input_object):\n",
    "        if isinstance(input_object, TreeNode):\n",
    "            content = str(input_object)  \n",
    "        elif isinstance(input_object, str):\n",
    "            content = input_object\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported object type for ID generation\")\n",
    "        hash_digest = hashlib.sha256(content.encode('utf-8')).digest()\n",
    "        return base64.urlsafe_b64encode(hash_digest).decode('utf-8').rstrip('=')\n",
    "\n",
    "        \n",
    "\n",
    "    def reset_per_gen(self):\n",
    "        self.root = TreeNode(\"ROOT\")\n",
    "        self.current_node = self.root  # Reset the tree for new generation\n",
    "        self.parts = []\n",
    "        self.selected_label_alias = {}\n",
    "        self.usable_labels.clear()\n",
    "        # self.usable_props.clear()\n",
    "        self.final_depth = 0\n",
    "        self.query_str = ''\n",
    "\n",
    "    def import_grouped_info(self, input_group):\n",
    "        if input_group:\n",
    "            if type(input_group) is dict:\n",
    "                self.grouped_info = input_group\n",
    "                self.node_labels = list(self.grouped_info.keys())\n",
    "                # print(\"loaded node_labels:\",self.node_labels)\n",
    "            else:\n",
    "                print(\"input grouped info need to be dictionary type\")\n",
    "        else:\n",
    "            print(\"input grouped info cannot be empty\")\n",
    "    \n",
    "    def import_relationships(self, input_relationships):\n",
    "        if input_relationships:\n",
    "            self.relationships = input_relationships\n",
    "        else:\n",
    "            print(\"relationships cannot be empty\")\n",
    "    \n",
    "    def create_unique_alias(self, label):\n",
    "        \"\"\"Creates a unique alias for a node to prevent label overlap in queries.\"\"\"\n",
    "        base_alias = label.lower()\n",
    "        alias = base_alias\n",
    "        counter = 1\n",
    "        while alias in self.usable_labels:\n",
    "            alias = f\"{base_alias}{counter}\"\n",
    "            counter += 1\n",
    "        return alias\n",
    "    \n",
    "    def extract_alias_label(self, node_value_str):\n",
    "        # Pattern to capture both the alias and the node label\n",
    "        pattern = r\"^\\(([^:]+):([^ {]+)\"\n",
    "        match = re.search(pattern, node_value_str)\n",
    "\n",
    "        if match:\n",
    "            alias = match.group(1).strip()  # Get the alias, strip any whitespace\n",
    "            label = match.group(2).strip()  # Get the label, strip any whitespace\n",
    "            return alias, label\n",
    "        else:\n",
    "            # No match found, return None for both\n",
    "            return None, None\n",
    "\n",
    "    def add_node(self):\n",
    "        \"\"\"Adds a node with random properties selected from grouped_info.\"\"\"\n",
    "        # node_label = ''\n",
    "        if self.node_labels:\n",
    "            node_label = random.choice(self.node_labels) \n",
    "            possible_props = self.grouped_info[node_label]\n",
    "            property_label, properties_list = random.choice(list(possible_props.items()))\n",
    "            alias = self.create_unique_alias(node_label)\n",
    "            self.selected_label_alias[alias]=node_label\n",
    "\n",
    "            property_value = random.choice(properties_list)\n",
    "            properties_str = f'''{property_label}: \"{property_value}\"''' if possible_props else ''\n",
    "            node_value = f\"{node_label} {{{properties_str}}}\"\n",
    "\n",
    "\n",
    "            node = Node(f\"({alias}:{node_value})\")\n",
    "            # self.current_node.add_child(node)\n",
    "\n",
    "            # self.nodes.append(node)\n",
    "            self.usable_labels.add(alias)  # Store label for possible RETURN clause usage\n",
    "            return node \n",
    "        print(\"No node labels available. Please import grouped info first.\")\n",
    "        return None\n",
    "\n",
    "    def add_hop(self):\n",
    "        \"\"\"\n",
    "        Randomly generate hops as condition to relationship based on a customizable possibility;\n",
    "        the default possibility is 0.2\n",
    "        \"\"\"\n",
    "        current_depth = self.dm.depth\n",
    "        hop = random.randint(1,3) #TODO: see if this is reasonable\n",
    "        upper_hop = hop + random.randint(1,5)\n",
    "        exact_hop = f\"*{hop}\"\n",
    "        ceiling_hop = f\"*..{upper_hop}\"\n",
    "        floor_hop = f\"*{hop}..\"\n",
    "        interval_hop = f\"*{hop}..{upper_hop}\"\n",
    "        hop_choices = [exact_hop, ceiling_hop, floor_hop, interval_hop]\n",
    "        if current_depth < self.dm._max_depth: #random.random() > hop_p and \n",
    "            hop_choice = random.choice(hop_choices)\n",
    "            return hop_choice\n",
    "        else:\n",
    "            return ''\n",
    "\n",
    "\n",
    "\n",
    "    def add_relationship(self, bi_dir_p=0.3, rev_dir_p=0.5, hop_only_p=0.2, hop_p=0.2):\n",
    "        \"\"\" \n",
    "        Randomly generate a relationship between two nodes \n",
    "        bi_dir: probability of getting a bidirectional direction\n",
    "        rev_dir_p: probability of getting a reversed direction\n",
    "        hop_only_p: probability of getting only hops without specific relationships\n",
    "        hop_p: probability of getting hops in addition to a specific relationship\n",
    "        \"\"\"\n",
    "        current_depth = self.dm.depth\n",
    "        rel_type = random.choice(self.relationships)\n",
    "        if random.random() < bi_dir_p:\n",
    "            direction1 = \"-\"\n",
    "            direction2 = \"-\"\n",
    "        if current_depth>=3 and random.random() > rev_dir_p: \n",
    "            direction1 = \"<-\"\n",
    "            direction2 = \"-\"\n",
    "        else:\n",
    "            direction1 = \"-\" \n",
    "            direction2 = \"->\"\n",
    "        # if random.random() > hop_p:\n",
    "        \n",
    "        hop_result = Relationship(self.add_hop()) if random.random() > hop_p else ''\n",
    "        if random.random() > hop_only_p and hop_result:\n",
    "            relationship = Node(f\"{direction1} [{hop_result}] {direction2}\")\n",
    "            return Relationship(relationship, hop_only=True)\n",
    "        else:\n",
    "            relationship = Node(f\"{direction1} [:{rel_type}{hop_result}] {direction2}\")\n",
    "            return Relationship(relationship)\n",
    "        # self.current_node.add_child(relationship)\n",
    "        # return relationship\n",
    "        # return Relationship(relationship, hop_only)\n",
    "        \n",
    "    @depth_control\n",
    "    def add_condition(self, children_source, where_p=0.5, for_ea=False):\n",
    "        \"\"\"\n",
    "        Randomly generate WHERE clause based on a customizable possibility;\n",
    "        Will add to a random node as its child (no)\n",
    "        the default possibility where_p is 0.5;\n",
    "        currently only accepts and will only generate values that are str type properties\n",
    "        children_source: list of children. If from tree_population, should be tree.children, or other list type.\n",
    "        \"\"\"\n",
    "        if for_ea == True:\n",
    "            current_depth = 0 #to make sure as long as WHERE is found in previous query, it will be replaced only by chance and not depth\n",
    "        else:\n",
    "            current_depth = self.dm.depth\n",
    "        if random.random() > where_p and current_depth < self.dm._max_depth:\n",
    "            \n",
    "            np_children = np.array(children_source,dtype=object)\n",
    "            is_node = np.vectorize(lambda x: isinstance(x, Node))\n",
    "            # Apply the function to the numpy array\n",
    "            node_checks = is_node(np_children)\n",
    "            node_idx = np.where(node_checks)\n",
    "            node_children = np_children[node_idx]\n",
    "            random_node = random.choice(node_children)\n",
    "\n",
    "            alias, node_label = self.extract_alias_label(random_node.value)\n",
    "            # alias, node_label = random.choice(list(self.selected_label_alias.items()))\n",
    "            # print(alias, node_label)\n",
    "\n",
    "            # selected_node_label = random.choice(selected_node_labels)\n",
    "            possible_properties = self.grouped_info[node_label.strip()]\n",
    "            if possible_properties:\n",
    "                property_label, properties_list = random.choice(list(possible_properties.items()))\n",
    "                sample_prop_type = properties_list[0]\n",
    "                # value = random.randint(20, 50) if isinstance(sample_prop_type, int) else random.choice(properties_list) \n",
    "                value = random.choice(properties_list) #TODO: generalize to other data type\n",
    "            #TODO: customize the int part\n",
    "\n",
    "                operator = random.choice([\">\", \"<\", \"=\", \"<=\", \">=\"]) if isinstance(sample_prop_type, int) else '='\n",
    "                # grandchild = Condition(\"WHERE\", [Condition(f\"{alias}.{property_label} {operator} '{value}'\")])\n",
    "                # random_node.add_child(grandchild)\n",
    "                # print(\"Added WHERE clause to node\", random_node)\n",
    "                return Condition(\"WHERE\", [Clause(f'''{alias}.{property_label} {operator} \"{value}\"''', [])])\n",
    "            else:\n",
    "                raise ValueError(\"No available properties for the label selected:\", {node_label})\n",
    "        return \n",
    "    \n",
    "    @staticmethod\n",
    "    def is_relationship(part):\n",
    "        \"\"\"\n",
    "        Determine if the given part of a query is a relationship based on containing \"[]\"\n",
    "        Ensures that part is a string before checking.\n",
    "        \"\"\"\n",
    "        # pattern = re.compile(r'\\[(.*?)\\]')\n",
    "        trying = r\"-\\s*\\[:?([A-Za-z0-9_]+)?(\\*\\d*(\\.\\.\\d*)?)?\\]\\s*[-<>]?\"\n",
    "        # Ensure part is a string or bytes-like object\n",
    "        if isinstance(part,str):\n",
    "            # if pattern.search(part):\n",
    "            if re.search(trying, part):\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        else:   \n",
    "            print(\"input has to be str!\")\n",
    "            return None\n",
    "        \n",
    "\n",
    "    \n",
    "    def get_usable_labels(self):\n",
    "        return list(self.usable_labels)\n",
    "    \n",
    "    def add_return(self, return_num=None):\n",
    "\n",
    "        # print(\"selected_label_alias:\", self.selected_label_alias)\n",
    "\n",
    "        selected_alias = list(self.selected_label_alias.keys())\n",
    "        selected_node_labels = list(self.selected_label_alias.values())\n",
    "        \n",
    "        if return_num:\n",
    "            random_k = random.randint(1,return_num)\n",
    "        else:\n",
    "            random_k = random.randint(1,len(selected_alias))\n",
    "           \n",
    "        # print(\"selected_node_labels:\",selected_node_labels)\n",
    "        # choices = random.sample(self.usable_labels, random_k)\n",
    "        random_indices = random.sample(range(len(selected_node_labels)), random_k)\n",
    "        return_list = []\n",
    "        for i in random_indices:\n",
    "            current_alias = selected_alias[i]\n",
    "            current_label = selected_node_labels[i]\n",
    "            # print(\"type of current_label:\", repr(current_label), type(current_label))\n",
    "            # print(\"check if current_label is in self.node_labels\", repr(self.node_labels), current_label in self.node_labels)\n",
    "            current_possible_properties = self.grouped_info[str(current_label).strip()]\n",
    "            if current_possible_properties:\n",
    "                property_label = random.choice(list(current_possible_properties.keys()))\n",
    "                current_return = Clause(f\"{current_alias}.{property_label}\")\n",
    "                return_list.append(current_return)\n",
    "\n",
    "        return Clause(\"RETURN\", return_list)\n",
    "        # return None\n",
    "    \n",
    "    def parts_to_str(self):\n",
    "        \"\"\"\n",
    "        convert the generate query tree into query string with ; separation to get ready for querying the Memgraph client\n",
    "        \"\"\"\n",
    "        final_query_str = 'MATCH'\n",
    "        for part in self.parts:\n",
    "            final_query_str = final_query_str + ' ' + str(part)\n",
    "        final_query_str += ';'\n",
    "        return final_query_str\n",
    "\n",
    "\n",
    "    \n",
    "    ### FOR CROSSOVER RETURN ADJUSTMENT\n",
    "\n",
    "    def collect_alias_labels(self, tree):\n",
    "        \"\"\" Recursively collect labels from the tree that are usable in the RETURN clause. \"\"\"\n",
    "        if isinstance(tree, TreeNode) and isinstance(tree.children, list):\n",
    "            for child in tree.children:\n",
    "                # Extract label from the current node's value and add it to usable labels\n",
    "                child_value = str(child.value)\n",
    "                # print(child_value, type(child_value))\n",
    "                # label = self.extract_node_alias(child_value)\n",
    "                alias, label = self.extract_alias_label(child_value)\n",
    "                if alias and label:\n",
    "                    self.selected_label_alias[alias] = label\n",
    "                    # self.usable_labels.add(label)\n",
    "            # Recursively process each child\n",
    "                # self.collect_labels(child)\n",
    "                \n",
    "    \n",
    "    def adjust_return(self, tree):\n",
    "        \"\"\" Adjust the RETURN clause based on the labels collected from the tree. \"\"\"\n",
    "        if not isinstance(tree, TreeNode):\n",
    "            raise TypeError(\"Expected a tree that is TreeNode instance\")\n",
    "        # Clear existing labels and recollect from the new tree structure\n",
    "        # self.usable_labels.clear() \n",
    "        self.selected_label_alias = {}\n",
    "        self.collect_alias_labels(tree)\n",
    "        \n",
    "        if self.selected_label_alias:\n",
    "            # random_k = random.randint(1, len(self.usable_labels))\n",
    "            # choices = random.sample(self.usable_labels, random_k)\n",
    "            new_return = self.add_return()\n",
    "            \n",
    "            if tree.children and isinstance(tree.children[-1], TreeNode) and \"RETURN\" in str(tree.children[-1].value):\n",
    "                tree.children[-1] = new_return  # Replace the last child with the new RETURN clause\n",
    "            else:\n",
    "                tree.add_child(new_return)  # Add new if no RETURN exists\n",
    "            # print(\"updated return:\",tree.children[-1])\n",
    "            return tree\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    \n",
    "    def generate_query(self, flag=True, return_num=None, part_num=None, hop_p=0.5, where_p=0.5):\n",
    "        self.reset_per_gen()\n",
    "        self.dm.reset_depth()\n",
    "        def alternate_functions(flag):\n",
    "            if flag:\n",
    "                return self.add_node(), not flag\n",
    "            else:\n",
    "                return self.add_relationship(hop_p), not flag\n",
    "        if part_num is None:\n",
    "            part_num = random.randint(1, self.dm._max_depth-2)\n",
    "        # Keep adding nodes and relationships while depth is within limit\n",
    "        for _ in range(part_num+1):\n",
    "            part, flag = alternate_functions(flag)\n",
    "            if part is None:\n",
    "                break\n",
    "            self.parts.append(part)\n",
    "            # self.current_node.add_child(TreeNode(part))\n",
    "            self.current_node.add_child(part)\n",
    "        if self.parts and self.is_relationship(str(self.parts[-1]))==True: #ensure the input part is in string format\n",
    "            final_node = self.add_node()  # Generate a final node\n",
    "            if final_node:\n",
    "                self.parts.append(final_node)\n",
    "                # print(\"final_node added:\", final_node)\n",
    "                # self.current_node.add_child(TreeNode(final_node))\n",
    "                self.current_node.add_child(final_node)\n",
    "        # Optionally add a WHERE clause to a random node if depth is still under max_depth\n",
    "        condition = self.add_condition(self.current_node.children, where_p) \n",
    "        if condition:\n",
    "            self.parts.append(condition)\n",
    "            self.current_node.add_child(condition)\n",
    "           \n",
    "        # Add RETURN clause \n",
    "        ret = self.add_return(return_num)\n",
    "        if ret:\n",
    "            self.parts.append(ret)\n",
    "            self.current_node.add_child(ret)\n",
    "\n",
    "        self.query_str = self.parts_to_str()\n",
    "        self.current_node.depth = self.dm.depth\n",
    "        return self.current_node, self.query_str #return the treenode type and string type of query\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next building workflow:\n",
    "- function to convert resulting tree nodes into working queries (done)\n",
    "- write them into test_queries.txt file (done)\n",
    "- then apply the bash file & convertion function to run the queries & store as readable csv (done)\n",
    "- write a basic scoring/evaluation function on depth (done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitness function basics buildup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EA try\n",
    "- initialize population with cutomizable size and max depth\n",
    "- all queried through mgconsole and only maintained and added to the population if it returns result\n",
    "- fitness function to evaluate initial population\n",
    "\n",
    "Questions\n",
    "- Do I need to make sure all initial queries must work?\n",
    "- generally how customizable we want EA to be? and how do we usually achieve such flexibility in adjusting parameters? through functions or...?\n",
    "- is the current method of initializing population \"wise\" enough?\n",
    "- \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mgclient\n",
    "import threading\n",
    "import concurrent.futures\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "class DatabaseConn:\n",
    "    def __init__(self, host, port):\n",
    "        self.host = host\n",
    "        self.port = port\n",
    "        # Create a thread-local data storage\n",
    "        self.local = threading.local()\n",
    "\n",
    "    def get_connection(self):\n",
    "        \"\"\"\n",
    "        Retrieve or establish a database connection for the current thread.\n",
    "        \"\"\"\n",
    "        if not hasattr(self.local, 'conn'):\n",
    "            self.local.conn = mgclient.connect(host=self.host, port=self.port)\n",
    "            self.local.conn.autocommit = True\n",
    "        return self.local.conn\n",
    "\n",
    "    def execute_query(self, query):\n",
    "        \"\"\"\n",
    "        Execute a given query using a thread-specific database connection.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            conn = self.get_connection()\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute(query)\n",
    "            results = cursor.fetchall()\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to execute query {query}: {str(e)}\")\n",
    "            return e\n",
    "        finally:\n",
    "            if 'cursor' in locals():\n",
    "                cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import mgclient\n",
    "import threading\n",
    "import concurrent.futures\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "conn = mgclient.connect(host='127.0.0.1', port=7687)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "class EvolutionaryAlgorithm:\n",
    "    def __init__(self, qm, depth_manager, population_size, max_depth, max_generation, mut_rate=0.05):\n",
    "        self.population_size = population_size\n",
    "        self.max_depth = max_depth\n",
    "        self.tree_population = []\n",
    "        self.str_population = []\n",
    "        self.fitness_scores = {}\n",
    "        self.query_ids = {}\n",
    "        self.observed_depths = set() #TODO: clear this set after each initializED population\n",
    "        self.qm = qm\n",
    "        self.depth_manager = depth_manager\n",
    "        self.depth_manager.set_max_depth(self.max_depth)\n",
    "        # self.database_conn = database_conn\n",
    "        self.generation = 0\n",
    "        self.max_generation = max_generation\n",
    "        self.mut_rate = mut_rate\n",
    "\n",
    "        # self.host = host\n",
    "        # self.port = port\n",
    "\n",
    "        self.valid_queries = [] #store queries with score = 2 across all generations\n",
    "\n",
    "            \n",
    "    def population_to_query_list(self, population):\n",
    "        return [tree.to_querystr() for tree in population]\n",
    "\n",
    "        \n",
    "    def evaluate_population(self, max_workers=5):\n",
    "        \"\"\" Evaluates the entire population and updates fitness scores. \"\"\"\n",
    "        self.generation += 1 \n",
    "        # query_list = self.population_to_query_list(self.tree_population)\n",
    "        successful_indices, failed_indices = self.run_queries_multithreaded(tree_population=self.tree_population, max_workers=max_workers)\n",
    "        for index, tree in enumerate(self.tree_population):\n",
    "            if index in successful_indices:\n",
    "                tree.score += 1\n",
    "                self.valid_queries.append(tree)\n",
    "            elif index in failed_indices:\n",
    "                tree.score -= 1\n",
    "\n",
    "\n",
    "    def initialize_population(self): #TODO: change to generate till size is satisfied\n",
    "        \"\"\" Initializes the population with random depth queries. \"\"\"\n",
    "        for _ in range(self.population_size):\n",
    "            # self.depth_manager.set_max_depth(self.max_depth)\n",
    "            tree, query = self.qm.generate_query()\n",
    "            self.tree_population.append(tree)\n",
    "            self.str_population.append(query)\n",
    "            self.qm.reset_per_gen()\n",
    "\n",
    "    # def test_initialize_population(self):\n",
    "    #     \"\"\" Initializes the population with random depth queries that return results. \"\"\"  \n",
    "    #     # Generate queries until the population size is satisfied\n",
    "    #     while len(self.tree_population) < self.population_size:\n",
    "    #         tree, query = self.qm.generate_query()\n",
    "    #         if self.is_valid_query(cursor, query):\n",
    "    #             self.tree_population.append(tree)\n",
    "    #             self.str_population.append(query)\n",
    "    #         self.qm.reset_per_gen()\n",
    "\n",
    "    #     cursor.close()\n",
    "    #     conn.close()\n",
    "\n",
    "    def is_valid_query(self, cursor, query):\n",
    "        \"\"\" Execute the query and check if it returns any results. \"\"\"\n",
    "        try:\n",
    "            cursor.execute(query)\n",
    "            results = cursor.fetchall()\n",
    "            return len(results) > 0  # Return True if there are results\n",
    "        except Exception as e:\n",
    "            print(f\"Query failed: {e}\")\n",
    "            return False  # Query failed or returned no results\n",
    "\n",
    "\n",
    "    def tournament_parent_selection(self, k: int = None):\n",
    "        \"\"\"\n",
    "        Selects the fittest individual from a random sample of the population using a tournament selection approach.\n",
    "\n",
    "        Parameters:\n",
    "        - k (int, optional): The number of individuals to sample for the tournament. Defaults to half the population size.\n",
    "\n",
    "        Returns:\n",
    "        - The fittest individual from the sampled tournament.\n",
    "        \"\"\"\n",
    "        if k is None:\n",
    "            k = self.population_size // 2\n",
    "        if k > len(self.tree_population):\n",
    "            raise ValueError(\"Sample size k cannot be larger than the population size.\")\n",
    "        tournament = random.sample(self.tree_population, k)\n",
    "        fittest = max(tournament, key=lambda individual: self.fitness_scores[self.query_ids[individual]])\n",
    "        return fittest\n",
    "\n",
    "    def select_parents(self, num_pairs, k):\n",
    "        \"\"\"\n",
    "        Selects pairs of parents for reproduction, ensuring parents within a pair would not repeat.\n",
    "\n",
    "        Parameters:\n",
    "        - num_pairs (int): The number of parent pairs to select.\n",
    "\n",
    "        Returns:\n",
    "        - List of tuples, where each tuple contains two parent individuals.\n",
    "        \"\"\"\n",
    "        if num_pairs <= 0:\n",
    "            raise ValueError(\"num_pairs must be a positive integer\")\n",
    "        elif num_pairs * 2 > len(self.tree_population):\n",
    "            raise ValueError(\"Insufficient population to select the requested number of unique pairs\")\n",
    "\n",
    "        parents = []\n",
    "        parent_pairs=[]\n",
    "        # selected_individuals = set()  # Keep track of selected individuals\n",
    "\n",
    "        while len(parents) < num_pairs * 2: #and len(selected_individuals) < len(self.tree_population):\n",
    "            parent1 = self.tournament_parent_selection(k)\n",
    "            parent2 = self.tournament_parent_selection(k)\n",
    "            while parent1 == parent2:\n",
    "                parent2 = self.tournament_parent_selection(k)\n",
    "            parent_pair = (parent1,parent2)\n",
    "            parents.append(parent1)\n",
    "            parents.append(parent2)\n",
    "            parent_pairs.append(parent_pair)\n",
    "        return parent_pairs\n",
    "\n",
    "    # JUL25: implementing mutation\n",
    "    def mutate_query(self, tree):\n",
    "        \"\"\"\n",
    "        Randomly mutate either choice based on mut_rate probability below:\n",
    "        - node label of the query\n",
    "        - WHERE clause\n",
    "        And the returned tree query will also have an adjusted RETURN clause\n",
    "        \"\"\"\n",
    "        mutations = ['node_label','condition']\n",
    "        mut_type = random.choice(mutations)\n",
    "        if type(tree)==TreeNode:\n",
    "            if mut_type == 'condition':\n",
    "                # mutated_condition = self.qm.add_condition(for_ea=True)\n",
    "                for index, element in enumerate(tree.children):\n",
    "                    if 'WHERE' in str(element.value):\n",
    "                        children_source = tree.children[:index] + tree.children[index+1:]\n",
    "                        tree.children[index] = self.qm.add_condition(children_source, for_ea=True)\n",
    "                    else:\n",
    "                        mut_type = 'node_label' #ensure mutation takes place for selected queries without WHERE clause\n",
    "            if mut_type == 'node_label':\n",
    "                # mutated_node = self.qm.add_node()\n",
    "                indices = []\n",
    "                for index, element in enumerate(tree.children):\n",
    "                    if type(element)==Node:\n",
    "                        indices.append(index)\n",
    "                ind = random.choice(indices)\n",
    "                tree.children[ind] = self.qm.add_node()\n",
    "            tree = self.qm.adjust_return(tree)\n",
    "            return tree\n",
    "        else:\n",
    "            raise ValueError(\"The input tree query has to be TreeNode type!\")\n",
    "\n",
    "    def mutation(self):\n",
    "        num_to_mutate = int(len(self.tree_population) * self.mut_rate)\n",
    "        for _ in range(num_to_mutate):\n",
    "            # Randomly pick an individual to mutate\n",
    "            individual_index = random.randint(0, len(self.tree_population) - 1)\n",
    "            print(\"mutated index:\", individual_index)\n",
    "            # Perform mutation on this individual\n",
    "            self.tree_population[individual_index] = self.mutate_query(self.tree_population[individual_index])\n",
    "        return self.tree_population  \n",
    "\n",
    "    def swap(self, tree1, tree2):\n",
    "        if not tree1.children or not tree2.children:\n",
    "            print(\"One of the trees does not have children to perform swapping.\")\n",
    "            return\n",
    "        # Select random subtree indices from both trees\n",
    "        index1 = random.randint(0, len(tree1.children) - 1)\n",
    "        index2 = random.randint(0, len(tree2.children) - 1)\n",
    "\n",
    "        tree1_swap = copy.deepcopy(tree1)\n",
    "        tree2_swap = copy.deepcopy(tree2)\n",
    "\n",
    "        # Swap the subtrees\n",
    "        tree1_swap.children[index1], tree2_swap.children[index2] = \\\n",
    "            tree2_swap.children[index2], tree1_swap.children[index1]\n",
    "        print(\"Swapping completed.\")\n",
    "        return tree1_swap, tree2_swap\n",
    "\n",
    "    def one_point_crossover(self, tree1, tree2):\n",
    "        if not tree1.children or not tree2.children:\n",
    "            print(\"One of the trees does not have children to perform crossover.\")\n",
    "            return\n",
    "        #get indices that are not relationships as possible crossover point\n",
    "        node_indices1 = [index for index, child in enumerate(tree1.children) if type(child)!= Relationship]\n",
    "        node_indices2 = [index for index, child in enumerate(tree2.children) if type(child)!= Relationship]\n",
    "\n",
    "        #check node existence\n",
    "        if not node_indices1 or not node_indices2:\n",
    "            print(\"No nodes available for crossover in one or both trees.\")\n",
    "            return\n",
    "\n",
    "        #select random node indices from the filtered lists\n",
    "        index1 = random.choice(node_indices1[:-1])\n",
    "        index2 = random.choice(node_indices2[:-1])\n",
    "\n",
    "        #exchange the subtrees at these indices\n",
    "        tree1_crossover = copy.deepcopy(tree1)\n",
    "        tree2_crossover = copy.deepcopy(tree2)\n",
    "\n",
    "        tree1_crossover.children[index1:], tree2_crossover.children[index2:] = \\\n",
    "            tree2_crossover.children[index2:], tree1_crossover.children[index1:]\n",
    "        \n",
    "        #adjust RETURN clause based on exchanged trees\n",
    "        tree1_crossover = self.qm.adjust_return(tree1_crossover)\n",
    "        tree2_crossover = self.qm.adjust_return(tree2_crossover)\n",
    "\n",
    "        print(\"Crossover and Return clause adjustment completed.\")\n",
    "        return tree1_crossover, tree2_crossover\n",
    "    \n",
    "\n",
    "    def output_top_queries(self, top_n):\n",
    "        \"\"\"\n",
    "        Outputs the top N queries from the current population based on fitness scores,\n",
    "        considering depth diversity and query diversity.\n",
    "        \n",
    "        Parameters:\n",
    "        - top_n (int): Number of top queries to return.\n",
    "        \n",
    "        Returns:\n",
    "        - list: Top N queries as per the defined criteria.\n",
    "        \"\"\"\n",
    "        # Sort the population based on fitness scores\n",
    "\n",
    "        sorted_population = sorted(self.tree_population, key=lambda x: self.fitness_scores[self.query_ids[x]], reverse=True)\n",
    "        # Implement logic to ensure diversity if needed, example placeholder:\n",
    "        # diverse_population = self.ensure_diversity(sorted_population, top_n)\n",
    "        top_queries_with_scores = [(query, self.fitness_scores[self.query_ids[query]]) for query in sorted_population[:top_n]]\n",
    "        return top_queries_with_scores\n",
    "        # return diverse_population[:top_n]\n",
    "\n",
    "\n",
    "    def reset_ea(self):\n",
    "        # self.depth_manager.reset_depth_record()\n",
    "        self.observed_depths = set()\n",
    "        self.tree_population = []\n",
    "        self.str_population = []\n",
    "        self.fitness_scores = {}\n",
    "        self.query_ids = {}\n",
    "        self.generation=0\n",
    "\n",
    "\n",
    "\n",
    "#TEST WITH INDEPENDENT FUNCTIONS\n",
    "# def execute_query(query):\n",
    "#     # Create thread-local storage for database connections\n",
    "#     if not hasattr(execute_query, \"conn\"):\n",
    "#         execute_query.conn = mgclient.connect(host='127.0.0.1', port=7687)\n",
    "#         execute_query.conn.autocommit = True\n",
    "#     try:\n",
    "#         cursor = execute_query.conn.cursor()\n",
    "#         cursor.execute(query)\n",
    "#         results = cursor.fetchall()\n",
    "#         return results\n",
    "#     except Exception as e:\n",
    "#         print(f\"Failed to execute query {query}: {str(e)}\")\n",
    "#         return None\n",
    "#     finally:\n",
    "#         cursor.close()\n",
    "# def execute_query(query):\n",
    "#     # Create thread-local storage for database connections\n",
    "#     if not hasattr(execute_query, \"conn\"):\n",
    "#         execute_query.conn = mgclient.connect(host='127.0.0.1', port=7687)\n",
    "#         execute_query.conn.autocommit = True\n",
    "#     try:\n",
    "#         cursor = execute_query.conn.cursor()\n",
    "#         cursor.execute(query)\n",
    "#         results = cursor.fetchall()\n",
    "#         if not results:\n",
    "#             # No results found, return a specific value that indicates empty results\n",
    "#             return 'NoResult'\n",
    "#         else:\n",
    "#             return results\n",
    "#     except Exception as e:\n",
    "#         # print(f\"Failed to execute query {query}: {str(e)}\")\n",
    "#         return None\n",
    "#     finally:\n",
    "#         cursor.close()\n",
    "\n",
    "# def run_queries_multithreaded(query_list, max_workers=5):\n",
    "#     # Create a ThreadPoolExecutor to manage multiple threads\n",
    "#     with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "#         # Map futures to their corresponding query indices\n",
    "#         future_to_index = {executor.submit(execute_query, query): i for i, query in enumerate(query_list)}\n",
    "\n",
    "#         # Create a list to store the indices of successful queries\n",
    "#         successful_indices = []\n",
    "#         failed_indices = []\n",
    "\n",
    "#         # Collect results as they complete\n",
    "#         for future in concurrent.futures.as_completed(future_to_index):\n",
    "#             index = future_to_index[future]\n",
    "#             try:\n",
    "#                 result = future.result()\n",
    "#                 if result:\n",
    "#                     print(f\"Query at index {index} returned results.\")\n",
    "#                     successful_indices.append(index)\n",
    "#                 else:\n",
    "#                     print(f\"Query at index {index} returned no results.\")\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Query execution failed for index {index}: {str(e)}\")\n",
    "#                 failed_indices.append(index)\n",
    "\n",
    "#         return successful_indices, failed_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "databaseTest = DatabaseConn(host='127.0.0.1',port=7687)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation: 0\n"
     ]
    }
   ],
   "source": [
    "dmTest = TestDepthManager.getInstance()\n",
    "qmTest = TestQueryManager(dm=dmTest)\n",
    "qmTest.import_grouped_info(alzkb_nested_dict)  \n",
    "qmTest.import_relationships(relationships)  \n",
    "\n",
    "\n",
    "ea = EvolutionaryAlgorithm(qm=qmTest, depth_manager=dmTest, population_size=50,max_depth=4, max_generation=3)\n",
    "# ea.test_initialize_population() \n",
    "print(\"generation:\",ea.generation)\n",
    "ea.initialize_population() \n",
    "# for index, query in enumerate(ea.str_population):\n",
    "#     print(index, query)\n",
    "# ea.mutation()\n",
    "query_list = ea.population_to_query_list(ea.tree_population)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MATCH (drugclass:DrugClass {commonName: \"Corticosteroid Hormone Receptor Agonists\"}) - [:DISEASELOCALIZESTOANATOMY] -> (drugclass1:DrugClass {commonName: \"UGT1A4 Inhibitors\"}) WHERE drugclass.commonName = \"Decreased Histamine Release\" RETURN drugclass.commonName;',\n",
       " 'MATCH (cellularcomponent:CellularComponent {commonName: \"nuclear condensin complex\"}) - [*1] -> (gene:Gene {geneSymbol: \"LOC110121188\"}) WHERE gene.geneSymbol = \"LOC130065403\" RETURN gene.geneSymbol;',\n",
       " 'MATCH (bodypart:BodyPart {commonName: \"autonomic ganglion\"}) - [*3..5] -> (symptom:Symptom {commonName: \"Ageusia\"}) WHERE bodypart.commonName = \"sphenoid bone\" RETURN bodypart.commonName;',\n",
       " 'MATCH (bodypart:BodyPart {commonName: \"nervous system\"}) - [:GENEASSOCIATEDWITHCELLULARCOMPONENT] -> (disease:Disease {commonName: \"Non-Alzheimer\\'s dementia (e.g., Lewy body dementia, vascular or multi-infarct dementia; mixed dementia; frontotemporal dementia such as Pick\\'s disease; and dementia related to stroke, Parkinson\\'s or Creutzfeldt-Jakob diseases)\"}) WHERE disease.commonName = \"Lewy Body Variant of Alzheimer Disease\" RETURN bodypart.commonName;',\n",
       " 'MATCH (symptom:Symptom {commonName: \"Halitosis\"}) - [*1] -> (pathway:Pathway {commonName: \"Fosinopril Metabolism Pathway\"}) WHERE pathway.commonName = \"regulation of FZD by ubiquitination\" RETURN pathway.commonName;',\n",
       " 'MATCH (drug:Drug {commonName: \"Japanese encephalitis vaccine (live, attenuated)\"}) - [*..7] -> (symptom:Symptom {commonName: \"Aphonia\"}) RETURN symptom.commonName;',\n",
       " 'MATCH (symptom:Symptom {commonName: \"Tinea Pedis\"}) - [:BODYPARTUNDEREXPRESSESGENE] -> (disease:Disease {commonName: \"Alzheimer Disease, Familial, 3, with Spastic Paraparesis and Apraxia\"}) WHERE symptom.commonName = \"Apnea\" RETURN symptom.commonName;',\n",
       " 'MATCH (bodypart:BodyPart {commonName: \"maxilla\"}) - [*2..] -> (gene:Gene {commonName: \"-\"}) WHERE gene.geneSymbol = \"LOC125371522\" RETURN gene.geneSymbol;',\n",
       " 'MATCH (symptom:Symptom {commonName: \"Rett Syndrome\"}) - [*2] -> (gene:Gene {geneSymbol: \"FOLR2\"}) RETURN gene.commonName;',\n",
       " 'MATCH (bodypart:BodyPart {commonName: \"renal pelvis\"}) - [:GENEASSOCIATEDWITHCELLULARCOMPONENT] -> (bodypart1:BodyPart {commonName: \"maxillary artery\"}) WHERE bodypart.commonName = \"azygos vein\" RETURN bodypart1.commonName;',\n",
       " 'MATCH (pathway:Pathway {commonName: \"HS-GAG degradation\"}) - [*..5] -> (drug:Drug {commonName: \"Ilepatril\"}) RETURN pathway.commonName;',\n",
       " 'MATCH (drugclass:DrugClass {commonName: \"Sigma-1 Receptor Agonists\"}) - [*3..] -> (cellularcomponent:CellularComponent {commonName: \"transcription elongation factor complex\"}) WHERE cellularcomponent.commonName = \"peroxisomal membrane\" RETURN cellularcomponent.commonName;',\n",
       " 'MATCH (biologicalprocess:BiologicalProcess {commonName: \"regulation of muscle cell apoptotic process\"}) - [:CHEMICALDECREASESEXPRESSION] -> (drugclass:DrugClass {commonName: \"Cytochrome P450 1A2 Inducers\"}) WHERE drugclass.commonName = \"Genitourinary Arterial Vasodilation\" RETURN biologicalprocess.commonName;',\n",
       " 'MATCH (drugclass:DrugClass {commonName: \"Thyroxine\"}) - [*3..] -> (disease:Disease {commonName: \"Prodromal Alzheimer\\'s disease\"}) WHERE drugclass.commonName = \"Decreased Blood Pressure\" RETURN drugclass.commonName;',\n",
       " 'MATCH (pathway:Pathway {commonName: \"IRAK2 mediated activation of TAK1 complex upon TLR7/8 or 9 stimulation\"}) - [*1] -> (molecularfunction:MolecularFunction {commonName: \"1-acylglycerol-3-phosphate O-acyltransferase activity\"}) RETURN molecularfunction.commonName;',\n",
       " 'MATCH (biologicalprocess:BiologicalProcess {commonName: \"rhombomere 3 morphogenesis\"}) - [*2..] -> (symptom:Symptom {commonName: \"Night Terrors\"}) WHERE biologicalprocess.commonName = \"positive regulation of catenin import into nucleus\" RETURN biologicalprocess.commonName;',\n",
       " 'MATCH (gene:Gene {commonName: \"-\"}) - [:GENEASSOCIATEDWITHCELLULARCOMPONENT] -> (molecularfunction:MolecularFunction {commonName: \"complement component C3b binding\"}) RETURN gene.geneSymbol;',\n",
       " 'MATCH (drugclass:DrugClass {commonName: \"Neurokinin 1 Antagonists\"}) - [*..7] -> (symptom:Symptom {commonName: \"Pseudobulbar Palsy\"}) RETURN symptom.commonName;',\n",
       " 'MATCH (disease:Disease {commonName: \"Prodromal Alzheimer\\'s disease\"}) - [*1..] -> (drug:Drug {commonName: \"2-Aminobenzyl alcohol\"}) WHERE disease.commonName = \"Alzheimer Disease, Familial, 3, with Spastic Paraparesis and Unusual Plaques\" RETURN drug.commonName;',\n",
       " 'MATCH (disease:Disease {commonName: \"Lewy Body Variant of Alzheimer Disease\"}) - [*3..7] -> (pathway:Pathway {commonName: \"RUNX3 regulates NOTCH signaling\"}) WHERE disease.commonName = \"Non-Alzheimer\\'s dementia (e.g., Lewy body dementia, vascular or multi-infarct dementia; mixed dementia; frontotemporal dementia such as Pick\\'s disease; and dementia related to stroke, Parkinson\\'s or Creutzfeldt-Jakob diseases)\" RETURN pathway.commonName;',\n",
       " 'MATCH (gene:Gene {commonName: \"-\"}) - [*3..] -> (pathway:Pathway {commonName: \"Cargo trafficking to the periciliary membrane\"}) WHERE gene.geneSymbol = \"LOC129999684\" RETURN gene.geneSymbol;',\n",
       " 'MATCH (bodypart:BodyPart {commonName: \"cardiac septum\"}) - [*..3] -> (gene:Gene {geneSymbol: \"LINC01467\"}) RETURN bodypart.commonName;',\n",
       " 'MATCH (molecularfunction:MolecularFunction {commonName: \"brain-derived neurotrophic factor binding\"}) - [*..8] -> (disease:Disease {commonName: \"Familial Alzheimer Disease (FAD)\"}) WHERE disease.commonName = \"Alzheimer Disease, Early Onset\" RETURN disease.commonName;',\n",
       " 'MATCH (pathway:Pathway {commonName: \"Tight junction - Homo sapiens (human)\"}) - [*..3] -> (drug:Drug {commonName: \"Prenoxdiazine\"}) RETURN drug.commonName;',\n",
       " 'MATCH (disease:Disease {commonName: \"Alzheimer Disease 9\"}) - [:CHEMICALINCREASESEXPRESSION] -> (cellularcomponent:CellularComponent {commonName: \"sperm cytoplasmic droplet\"}) WHERE cellularcomponent.commonName = \"actomyosin\" RETURN disease.commonName;',\n",
       " 'MATCH (symptom:Symptom {commonName: \"Hypergammaglobulinemia\"}) - [:SYMPTOMMANIFESTATIONOFDISEASE] -> (cellularcomponent:CellularComponent {commonName: \"apicolateral plasma membrane\"}) RETURN cellularcomponent.commonName;',\n",
       " 'MATCH (drug:Drug {commonName: \"AB-8939\"}) - [:SYMPTOMMANIFESTATIONOFDISEASE] -> (gene:Gene {geneSymbol: \"LOC127270534\"}) RETURN drug.commonName;',\n",
       " 'MATCH (bodypart:BodyPart {commonName: \"carpal region\"}) - [*..7] -> (drugclass:DrugClass {commonName: \"Aminosalicylic Acids\"}) WHERE drugclass.commonName = \"Decreased Cell Wall Integrity\" RETURN drugclass.commonName;',\n",
       " 'MATCH (gene:Gene {geneSymbol: \"LOC127818665\"}) - [*2] -> (disease:Disease {commonName: \"Alzheimer\\'s Disease\"}) WHERE gene.commonName = \"long intergenic non-protein coding RNA 2452\" RETURN disease.commonName;',\n",
       " 'MATCH (disease:Disease {commonName: \"Non-Alzheimer\\'s dementia (e.g., Lewy body dementia, vascular or multi-infarct dementia; mixed dementia; frontotemporal dementia such as Pick\\'s disease; and dementia related to stroke, Parkinson\\'s or Creutzfeldt-Jakob diseases)\"}) - [*2] -> (drug:Drug {commonName: \"Pranlukast\"}) RETURN drug.commonName;',\n",
       " 'MATCH (biologicalprocess:BiologicalProcess {commonName: \"negative regulation of muscle hyperplasia\"}) - [*..8] -> (gene:Gene {geneSymbol: \"LINC02646\"}) WHERE gene.geneSymbol = \"HES7\" RETURN biologicalprocess.commonName;',\n",
       " 'MATCH (disease:Disease {commonName: \"Prodromal Alzheimer\\'s disease\"}) - [*2..6] -> (disease1:Disease {commonName: \"Alzheimer Disease, Familial, 3, with Spastic Paraparesis and Unusual Plaques\"}) RETURN disease.commonName;',\n",
       " 'MATCH (pathway:Pathway {commonName: \"Phase I biotransformations, non P450\"}) - [:BODYPARTUNDEREXPRESSESGENE] -> (pathway1:Pathway {commonName: \"PI3K-Akt signaling pathway - Homo sapiens (human)\"}) WHERE pathway1.commonName = \"CXCR4-mediated signaling events\" RETURN pathway.commonName;',\n",
       " 'MATCH (bodypart:BodyPart {commonName: \"telencephalic ventricle\"}) - [*..8] -> (biologicalprocess:BiologicalProcess {commonName: \"mesenchyme migration\"}) WHERE biologicalprocess.commonName = \"regulation of fertilization\" RETURN biologicalprocess.commonName;',\n",
       " 'MATCH (bodypart:BodyPart {commonName: \"sesamoid bone\"}) - [:GENEASSOCIATESWITHDISEASE] -> (drugclass:DrugClass {commonName: \"Aminosalicylic Acids\"}) WHERE bodypart.commonName = \"superior mesenteric artery\" RETURN bodypart.commonName;',\n",
       " 'MATCH (symptom:Symptom {commonName: \"Auditory Perceptual Disorders\"}) - [*1..] -> (gene:Gene {commonName: \"-\"}) RETURN symptom.commonName;',\n",
       " 'MATCH (bodypart:BodyPart {commonName: \"pleura\"}) - [*3..4] -> (drug:Drug {commonName: \"N-BENZYLOXYCARBONYL-ALA-PRO-3-AMINO-4-PHENYL-BUTAN-2-OL\"}) WHERE drug.commonName = \"Methimazole\" RETURN drug.commonName;',\n",
       " 'MATCH (gene:Gene {geneSymbol: \"UHRF2\"}) - [*1..6] -> (symptom:Symptom {commonName: \"Hypokinesia\"}) RETURN gene.geneSymbol;',\n",
       " 'MATCH (drugclass:DrugClass {commonName: \"Serotonin 1d Receptor Agonists\"}) - [*3..8] -> (bodypart:BodyPart {commonName: \"clavicle bone\"}) WHERE bodypart.commonName = \"azygos vein\" RETURN drugclass.commonName;',\n",
       " 'MATCH (drug:Drug {commonName: \"Phosphorylisopropane\"}) - [:DRUGCAUSESEFFECT] -> (pathway:Pathway {commonName: \"XAV939 inhibits tankyrase, stabilizing AXIN\"}) RETURN drug.commonName;',\n",
       " 'MATCH (biologicalprocess:BiologicalProcess {commonName: \"peptidyl-serine dephosphorylation\"}) - [:DISEASEASSOCIATESWITHDISEASE*..5] -> (bodypart:BodyPart {commonName: \"myometrium\"}) RETURN biologicalprocess.commonName;',\n",
       " 'MATCH (drugclass:DrugClass {commonName: \"M2 Protein Inhibitors\"}) - [*3..4] -> (pathway:Pathway {commonName: \"y branching of actin filaments\"}) RETURN drugclass.commonName;',\n",
       " 'MATCH (bodypart:BodyPart {commonName: \"cranial suture\"}) - [*..5] -> (biologicalprocess:BiologicalProcess {commonName: \"osteoclast development\"}) WHERE biologicalprocess.commonName = \"trachea morphogenesis\" RETURN bodypart.commonName;',\n",
       " 'MATCH (drug:Drug {commonName: \"Gusacitinib\"}) - [*2] -> (pathway:Pathway {commonName: \"HTLV-I infection - Homo sapiens (human)\"}) RETURN pathway.commonName;',\n",
       " 'MATCH (molecularfunction:MolecularFunction {commonName: \"MHC class I protein complex binding\"}) - [*..4] -> (bodypart:BodyPart {commonName: \"optic chiasma\"}) RETURN bodypart.commonName;',\n",
       " 'MATCH (drugclass:DrugClass {commonName: \"beta Lactamase Inhibitors\"}) - [:GENEINTERACTSWITHGENE] -> (gene:Gene {geneSymbol: \"LOC130005287\"}) WHERE drugclass.commonName = \"Cytochrome P450 2C9 Inducers\" RETURN drugclass.commonName;',\n",
       " 'MATCH (disease:Disease {commonName: \"Prodromal Alzheimer\\'s disease\"}) - [*2..] -> (gene:Gene {geneSymbol: \"PPIAP1\"}) RETURN gene.geneSymbol;',\n",
       " 'MATCH (symptom:Symptom {commonName: \"Tremor\"}) - [*..5] -> (drugclass:DrugClass {commonName: \"Decreased Cholesterol Absorption\"}) WHERE drugclass.commonName = \"Partial Cholinergic Nicotinic Agonists\" RETURN symptom.commonName;',\n",
       " 'MATCH (symptom:Symptom {commonName: \"Hearing Loss, Noise-Induced\"}) - [*2..] -> (disease:Disease {commonName: \"Lewy Body Variant of Alzheimer Disease\"}) WHERE disease.commonName = \"Familial Alzheimer\\'s disease of early onset\" RETURN symptom.commonName;',\n",
       " 'MATCH (drug:Drug {commonName: \"Remifentanil\"}) - [*..4] -> (bodypart:BodyPart {commonName: \"radial nerve\"}) WHERE bodypart.commonName = \"brachial nerve plexus\" RETURN bodypart.commonName;']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Memgraph and Execute Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First half: connect to memgraph console mgconsole\n",
    "- Need docker installation following notion notes; paste them here later\n",
    "- But the bash file will handle the querying so no actual terminal operation needed from the user side"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second half: query execution\n",
    "- because of the csv format glich of memgraph, the bash file **single_query_run.s**h** first execute each individual generated query in the **test_queries.txt**\n",
    "- the results are stored in the outputs folder in fake csv format\n",
    "- a function converting and aggregating the results into readable, interpretable csv is then applied to obtain a **aggregated_results.csv**\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1. write 10 queries into a txt file\\n2. execute\\n3. write another 10 queries into txt, execute ... till all are executed -- DONE\\n4. grep results from aggregated csv with indices. --DONE\\n    if \"no_result\", no change;\\n    if error, corresponding query.score = 0\\n    if return results, corresponding query.score = 2 \\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "1. write 10 queries into a txt file\n",
    "2. execute\n",
    "3. write another 10 queries into txt, execute ... till all are executed -- DONE\n",
    "4. grep results from aggregated csv with indices. --DONE\n",
    "    if \"no_result\", no change;\n",
    "    if error, corresponding query.score = 0\n",
    "    if return results, corresponding query.score = 2 \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import subprocess\n",
    "from multiprocessing import Pool\n",
    "from test_multiprocessing import create_batch, execute_script, aggregate_text_files, merge_csv_files, get_indices\n",
    "########################################\n",
    "#TESTING WITH MULTIPROCESSING\n",
    "# def process_batches(batch_folder):\n",
    "#     # Find all batch files\n",
    "#     batch_files = [os.path.join(batch_folder, f) for f in os.listdir(batch_folder) if f.startswith('batch') and f.endswith('.txt')]\n",
    "#     # Ensure the script is executable\n",
    "#     subprocess.run(['chmod', '+x', 'test_query_run.sh'])\n",
    "    \n",
    "#     # Process each batch in parallel\n",
    "#     with Pool(processes=os.cpu_count()) as pool:\n",
    "#         pool.map(execute_script, batch_files)\n",
    "\n",
    "#     # Aggregate results\n",
    "#     folder_path = './outputs'\n",
    "#     output_file = './aggregates/results.csv'\n",
    "#     aggregate_text_files(folder_path, output_file)\n",
    "\n",
    "# # Merge CSV files if needed\n",
    "# merge_csv_files('aggregates', 'multiproc_merged_output.csv')\n",
    "\n",
    "# # Get indices of successful and failed queries\n",
    "# get_indices('multiproc_merged_output.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Batch processed.\n",
      "Aggregated to ./aggregates/results3.csv\n",
      "Current Batch processed.\n",
      "Aggregated to ./aggregates/results4.csv\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Pool(processes\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mcpu_count()) \u001b[38;5;28;01mas\u001b[39;00m pool:\n\u001b[1;32m     11\u001b[0m     batch_files \u001b[38;5;241m=\u001b[39m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(batch_folder, f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(batch_folder) \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[0;32m---> 12\u001b[0m     \u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstarmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_files\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/multiprocessing/pool.py:372\u001b[0m, in \u001b[0;36mPool.starmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstarmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, iterable, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    367\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;124;03m    Like `map()` method but the elements of the `iterable` are expected to\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;124;03m    be iterables as well and will be unpacked as arguments. Hence\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;124;03m    `func` and (a, b) becomes func(a, b).\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 372\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstarmapstar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/multiprocessing/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    766\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mready():\n\u001b[1;32m    767\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/multiprocessing/pool.py:762\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 762\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/threading.py:574\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    572\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 574\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed query: MATCH (gene:Gene {commonName: \"-\"}) - [*3..] -> (pathway:Pathway {commonName: \"Cargo trafficking to the periciliary membrane\"}) WHERE gene.geneSymbol = \"LOC129999684\" RETURN gene.geneSymbol\n",
      "Client received query exception: Transaction was asked to abort because of transaction timeout.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query execution failed, writing 'failed' to ./outputs/1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed query: MATCH (symptom:Symptom {commonName: \"Auditory Perceptual Disorders\"}) - [*1..] -> (gene:Gene {commonName: \"-\"}) RETURN symptom.commonName\n",
      "Client received query exception: Transaction was asked to abort because of transaction timeout.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query execution failed, writing 'failed' to ./outputs/6.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed query: MATCH (disease:Disease {commonName: \"Lewy Body Variant of Alzheimer Disease\"}) - [*3..7] -> (pathway:Pathway {commonName: \"RUNX3 regulates NOTCH signaling\"}) WHERE disease.commonName = \"Non-Alzheimer's dementia (e.g., Lewy body dementia, vascular or multi-infarct dementia; mixed dementia; frontotemporal dementia such as Pick's disease; and dementia related to stroke, Parkinson's or Creutzfeldt-Jakob diseases)\" RETURN pathway.commonName\n",
      "Client received query exception: Transaction was asked to abort because of transaction timeout.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query execution failed, writing 'failed' to ./outputs/10.csv\n",
      "Current Batch processed.\n",
      "Current Batch processed.\n",
      "Current Batch processed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed query: MATCH (gene:Gene {commonName: \"-\"}) - [*3..] -> (pathway:Pathway {commonName: \"Cargo trafficking to the periciliary membrane\"}) WHERE gene.geneSymbol = \"LOC129999684\" RETURN gene.geneSymbol\n",
      "Client received query exception: Transaction was asked to abort because of transaction timeout.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query execution failed, writing 'failed' to ./outputs/1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed query: MATCH (symptom:Symptom {commonName: \"Auditory Perceptual Disorders\"}) - [*1..] -> (gene:Gene {commonName: \"-\"}) RETURN symptom.commonName\n",
      "Client received query exception: Transaction was asked to abort because of transaction timeout.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query execution failed, writing 'failed' to ./outputs/6.csv\n",
      "Current Batch processed.\n",
      "Current Batch processed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed query: MATCH (disease:Disease {commonName: \"Lewy Body Variant of Alzheimer Disease\"}) - [*3..7] -> (pathway:Pathway {commonName: \"RUNX3 regulates NOTCH signaling\"}) WHERE disease.commonName = \"Non-Alzheimer's dementia (e.g., Lewy body dementia, vascular or multi-infarct dementia; mixed dementia; frontotemporal dementia such as Pick's disease; and dementia related to stroke, Parkinson's or Creutzfeldt-Jakob diseases)\" RETURN pathway.commonName\n",
      "Client received query exception: Transaction was asked to abort because of transaction timeout.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query execution failed, writing 'failed' to ./outputs/10.csv\n",
      "Current Batch processed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "from multiprocessing import Pool\n",
    "from test_multiprocessing import create_batch, process_batch\n",
    "\n",
    "# create_batch(query_list, batch_size=10)\n",
    "if __name__ == \"__main__\":\n",
    "    batch_folder = './input_batch'\n",
    "    with Pool(processes=os.cpu_count()) as pool:\n",
    "        batch_files = [os.path.join(batch_folder, f) for f in os.listdir(batch_folder) if f.startswith('batch')]\n",
    "        pool.starmap(process_batch, [(batch_file, i) for i, batch_file in enumerate(batch_files)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process SpawnPoolWorker-98:\n",
      "Process SpawnPoolWorker-101:\n",
      "Process SpawnPoolWorker-96:\n",
      "Process SpawnPoolWorker-102:\n",
      "Process SpawnPoolWorker-100:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yufeimeng/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/yufeimeng/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/yufeimeng/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/yufeimeng/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/yufeimeng/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/yufeimeng/opt/anaconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/Users/yufeimeng/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/yufeimeng/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/yufeimeng/opt/anaconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/Users/yufeimeng/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/yufeimeng/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/yufeimeng/opt/anaconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'process_batch' on <module '__main__' (built-in)>\n",
      "AttributeError: Can't get attribute 'process_batch' on <module '__main__' (built-in)>\n",
      "AttributeError: Can't get attribute 'process_batch' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yufeimeng/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yufeimeng/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/yufeimeng/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/yufeimeng/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/yufeimeng/opt/anaconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/Users/yufeimeng/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/yufeimeng/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/Users/yufeimeng/opt/anaconda3/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'process_batch' on <module '__main__' (built-in)>\n",
      "AttributeError: Can't get attribute 'process_batch' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [42]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Pool(processes\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mcpu_count()) \u001b[38;5;28;01mas\u001b[39;00m pool:\n\u001b[1;32m      4\u001b[0m     batch_files \u001b[38;5;241m=\u001b[39m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(batch_folder, f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(batch_folder) \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[0;32m----> 5\u001b[0m     \u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstarmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_files\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/multiprocessing/pool.py:372\u001b[0m, in \u001b[0;36mPool.starmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstarmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, iterable, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    367\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;124;03m    Like `map()` method but the elements of the `iterable` are expected to\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;124;03m    be iterables as well and will be unpacked as arguments. Hence\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;124;03m    `func` and (a, b) becomes func(a, b).\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 372\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstarmapstar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/multiprocessing/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    766\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mready():\n\u001b[1;32m    767\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/multiprocessing/pool.py:762\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 762\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/threading.py:574\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    572\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 574\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    batch_folder = './input_batch'\n",
    "    with Pool(processes=os.cpu_count()) as pool:\n",
    "        batch_files = [os.path.join(batch_folder, f) for f in os.listdir(batch_folder) if f.startswith('batch')]\n",
    "        pool.starmap(process_batch, [(batch_file, i) for i, batch_file in enumerate(batch_files)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Batch processed.\n",
      "Current Batch processed.\n",
      "Current Batch processed.\n",
      "Current Batch processed.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [30]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      2\u001b[0m     batch_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./input_batch\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m     suc_ind, fail_ind \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_batches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_folder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSuccessful indices: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msuc_ind\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed indices: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfail_ind\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Input \u001b[0;32mIn [29]\u001b[0m, in \u001b[0;36mprocess_batches\u001b[0;34m(batch_folder)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Process each batch in parallel\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Pool(processes\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mcpu_count()) \u001b[38;5;28;01mas\u001b[39;00m pool:\n\u001b[0;32m---> 16\u001b[0m     \u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecute_script\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_files\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Aggregate results\u001b[39;00m\n\u001b[1;32m     19\u001b[0m folder_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./outputs\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/multiprocessing/pool.py:364\u001b[0m, in \u001b[0;36mPool.map\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, iterable, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    360\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    Apply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;124;03m    in a list that is returned.\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapstar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/multiprocessing/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    766\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mready():\n\u001b[1;32m    767\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/multiprocessing/pool.py:762\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 762\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/threading.py:574\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    572\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 574\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    batch_folder = './input_batch'\n",
    "    suc_ind, fail_ind = process_batches(batch_folder)\n",
    "    print(f\"Successful indices: {suc_ind}\")\n",
    "    print(f\"Failed indices: {fail_ind}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged CSV saved as merged_output.csv\n"
     ]
    }
   ],
   "source": [
    "#changed result at index 5 to be something valid to test later steps\n",
    "merge_csv_files('aggregates', 'merged_output.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRY TO CONNECT TO PREVIOUS QUERY\n",
    "def get_indices(csv_dir):\n",
    "    # csv_dir_to_examine = 'merged_output.csv'\n",
    "    merged = pd.read_csv(csv_dir)\n",
    "    mask = (merged['results_returned'] != 'no_result') & (merged['results_returned'] != 'failed')\n",
    "    # mask = merged['results_returned'] != 'no_result' or 'failed'\n",
    "    failed_mask = merged['results_returned'] == 'failed'\n",
    "    successful_indices = merged.index[mask].tolist()\n",
    "    failed_indices = merged.index[failed_mask].tolist()\n",
    "    return successful_indices, failed_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed query: MATCH (bodypart:BodyPart {commonName: \"scrotum\"}) - [:CHEMICALBINDSGENE] -> (drug:Drug {commonName: \"Hydroxyethylcysteine\"}) RETURN r\n",
      "Client received query exception: Unbound variable: r.\n",
      "Query execution failed, writing 'failed' to ./outputs/1.csv\n",
      "Query 2 executed and output saved to ./outputs/2.csv\n",
      "Query 3 executed and output saved to ./outputs/3.csv\n",
      "Query 4 executed and output saved to ./outputs/4.csv\n",
      "Query 5 executed and output saved to ./outputs/5.csv\n",
      "Query 6 executed and output saved to ./outputs/6.csv\n",
      "Query 7 executed and output saved to ./outputs/7.csv\n",
      "Query 8 executed and output saved to ./outputs/8.csv\n",
      "Query 9 executed and output saved to ./outputs/9.csv\n",
      "Query 10 executed and output saved to ./outputs/10.csv\n",
      "All queries processed.\n"
     ]
    }
   ],
   "source": [
    "#test if failed query can be outputed\n",
    "!chmod +x test_query_run.sh\n",
    "!./test_query_run.sh ./input_batch/batch0.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = './outputs'\n",
    "output_file = f'./aggregates/results0.csv'\n",
    "aggregate_text_files(folder_path, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged CSV saved as test_merged_output.csv\n"
     ]
    }
   ],
   "source": [
    "merge_csv_files('aggregates', 'test_merged_output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suc_ind, fail_ind = get_indices('test_merged_output.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 15] [0]\n"
     ]
    }
   ],
   "source": [
    "print(suc_ind, fail_ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying mgclient with timeout feature for single query as a fatal feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mgclient\n",
    "import concurrent.futures\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import threading\n",
    "# conn = mgclient.connect(host='127.0.0.1', port=7687)\n",
    "# cursor = conn.cursor()\n",
    "# Define a function to connect to Memgraph and execute a single query\n",
    "def execute_query(query):\n",
    "    # Create thread-local storage for database connections\n",
    "    if not hasattr(execute_query, \"conn\"):\n",
    "        execute_query.conn = mgclient.connect(host='127.0.0.1', port=7687)\n",
    "        execute_query.conn.autocommit = True\n",
    "    try:\n",
    "        cursor = execute_query.conn.cursor()\n",
    "        cursor.execute(query)\n",
    "        results = cursor.fetchall()\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to execute query {query}: {str(e)}\")\n",
    "        return None\n",
    "    finally:\n",
    "        cursor.close()\n",
    "\n",
    "# def run_queries_multithreaded(query_list, max_workers=5):\n",
    "#     # Create a ThreadPoolExecutor to manage multiple threads\n",
    "#     with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "#         # Map futures to their corresponding query indices\n",
    "#         future_to_index = {executor.submit(execute_query, query): i for i, query in enumerate(query_list)}\n",
    "\n",
    "#         # Create a list to store the indices of successful queries\n",
    "#         successful_indices = []\n",
    "#         failed_indices = []\n",
    "\n",
    "#         # Collect results as they complete\n",
    "#         for future in concurrent.futures.as_completed(future_to_index):\n",
    "#             index = future_to_index[future]\n",
    "#             try:\n",
    "#                 result = future.result()\n",
    "#                 if result:\n",
    "#                     print(f\"Query at index {index} returned results.\")\n",
    "#                     successful_indices.append(index)\n",
    "#                 else:\n",
    "#                     print(f\"Query at index {index} returned no results.\")\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Query execution failed for index {index}: {str(e)}\")\n",
    "#                 failed_indices.append(index)\n",
    "\n",
    "#         return successful_indices, failed_indices\n",
    "\n",
    "def run_queries_multithreaded(query_list, max_workers=5, timeout=10):\n",
    "    # Create a ThreadPoolExecutor to manage multiple threads\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Map futures to their corresponding query indices\n",
    "        future_to_index = {executor.submit(execute_query, query): i for i, query in enumerate(query_list)}\n",
    "\n",
    "        # Create a list to store the indices of successful and failed queries\n",
    "        successful_indices = []\n",
    "        failed_indices = []\n",
    "\n",
    "        # Collect results as they complete\n",
    "        for future in concurrent.futures.as_completed(future_to_index, timeout=timeout):\n",
    "            index = future_to_index[future]\n",
    "            try:\n",
    "                result = future.result(timeout=timeout)  # Apply timeout for each query result collection\n",
    "                if result:\n",
    "                    print(f\"Query at index {index} returned results.\")\n",
    "                    successful_indices.append(index)\n",
    "                # else:\n",
    "                    # print(f\"Query at index {index} returned no results.\")\n",
    "                    # failed_indices.append(index)  # Treat no results as failed (or adjust as necessary)\n",
    "            except concurrent.futures.TimeoutError:\n",
    "                print(f\"Query at index {index} timed out.\")\n",
    "                failed_indices.append(index)\n",
    "            except Exception as e:\n",
    "                print(f\"Query execution failed for index {index}: {str(e)}\")\n",
    "                failed_indices.append(index)\n",
    "        return successful_indices, failed_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MATCH (drug:Drug {commonName: \"Indium In-111 oxyquinoline\"}) - [*..5] -> (pathway:Pathway {commonName: \"Human Complement System\"}) RETURN pathway.commonName;',\n",
       " 'MATCH (molecularfunction:MolecularFunction {commonName: \"thyroid hormone receptor coactivator activity\"}) - [*1] -> (drug:Drug {commonName: \"Bean\"}) RETURN drug.commonName;',\n",
       " 'MATCH (symptom:Symptom {commonName: \"Overweight\"}) - [*..3] -> (symptom1:Symptom {commonName: \"Vocal Cord Paralysis\"}) WHERE symptom.commonName = \"Angina, Unstable\" RETURN symptom1.commonName;',\n",
       " 'MATCH (disease:Disease {commonName: \"Alzheimer disease, familial, type 3\"}) - [*1] -> (drugclass:DrugClass {commonName: \"Osmotic Activity\"}) WHERE drugclass.commonName = \"Nucleic Acid Synthesis Inhibitors\" RETURN drugclass.commonName;',\n",
       " 'MATCH (cellularcomponent:CellularComponent {commonName: \"cytosolic large ribosomal subunit\"}) - [:DRUGCAUSESEFFECT] -> (pathway:Pathway {commonName: \"NADE modulates death signalling\"}) RETURN pathway.commonName;',\n",
       " 'MATCH (biologicalprocess:BiologicalProcess {commonName: \"protein-carbohydrate complex subunit organization\"}) - [:DRUGCAUSESEFFECT] -> (biologicalprocess1:BiologicalProcess {commonName: \"regulation of cortisol secretion\"}) RETURN biologicalprocess1.commonName;',\n",
       " 'MATCH (molecularfunction:MolecularFunction {commonName: \"endoribonuclease activity, producing 3\\'-phosphomonoesters\"}) - [:SYMPTOMMANIFESTATIONOFDISEASE*2] -> (gene:Gene {geneSymbol: \"LOC127885110\"}) WHERE molecularfunction.commonName = \"transmembrane-ephrin receptor activity\" RETURN gene.commonName;',\n",
       " 'MATCH (drugclass:DrugClass {commonName: \"Microtubule Inhibition\"}) - [:BODYPARTOVEREXPRESSESGENE*3..] -> (gene:Gene {commonName: \"-\"}) WHERE gene.commonName = \"-\" RETURN drugclass.commonName;',\n",
       " 'MATCH (cellularcomponent:CellularComponent {commonName: \"clathrin-sculpted gamma-aminobutyric acid transport vesicle\"}) - [:GENEASSOCIATEDWITHCELLULARCOMPONENT*3..6] -> (drugclass:DrugClass {commonName: \"Glucosylceramide Synthase Inhibitors\"}) RETURN cellularcomponent.commonName;',\n",
       " 'MATCH (pathway:Pathway {commonName: \"Nicotine Activity on Dopaminergic Neurons\"}) - [*..5] -> (cellularcomponent:CellularComponent {commonName: \"myosin II filament\"}) RETURN cellularcomponent.commonName;',\n",
       " 'MATCH (molecularfunction:MolecularFunction {commonName: \"lamin binding\"}) - [*..3] -> (disease:Disease {commonName: \"Alzheimer Disease 7\"}) RETURN molecularfunction.commonName;',\n",
       " 'MATCH (molecularfunction:MolecularFunction {commonName: \"G-protein coupled receptor binding\"}) - [*..3] -> (gene:Gene {commonName: \"-\"}) RETURN molecularfunction.commonName;',\n",
       " 'MATCH (bodypart:BodyPart {commonName: \"embryo\"}) - [*2] -> (disease:Disease {commonName: \"Familial Alzheimer-like prion disease\"}) WHERE bodypart.commonName = \"scrotum\" RETURN disease.commonName;',\n",
       " 'MATCH (drug:Drug {commonName: \"Ethyl pyruvate\"}) - [:CHEMICALBINDSGENE*3..] -> (cellularcomponent:CellularComponent {commonName: \"signal recognition particle\"}) WHERE cellularcomponent.commonName = \"apicolateral plasma membrane\" RETURN drug.commonName;',\n",
       " 'MATCH (symptom:Symptom {commonName: \"Obesity, Abdominal\"}) - [*..6] -> (gene:Gene {geneSymbol: \"GADD45AP1\"}) RETURN symptom.commonName;',\n",
       " 'MATCH (drugclass:DrugClass {commonName: \"Receptor Tyrosine Kinase Inhibitors\"}) - [:GENEASSOCIATESWITHDISEASE] -> (drugclass1:DrugClass {commonName: \"Increased Cytokine Activity\"}) RETURN drugclass.commonName;',\n",
       " 'MATCH (drug:Drug {commonName: \"4-{5-[(Z)-(2-IMINO-4-OXO-1,3-THIAZOLIDIN-5-YLIDENE)METHYL]FURAN-2-YL}BENZENESULFONAMIDE\"}) - [:GENECOVARIESWITHGENE*2] -> (symptom:Symptom {commonName: \"Mouth Breathing\"}) WHERE symptom.commonName = \"Sleep Deprivation\" RETURN symptom.commonName;',\n",
       " 'MATCH (drug:Drug {commonName: \"Iron Dextran\"}) - [*2..7] -> (molecularfunction:MolecularFunction {commonName: \"ubiquitin-like protein binding\"}) RETURN molecularfunction.commonName;',\n",
       " 'MATCH (gene:Gene {geneSymbol: \"LOC130063932\"}) - [*2..7] -> (disease:Disease {commonName: \"Alzheimer Disease, Familial, 3, with Spastic Paraparesis and Unusual Plaques\"}) RETURN disease.commonName;',\n",
       " 'MATCH (drugclass:DrugClass {commonName: \"Folic Acid\"}) - [*..6] -> (drug:Drug {commonName: \"Cobalt hexammine ion\"}) WHERE drugclass.commonName = \"UGT1A4 Inhibitors\" RETURN drug.commonName;']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [33]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# queries = [\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m#     \"MATCH (n) RETURN n LIMIT 1\",\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m#     \"MATCH (n:Person) WHERE n.name = 'Alice' RETURN n\",\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m#     \"MATCH (n) RETURN n\"\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# ]\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m     successful_query_indices, failed_indices \u001b[38;5;241m=\u001b[39m \u001b[43mrun_queries_multithreaded\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSuccessful query indices:\u001b[39m\u001b[38;5;124m\"\u001b[39m, successful_query_indices)\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfailed_indices:\u001b[39m\u001b[38;5;124m\"\u001b[39m, failed_indices)\n",
      "Input \u001b[0;32mIn [31]\u001b[0m, in \u001b[0;36mrun_queries_multithreaded\u001b[0;34m(query_list, max_workers, timeout)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_queries_multithreaded\u001b[39m(query_list, max_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;66;03m# Create a ThreadPoolExecutor to manage multiple threads\u001b[39;00m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ThreadPoolExecutor(max_workers\u001b[38;5;241m=\u001b[39mmax_workers) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;66;03m# Map futures to their corresponding query indices\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m         future_to_index \u001b[38;5;241m=\u001b[39m {executor\u001b[38;5;241m.\u001b[39msubmit(execute_query, query): i \u001b[38;5;28;01mfor\u001b[39;00m i, query \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(query_list)}\n\u001b[1;32m     56\u001b[0m         \u001b[38;5;66;03m# Create a list to store the indices of successful and failed queries\u001b[39;00m\n\u001b[1;32m     57\u001b[0m         successful_indices \u001b[38;5;241m=\u001b[39m []\n",
      "Input \u001b[0;32mIn [31]\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_queries_multithreaded\u001b[39m(query_list, max_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;66;03m# Create a ThreadPoolExecutor to manage multiple threads\u001b[39;00m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ThreadPoolExecutor(max_workers\u001b[38;5;241m=\u001b[39mmax_workers) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;66;03m# Map futures to their corresponding query indices\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m         future_to_index \u001b[38;5;241m=\u001b[39m {\u001b[43mexecutor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecute_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m: i \u001b[38;5;28;01mfor\u001b[39;00m i, query \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(query_list)}\n\u001b[1;32m     56\u001b[0m         \u001b[38;5;66;03m# Create a list to store the indices of successful and failed queries\u001b[39;00m\n\u001b[1;32m     57\u001b[0m         successful_indices \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/concurrent/futures/thread.py:176\u001b[0m, in \u001b[0;36mThreadPoolExecutor.submit\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m w \u001b[38;5;241m=\u001b[39m _WorkItem(f, fn, args, kwargs)\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_work_queue\u001b[38;5;241m.\u001b[39mput(w)\n\u001b[0;32m--> 176\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_adjust_thread_count\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/concurrent/futures/thread.py:199\u001b[0m, in \u001b[0;36mThreadPoolExecutor._adjust_thread_count\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    192\u001b[0m thread_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_thread_name_prefix \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    193\u001b[0m                          num_threads)\n\u001b[1;32m    194\u001b[0m t \u001b[38;5;241m=\u001b[39m threading\u001b[38;5;241m.\u001b[39mThread(name\u001b[38;5;241m=\u001b[39mthread_name, target\u001b[38;5;241m=\u001b[39m_worker,\n\u001b[1;32m    195\u001b[0m                      args\u001b[38;5;241m=\u001b[39m(weakref\u001b[38;5;241m.\u001b[39mref(\u001b[38;5;28mself\u001b[39m, weakref_cb),\n\u001b[1;32m    196\u001b[0m                            \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_work_queue,\n\u001b[1;32m    197\u001b[0m                            \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initializer,\n\u001b[1;32m    198\u001b[0m                            \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initargs))\n\u001b[0;32m--> 199\u001b[0m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads\u001b[38;5;241m.\u001b[39madd(t)\n\u001b[1;32m    201\u001b[0m _threads_queues[t] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_work_queue\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/threading.py:897\u001b[0m, in \u001b[0;36mThread.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    895\u001b[0m         \u001b[38;5;28;01mdel\u001b[39;00m _limbo[\u001b[38;5;28mself\u001b[39m]\n\u001b[1;32m    896\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m--> 897\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_started\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/threading.py:574\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    572\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 574\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # queries = [\n",
    "    #     \"MATCH (n) RETURN n LIMIT 1\",\n",
    "    #     \"MATCH (n:Person) WHERE n.name = 'Alice' RETURN n\",\n",
    "    #     \"MATCH (n) RETURN n\"\n",
    "    # ]\n",
    "    \n",
    "    successful_query_indices, failed_indices = run_queries_multithreaded(query_list)\n",
    "    print(\"Successful query indices:\", successful_query_indices)\n",
    "    print(\"failed_indices:\", failed_indices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single_query_run bash file archive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Archive-Multithreading/For loop single threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # def run_queries_multithreaded(self,query_list, execution, max_workers=5):\n",
    "    #     # Create a ThreadPoolExecutor to manage multiple threads\n",
    "    #     with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    #         # Map futures to their corresponding query indices\n",
    "    #         future_to_index = {executor.submit(execution, query): i for i, query in enumerate(query_list)}\n",
    "\n",
    "    #         # Create a list to store the indices of successful queries\n",
    "    #         successful_indices = []\n",
    "    #         failed_indices = []\n",
    "\n",
    "    #         # Collect results as they complete\n",
    "    #         for future in concurrent.futures.as_completed(future_to_index):\n",
    "    #             index = future_to_index[future]\n",
    "    #             try:\n",
    "    #                 result = future.result()\n",
    "    #                 if result:\n",
    "    #                     print(f\"Query at index {index} returned results.\")\n",
    "    #                     successful_indices.append(index)\n",
    "    #                 else:\n",
    "    #                     print(f\"Query at index {index} returned no results.\")\n",
    "    #             except Exception as e:\n",
    "    #                 print(f\"Query execution failed for index {index}: {str(e)}\")\n",
    "    #                 failed_indices.append(index)\n",
    "\n",
    "    #         return successful_indices, failed_indices\n",
    "\n",
    "    # Function to run multiple queries using multithreading\n",
    "    # def run_queries_multithreaded(self, tree_population, max_workers=10):\n",
    "    #     # Using ThreadPoolExecutor to manage multiple threads\n",
    "    #     with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    #         query_list = self.population_to_query_list(tree_population)\n",
    "    #         # Submit all queries to the executor\n",
    "    #         future_to_query = {executor.submit(self.execute_query, query): query for query in query_list}\n",
    "    #         # Collect results as they complete\n",
    "    #         for future in concurrent.futures.as_completed(future_to_query):\n",
    "    #             query = future_to_query[future]\n",
    "    #             try:\n",
    "    #                 result = future.result()\n",
    "    #                 if result:\n",
    "    #                     print(f\"Query: {query} returned results\")\n",
    "    #                     self.valid_queries.append(query)\n",
    "                        \n",
    "    #                 else:\n",
    "                        \n",
    "    #                     # print(f\"Query: {query} returned no results\")\n",
    "    #             except Exception as e:\n",
    "                    \n",
    "                    # print(f\"Query execution failed for {query}: {str(e)}\")\n",
    "\n",
    "    # def run_queries_multithreaded(self, query_list, max_workers=5):\n",
    "    #     \"\"\" \n",
    "    #     Us multithreading to run queries on the Memgraph client.\n",
    "    #     Returns indices corresponding to successful queries and garbage queries for later use.\n",
    "    #     \"\"\"\n",
    "    #     # Create a ThreadPoolExecutor to manage multiple threads\n",
    "    #     with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    #         # Map futures to their corresponding query indices\n",
    " #         future_to_index = {executor.submit(self.database_conn.execute_query, query): i for i, query in enumerate(query_list)}\n",
    "    #         # Create a list to store the indices of successful queries\n",
    "    #         successful_indices = []\n",
    "    #         failed_indices = []\n",
    "    #         # Collect results as they complete\n",
    "    #         for future in concurrent.futures.as_completed(future_to_index):\n",
    "    #             index = future_to_index[future]\n",
    "    #             try:\n",
    "    #                 result = future.result()\n",
    "    #                 if result:\n",
    "    #                     print(f\"Query at index {index} returned results.\")\n",
    "    #                     successful_indices.append(index)\n",
    "    #                 else:\n",
    "    #                     # garbage_indices.append(index)\n",
    "    #                     print(f\"Query at index {index} returned no results.\")\n",
    "    #             except Exception as e:\n",
    "    #                 print(f\"Query execution failed for index {index}: {str(e)}\")\n",
    "    #                 failed_indices.append(index)\n",
    "    #         return successful_indices, failed_indices\n",
    "        \n",
    "    # def run_queries_multithreaded(self, tree_population, max_workers=5):\n",
    "    #     \"\"\" \n",
    "    #     Us multithreading to run queries on the Memgraph client.\n",
    "    #     Returns indices corresponding to successful queries and garbage queries for later use.\n",
    "    #     \"\"\"\n",
    "    #     query_list = self.population_to_query_list(tree_population)\n",
    "    #     # Create a ThreadPoolExecutor to manage multiple threads\n",
    "    #     with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    #         future_to_index = {executor.submit(self.database_conn.execute_query, query): i for i, query in enumerate(query_list)}\n",
    "    #         return self.process_futures(future_to_index)\n",
    "            # # Map futures to their corresponding query indices\n",
    "            # future_to_index = {executor.submit(self.database_conn.execute_query, query): i for i, query in enumerate(query_list)}\n",
    "            # # Create a list to store the indices of successful queries\n",
    "            # successful_indices = []\n",
    "            # failed_indices = []\n",
    "            # # Collect results as they complete\n",
    "            # for future in concurrent.futures.as_completed(future_to_index):\n",
    "            #     index = future_to_index[future]\n",
    "            #     try:\n",
    "            #         result = future.result()\n",
    "            #         if result:\n",
    "            #             print(f\"Query at index {index} returned results.\")\n",
    "            #             successful_indices.append(index)\n",
    "            #         else:\n",
    "            #             print(f\"Query at index {index} returned no results.\")\n",
    "            #     except Exception as e:\n",
    "            #         print(f\"Query execution failed for index {index}: {str(e)}\")\n",
    "            #         failed_indices.append(index)\n",
    "            # return successful_indices, failed_indices\n",
    "        \n",
    "    # def process_futures(self, future_to_index):\n",
    "    #     successful_indices = []\n",
    "    #     failed_indices = []\n",
    "    #     for future in concurrent.futures.as_completed(future_to_index):\n",
    "    #         index = future_to_index[future]\n",
    "    #         try:\n",
    "    #             result = future.result(timeout=10)  # Add timeout to avoid hanging indefinitely\n",
    "    #             if result:\n",
    "    #                 successful_indices.append(index)\n",
    "    #             # else:\n",
    "    #             #     failed_indices.append(index)\n",
    "    #         except Exception as e:\n",
    "    #             print(f\"Query execution failed for index {index}: {str(e)}\")\n",
    "    #             failed_indices.append(index)\n",
    "\n",
    "   \n",
    "    #     return successful_indices, failed_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MATCH (drugclass:DrugClass {commonName: \"Hematologic Activity Alteration\"}) - [:GENEREGULATESGENE] -> (gene:Gene {commonName: \"-\"}) RETURN drugclass.commonName;',\n",
       " 'MATCH (biologicalprocess:BiologicalProcess {commonName: \"regulation of bone trabecula formation\"}) - [:GENEASSOCIATESWITHDISEASE] -> (pathway:Pathway {commonName: \"Methotrexate Pathway, Pharmacokinetics\"}) WHERE pathway.commonName = \"Esomeprazole Action Pathway\" RETURN biologicalprocess.commonName;',\n",
       " 'MATCH (drugclass:DrugClass {commonName: \"Genitourinary Arterial Vasodilation\"}) - [:GENECOVARIESWITHGENE] -> (symptom:Symptom {commonName: \"Polydipsia, Psychogenic\"}) WHERE drugclass.commonName = \"Increased Blood Pressure\" RETURN symptom.commonName;',\n",
       " 'MATCH (biologicalprocess:BiologicalProcess {commonName: \"blastoderm segmentation\"}) - [*7..10] -> (disease:Disease {commonName: \"Familial Alzheimer-like prion disease\"}) RETURN biologicalprocess.commonName;',\n",
       " 'MATCH (gene:Gene {commonName: \"-\"}) - [*..18] -> (drugclass:DrugClass {commonName: \"Central Nervous System Depression\"}) WHERE drugclass.commonName = \"Biguanides\" RETURN gene.commonName;']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = query_list[:5]\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_query_list(query_list, size=5):\n",
    "    \"\"\"\n",
    "    Splits a list of queries into sublists, each containing a specific number of queries.\n",
    "\n",
    "    Parameters:\n",
    "    - query_list (list): The original list of queries.\n",
    "    - size (int): The number of queries each sublist should contain.\n",
    "\n",
    "    Returns:\n",
    "    - list of lists: A list where each element is a sublist containing 'size' number of queries.\n",
    "    \"\"\"\n",
    "    return [query_list[i:i + size] for i in range(0, len(query_list), size)]\n",
    "\n",
    "splits = split_query_list(query_list, size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "successful_indices = []\n",
    "failed_indices = []\n",
    "\n",
    "for sublist in splits:\n",
    "    for index, query in enumerate(sublist):\n",
    "    # print(index)\n",
    "        result = execute_query(query)\n",
    "        if result:\n",
    "            if result == 'NoResult':\n",
    "                pass\n",
    "            else:\n",
    "                successful_indices.append(index)\n",
    "        else:\n",
    "            failed_indices.append(index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try without multithreading\n",
    "successful_indices = []\n",
    "failed_indices = []\n",
    "\n",
    "\n",
    "for index, query in enumerate(test):\n",
    "    # print(index)\n",
    "    result = execute_query(query)\n",
    "    if result:\n",
    "        if result == 'NoResult':\n",
    "            pass\n",
    "        else:\n",
    "            successful_indices.append(index)\n",
    "    else:\n",
    "        failed_indices.append(index)\n",
    "\n",
    "    # try:\n",
    "    #     result = execute_query(query)\n",
    "    #     if result:\n",
    "    #         print(f\"Query at index {index} returned results.\")\n",
    "    #         successful_indices.append(index)\n",
    "    #     else:\n",
    "    #         print(f\"Query at index {index} returned no results.\")\n",
    "    # except Exception as e:\n",
    "    #     print(f\"Query execution failed for index {index}: {str(e)}\")\n",
    "    #     failed_indices.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "successful_indices\n",
    "failed_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query at index 2 returned results.\n",
      "Query at index 3 returned results.\n",
      "Query at index 4 returned results.\n",
      "Query at index 0 returned results.\n",
      "Query at index 1 returned results.\n",
      "Query at index 5 returned results.\n",
      "Query at index 7 returned results.\n",
      "Query at index 8 returned results.\n",
      "Query at index 9 returned results.\n",
      "Query at index 6 returned results.\n",
      "Query at index 11 returned results.\n",
      "Query at index 12 returned results.\n",
      "Query at index 15 returned results.\n",
      "Query at index 14 returned results.\n",
      "Query at index 16 returned results.\n",
      "Query at index 10 returned results.\n",
      "Query at index 13 returned results.\n",
      "Query at index 17 returned results.\n",
      "Query at index 18 returned results.\n",
      "Query at index 19 returned results.\n",
      "Successful query indices: [2, 3, 4, 0, 1, 5, 7, 8, 9, 6, 11, 12, 15, 14, 16, 10, 13, 17, 18, 19]\n",
      "failed_indices: []\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    successful_query_indices, failed_indices = run_queries_multithreaded(query_list)\n",
    "    print(\"Successful query indices:\", successful_query_indices)\n",
    "    print(\"failed_indices:\", failed_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "Input \u001b[0;32mIn [47]\u001b[0m, in \u001b[0;36m<cell line: 29>\u001b[0;34m()\u001b[0m\n",
      "\u001b[1;32m     19\u001b[0m conn\u001b[38;5;241m.\u001b[39mautocommit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# Turn off autocommit for manual transaction management\u001b[39;00m\n",
      "\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# # List of queries to execute as part of a single batch\u001b[39;00m\n",
      "\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# queries = [\u001b[39;00m\n",
      "\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m#     \"CREATE (n:Person {name: 'Alice'})\",\u001b[39;00m\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m     27\u001b[0m \n",
      "\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Execute as batch\u001b[39;00m\n",
      "\u001b[0;32m---> 29\u001b[0m execute_queries_as_batch(conn, \u001b[43mtest_list\u001b[49m)\n",
      "\u001b[1;32m     30\u001b[0m conn\u001b[38;5;241m.\u001b[39mclose()\n",
      "\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_list' is not defined"
     ]
    }
   ],
   "source": [
    "#TEST if test_3 can run\n",
    "import mgclient\n",
    "\n",
    "def execute_queries_as_batch(conn, queries):\n",
    "    cursor = conn.cursor()\n",
    "    try:\n",
    "        # cursor.execute(\"BEGIN;\")\n",
    "        for query in queries:\n",
    "            cursor.execute(query)\n",
    "        # cursor.execute(\"COMMIT;\")\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", str(e))\n",
    "        # cursor.execute(\"ROLLBACK;\")  # Rollback in case of an error\n",
    "    finally:\n",
    "        cursor.close()  # Ensure the cursor is closed after operation\n",
    "\n",
    "# Connection setup\n",
    "conn = mgclient.connect(host='127.0.0.1', port=7687)\n",
    "conn.autocommit = False  # Turn off autocommit for manual transaction management\n",
    "\n",
    "# # List of queries to execute as part of a single batch\n",
    "# queries = [\n",
    "#     \"CREATE (n:Person {name: 'Alice'})\",\n",
    "#     \"MATCH (n:Person) WHERE n.name = 'Alice' RETURN n\",\n",
    "#     \"CREATE (n:Person {name: 'Bob'})\"\n",
    "# ]\n",
    "\n",
    "# Execute as batch\n",
    "execute_queries_as_batch(conn, test_list)\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 56>\u001b[0;34m()\u001b[0m\n",
      "\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n",
      "\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;32m     57\u001b[0m     \u001b[38;5;66;03m# queries = [\u001b[39;00m\n",
      "\u001b[1;32m     58\u001b[0m     \u001b[38;5;66;03m#     \"MATCH (n) RETURN n LIMIT 1\",\u001b[39;00m\n",
      "\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m#     \"MATCH (n:Person) WHERE n.name = 'Alice' RETURN n\",\u001b[39;00m\n",
      "\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m#     \"MATCH (n) RETURN n\"\u001b[39;00m\n",
      "\u001b[1;32m     61\u001b[0m     \u001b[38;5;66;03m# ]\u001b[39;00m\n",
      "\u001b[0;32m---> 62\u001b[0m     successful_query_indices, failed_indices \u001b[38;5;241m=\u001b[39m run_queries_multithreaded(\u001b[43mtest_list\u001b[49m)\n",
      "\u001b[1;32m     63\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSuccessful query indices:\u001b[39m\u001b[38;5;124m\"\u001b[39m, successful_query_indices)\n",
      "\u001b[1;32m     64\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfailed_indices:\u001b[39m\u001b[38;5;124m\"\u001b[39m, failed_indices)\n",
      "\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_list' is not defined"
     ]
    }
   ],
   "source": [
    "#TRYING MULTITHREADING\n",
    "import mgclient\n",
    "import concurrent.futures\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import threading\n",
    "# conn = mgclient.connect(host='127.0.0.1', port=7687)\n",
    "# cursor = conn.cursor()\n",
    "# Define a function to connect to Memgraph and execute a single query\n",
    "def execute_query(query):\n",
    "    # Create thread-local storage for database connections\n",
    "    if not hasattr(execute_query, \"conn\"):\n",
    "        execute_query.conn = mgclient.connect(host='127.0.0.1', port=7687)\n",
    "        execute_query.conn.autocommit = True\n",
    "    try:\n",
    "        cursor = execute_query.conn.cursor()\n",
    "        cursor.execute(query)\n",
    "        results = cursor.fetchall()\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to execute query {query}: {str(e)}\")\n",
    "        return None\n",
    "    finally:\n",
    "        cursor.close()\n",
    "\n",
    "def run_queries_multithreaded(query_list, max_workers=5):\n",
    "    # Create a ThreadPoolExecutor to manage multiple threads\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Map futures to their corresponding query indices\n",
    "        future_to_index = {executor.submit(execute_query, query): i for i, query in enumerate(query_list)}\n",
    "\n",
    "        # Create a list to store the indices of successful queries\n",
    "        successful_indices = []\n",
    "        failed_indices = []\n",
    "\n",
    "        # Collect results as they complete\n",
    "        for future in concurrent.futures.as_completed(future_to_index):\n",
    "            index = future_to_index[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "                if result:\n",
    "                    print(f\"Query at index {index} returned results.\")\n",
    "                    successful_indices.append(index)\n",
    "                else:\n",
    "                    print(f\"Query at index {index} returned no results.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Query execution failed for index {index}: {str(e)}\")\n",
    "                failed_indices.append(index)\n",
    "\n",
    "        return successful_indices, failed_indices\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # queries = [\n",
    "    #     \"MATCH (n) RETURN n LIMIT 1\",\n",
    "    #     \"MATCH (n:Person) WHERE n.name = 'Alice' RETURN n\",\n",
    "    #     \"MATCH (n) RETURN n\"\n",
    "    # ]\n",
    "    successful_query_indices, failed_indices = run_queries_multithreaded(test_list)\n",
    "    print(\"Successful query indices:\", successful_query_indices)\n",
    "    print(\"failed_indices:\", failed_indices)\n",
    "\n",
    "# # Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Example list of queries\n",
    "#     queries = [\n",
    "#         \"MATCH (n) RETURN n LIMIT 1\",\n",
    "#         \"MATCH (n:Person) WHERE n.name = 'Alice' RETURN n\",\n",
    "#         '''MATCH (cellularcomponent:CellularComponent {commonName: \"cortical cytoskeleton\"}) - [:CHEMICALDECREASESEXPRESSION] -> (cellularcomponent1:CellularComponent {commonName: \"ubiquitin conjugating enzyme complex\"}) RETURN cellularcomponent.commonName, cellularcomponent1.commonName;'''\n",
    "#         # Add more queries as needed\n",
    "#     ]\n",
    "\n",
    "    # run_queries_multithreaded(queries, max_workers=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ea = EvolutionaryAlgorithm(qm=qmTest, depth_manager=dmTest, population_size=10,max_depth=10, max_generation=3)\n",
    "# print(ea.depth_manager._max_depth)\n",
    "print(\"generation:\",ea.generation)\n",
    "ea.initialize_population() \n",
    "ea.evaluate_population()\n",
    "print(\"generation:\",ea.generation)\n",
    "print(\"Fitness Scores:\", ea.fitness_scores)\n",
    "\n",
    "parent_pairs = ea.select_parents(num_pairs=5,k=3) #assume 5 pairs of parents --> maintain 10 total in next gen\n",
    "offspring_list = []\n",
    "for parent1, parent2 in parent_pairs:\n",
    "    # print(\"parent:\",parent1.children[-1])\n",
    "    # print(parent2.children[-1])\n",
    "    offspring1, offspring2 = ea.one_point_crossover(parent1, parent2)\n",
    "    # print(\"offspring:\", offspring1.children[-1])\n",
    "    # print(offspring2.children[-1])\n",
    "    offspring_list.extend([offspring1, offspring2])\n",
    "\n",
    "ea.tree_population = offspring_list\n",
    "ea.evaluate_population()\n",
    "print(\"generation:\",ea.generation)\n",
    "print(\"Fitness Scores:\", ea.fitness_scores)\n",
    "# ea.reset_ea()\n",
    "\n",
    "\n",
    "# print(ea.depth_manager.depth_record)\n",
    "result = ea.output_top_queries(top_n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All queries have been written to output_queries.txt\n"
     ]
    }
   ],
   "source": [
    "output_file_path = 'output_queries.txt'\n",
    "with open(output_file_path, 'w') as file:\n",
    "    for tuple in result:\n",
    "        tree = tuple[0]\n",
    "        querystr = tree.to_querystr()\n",
    "        file.write(querystr + '\\n')\n",
    "print(\"All queries have been written to\", output_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 queries have been written to output_queries.txt\n"
     ]
    }
   ],
   "source": [
    "output_file_path = 'output_queries.txt'\n",
    "with open(output_file_path, 'w') as file:\n",
    "    counter = 0\n",
    "    for query in query_list:\n",
    "        if counter >= 10:\n",
    "            break  # Stop writing if the counter reaches 10\n",
    "        file.write(query + '\\n')  # Write the query to the file\n",
    "        counter += 1 \n",
    "print(\"10 queries have been written to\", output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1 executed and output saved to ./outputs/query1.csv\n",
      "Query 2 executed and output saved to ./outputs/query2.csv\n",
      "Query 3 executed and output saved to ./outputs/query3.csv\n",
      "Query 4 executed and output saved to ./outputs/query4.csv\n",
      "Query 5 executed and output saved to ./outputs/query5.csv\n",
      "Query 6 executed and output saved to ./outputs/query6.csv\n",
      "Query 7 executed and output saved to ./outputs/query7.csv\n",
      "Query 8 executed and output saved to ./outputs/query8.csv\n",
      "Query 9 executed and output saved to ./outputs/query9.csv\n",
      "Query 10 executed and output saved to ./outputs/query10.csv\n",
      "All queries processed.\n"
     ]
    }
   ],
   "source": [
    "!chmod +x single_query_run.sh\n",
    "!./single_query_run.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def aggregate_text_files(folder_path, output_file):\n",
    "    # Initialize a list to store the aggregated data\n",
    "    aggregated_data = []\n",
    "\n",
    "    # List all files in the specified folder\n",
    "    file_list = [f for f in os.listdir(folder_path) if f.endswith('.csv')]  # or change to '.txt' if appropriate\n",
    "\n",
    "    for file_name in file_list:\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        \n",
    "        # Check if the file is empty\n",
    "        if os.stat(file_path).st_size == 0:\n",
    "            aggregated_data.append({\"variables_returned\": \"no_result\", \"results_returned\": \"no_result\"})\n",
    "            continue\n",
    "        \n",
    "        # Open and read the file as a plain text file\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            lines = file.readlines()\n",
    "        \n",
    "        # Check again for empty content after reading lines\n",
    "        if not lines:\n",
    "            aggregated_data.append({\"variables_returned\": \"no_result\", \"results_returned\": \"no_result\"})\n",
    "            continue\n",
    "\n",
    "        # First line as 'variables_returned'\n",
    "        variables_returned = lines[0].strip()\n",
    "        # Join the rest of the lines for 'results_returned'\n",
    "        if len(lines) > 1:\n",
    "            results_returned = '; '.join([line.strip().replace('\\n', ';') for line in lines[1:]])\n",
    "        else:\n",
    "            results_returned = \"no_result\"\n",
    "        \n",
    "        aggregated_data.append({\n",
    "            \"variables_returned\": variables_returned,\n",
    "            \"results_returned\": results_returned\n",
    "        })\n",
    "\n",
    "    # Create a DataFrame from the aggregated data\n",
    "    result_df = pd.DataFrame(aggregated_data)\n",
    "    # Save the aggregated data to a CSV file\n",
    "    result_df.to_csv(output_file, index=False)\n",
    "\n",
    "# Example usage\n",
    "folder_path = './outputs'\n",
    "output_file = './aggregated_results.csv'\n",
    "aggregate_text_files(folder_path, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variables_returned</th>\n",
       "      <th>results_returned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>no_result</td>\n",
       "      <td>no_result</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>no_result</td>\n",
       "      <td>no_result</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>no_result</td>\n",
       "      <td>no_result</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>no_result</td>\n",
       "      <td>no_result</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>no_result</td>\n",
       "      <td>no_result</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>no_result</td>\n",
       "      <td>no_result</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>no_result</td>\n",
       "      <td>no_result</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>no_result</td>\n",
       "      <td>no_result</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>no_result</td>\n",
       "      <td>no_result</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>no_result</td>\n",
       "      <td>no_result</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>no_result</td>\n",
       "      <td>no_result</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>no_result</td>\n",
       "      <td>no_result</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>no_result</td>\n",
       "      <td>no_result</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>no_result</td>\n",
       "      <td>no_result</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>no_result</td>\n",
       "      <td>no_result</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>no_result</td>\n",
       "      <td>no_result</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>no_result</td>\n",
       "      <td>no_result</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>no_result</td>\n",
       "      <td>no_result</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>no_result</td>\n",
       "      <td>no_result</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   variables_returned results_returned\n",
       "0           no_result        no_result\n",
       "1           no_result        no_result\n",
       "2           no_result        no_result\n",
       "3           no_result        no_result\n",
       "4           no_result        no_result\n",
       "5           no_result        no_result\n",
       "6           no_result        no_result\n",
       "7           no_result        no_result\n",
       "8           no_result        no_result\n",
       "9           no_result        no_result\n",
       "10          no_result        no_result\n",
       "11          no_result        no_result\n",
       "12          no_result        no_result\n",
       "13          no_result        no_result\n",
       "14          no_result        no_result\n",
       "15          no_result        no_result\n",
       "16          no_result        no_result\n",
       "17          no_result        no_result\n",
       "18          no_result        no_result"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.read_csv(\"aggregated_results.csv\")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"(:Gene {commonName: \"\"TATA-box binding protein\"\", geneSymbol: \"\"TBP\"\", nodeID: \"\"1\"\", typeOfGene: \"\"protein-coding\"\", uri: \"\"http://jdr.bio/ontologies/alzkb.owl#gene_tbp\"\", xrefEnsembl: \"\"ENSG00000112592\"\", xrefHGNC: \"\"11588\"\", xrefNcbiGene: 6908, xrefOMIM: \"\"600075\"\"})\";\"[:GENEPARTICIPATESINBIOLOGICALPROCESS]\"; \"(:Gene {commonName: \"\"TATA-box binding protein\"\", geneSymbol: \"\"TBP\"\", nodeID: \"\"1\"\", typeOfGene: \"\"protein-coding\"\", uri: \"\"http://jdr.bio/ontologies/alzkb.owl#gene_tbp\"\", xrefEnsembl: \"\"ENSG00000112592\"\", xrefHGNC: \"\"11588\"\", xrefNcbiGene: 6908, xrefOMIM: \"\"600075\"\"})\";\"[:GENEPARTICIPATESINBIOLOGICALPROCESS]\"'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results[\"results_returned\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRING mgclient\n",
    "import mgclient\n",
    "conn = mgclient.connect(host='127.0.0.1', port=7687)\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_connection(conn):\n",
    "    if conn is not None:\n",
    "        try:\n",
    "            cursor = conn.cursor()\n",
    "            # Example test query to fetch 10 nodes\n",
    "            cursor.execute(\"MATCH (n) RETURN n LIMIT 10\")\n",
    "            results = cursor.fetchall()\n",
    "            if results:\n",
    "                print(\"Connection test successful, received data:\", results)\n",
    "            else:\n",
    "                print(\"Connection test successful, no data received\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to execute test query: {e}\")\n",
    "        finally:\n",
    "            cursor.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection test successful, received data: [(<mgclient.Node(id=0, labels={'Drug'}, properties={'commonName': 'Basiliximab', 'nodeID': '263', 'uri': 'http://jdr.bio/ontologies/alzkb.owl#drug_db00074', 'xrefCasRN': '179045-86-4', 'xrefDrugbank': 'DB00074'}) at 0x7feffadcb150>,), (<mgclient.Node(id=1, labels={'Drug'}, properties={'commonName': 'Muromonab', 'nodeID': '264', 'uri': 'http://jdr.bio/ontologies/alzkb.owl#drug_db00075', 'xrefCasRN': '140608-64-6', 'xrefDrugbank': 'DB00075'}) at 0x7ff00a34e2d0>,), (<mgclient.Node(id=2, labels={'Drug'}, properties={'commonName': 'Trastuzumab', 'nodeID': '265', 'uri': 'http://jdr.bio/ontologies/alzkb.owl#drug_db00072', 'xrefCasRN': '180288-69-1', 'xrefDrugbank': 'DB00072'}) at 0x7feffb7d3c00>,), (<mgclient.Node(id=3, labels={'Drug'}, properties={'commonName': 'Rituximab', 'nodeID': '266', 'uri': 'http://jdr.bio/ontologies/alzkb.owl#drug_db00073', 'xrefCasRN': '174722-31-7', 'xrefDrugbank': 'DB00073'}) at 0x7feffb7d3810>,), (<mgclient.Node(id=4, labels={'Drug'}, properties={'commonName': 'Ibritumomab tiuxetan', 'nodeID': '267', 'uri': 'http://jdr.bio/ontologies/alzkb.owl#drug_db00078', 'xrefCasRN': '206181-63-7', 'xrefDrugbank': 'DB00078'}) at 0x7feffb7d3a80>,), (<mgclient.Node(id=5, labels={'Drug'}, properties={'commonName': 'Digoxin Immune Fab (Ovine)', 'nodeID': '270', 'uri': 'http://jdr.bio/ontologies/alzkb.owl#drug_db00076', 'xrefCasRN': '', 'xrefDrugbank': 'DB00076'}) at 0x7feffb7d3780>,), (<mgclient.Node(id=6, labels={'Drug'}, properties={'commonName': 'Hyaluronidase (ovine)', 'nodeID': '274', 'uri': 'http://jdr.bio/ontologies/alzkb.owl#drug_db00070', 'xrefCasRN': '488712-31-8', 'xrefDrugbank': 'DB00070'}) at 0x7feffb7d3ba0>,), (<mgclient.Node(id=7, labels={'Drug'}, properties={'commonName': 'Insulin pork', 'nodeID': '275', 'uri': 'http://jdr.bio/ontologies/alzkb.owl#drug_db00071', 'xrefCasRN': '12584-58-6', 'xrefDrugbank': 'DB00071'}) at 0x7feffb7d3c30>,), (<mgclient.Node(id=8, labels={'Drug'}, properties={'commonName': 'Pancrelipase', 'nodeID': '286', 'uri': 'http://jdr.bio/ontologies/alzkb.owl#drug_db00085', 'xrefCasRN': '53608-75-6', 'xrefDrugbank': 'DB00085'}) at 0x7feffb7d3c60>,), (<mgclient.Node(id=9, labels={'Drug'}, properties={'commonName': 'Streptokinase', 'nodeID': '287', 'uri': 'http://jdr.bio/ontologies/alzkb.owl#drug_db00086', 'xrefCasRN': '9002-01-1', 'xrefDrugbank': 'DB00086'}) at 0x7feffb7d3a20>,)]\n"
     ]
    }
   ],
   "source": [
    "test_connection(conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    def initialize_population(self):\n",
    "        \"\"\" Initializes the population with random depth queries that return results. \"\"\"\n",
    "        # Connect to Memgraph database\n",
    "        conn = connect(host='localhost', port=7687)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Generate queries until the population size is satisfied\n",
    "        while len(self.tree_population) < self.population_size:\n",
    "            tree, query = self.qm.generate_query()\n",
    "            if self.is_valid_query(cursor, query):\n",
    "                self.tree_population.append(tree)\n",
    "                self.str_population.append(query)\n",
    "            self.qm.reset_per_gen()\n",
    "\n",
    "        # Close the database connection\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "    def is_valid_query(self, cursor, query):\n",
    "        \"\"\" Execute the query and check if it returns any results. \"\"\"\n",
    "        try:\n",
    "            cursor.execute(query)\n",
    "            results = cursor.fetchall()\n",
    "            return len(results) > 0  # Return True if there are results\n",
    "        except Exception as e:\n",
    "            print(f\"Query failed: {e}\")\n",
    "            return False  # Query failed or returned no results\n",
    "\n",
    "# Example usage:\n",
    "# Assuming 'qm' is an instance of QueryManager initialized properly\n",
    "ea = EvolutionaryAlgorithm(qm)\n",
    "ea.initialize_population()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

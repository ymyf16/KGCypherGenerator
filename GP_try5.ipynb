{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cook labels and properties\n",
    "- Grep csv file containing node labels, relationships, property labels, and wanted properties from Memgraph\n",
    "- Create a nested dictionary with {node_labels:{sub_dictionary of properties}}, the sub_dictionary contains corresponding {property_labels: properties}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import json\n",
    "import pandas as pd\n",
    "import copy\n",
    "import hashlib\n",
    "import base64\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Open and load the graph schema json file\n",
    "with open('/Users/yufeimeng/Desktop/KGCypherGenerator/schema.json', 'r',encoding='utf-8-sig') as file:\n",
    "    schema = json.load(file)\n",
    "    \n",
    "# Extract nodes and edges from the schema\n",
    "#labels = [node['labels'][0] for node in schema[0]['nodes']]\n",
    "relationships = [relationship['type'] for relationship in schema[0]['relationships']]\n",
    "\n",
    "\n",
    "# Get detailed properties from the csv file\n",
    "common_names = pd.read_csv('/Users/yufeimeng/Desktop/KGCypherGenerator/memgraph-query-results-export.csv', index_col=False)\n",
    "\n",
    "def group_labels(df, label_col, name_col):\n",
    "    grouped = df.groupby(label_col)[name_col].apply(list).to_dict()\n",
    "    return grouped\n",
    "\n",
    "# Applying the function\n",
    "grouped_names = group_labels(common_names, 'label', 'commonName')\n",
    "\n",
    "#***************************************\n",
    "labels = list(grouped_names.keys())\n",
    "property_labels= [\"commonName\"] #will be generalized later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create alzkb nested dictionary\n",
    "- In the current test case, all node labels are used; only commonName properties are selected for all nodes except that geneSymbol for Gene node is also added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(grouped_names)\n",
    "# grouped_names.keys()\n",
    "# list(grouped_names.values())[0]\n",
    "\n",
    "geneSymbol_csv = pd.read_csv('geneSymbol.csv', index_col=False)\n",
    "# type(geneSymbol_csv)\n",
    "# type(geneSymbol_csv['g.geneSymbol'])\n",
    "\n",
    "geneSymbol = list(geneSymbol_csv['g.geneSymbol'])\n",
    "# geneSymbol\n",
    "\n",
    "geneSymbol_sub_dict = {}\n",
    "geneSymbol_sub_dict['geneSymbol'] = geneSymbol\n",
    "\n",
    "\n",
    "alzkb_nested_dict = {}\n",
    "for key in grouped_names.keys():\n",
    "    sub_dict = {}\n",
    "    if key == 'Gene':\n",
    "        sub_dict['commonName']= grouped_names[key]\n",
    "        sub_dict['geneSymbol']= geneSymbol\n",
    "    else:\n",
    "        sub_dict['commonName']= grouped_names[key]\n",
    "    alzkb_nested_dict[key] = sub_dict\n",
    "\n",
    "# alzkb_nested_dict['Gene'].keys()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DepthManager, QueryManager, Nodes definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DepthManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDepthManager: #TRY TO MAKE DEPTHMANAGER SINGLETON \n",
    "    _max_depth = 5  # Default maximum depth\n",
    "    _min_depth = 3\n",
    "    _instance = None\n",
    "\n",
    "    @classmethod\n",
    "    def getInstance(cls):\n",
    "        if cls._instance is None:\n",
    "            cls._instance = cls()\n",
    "        return cls._instance\n",
    "\n",
    "    def __init__(self):\n",
    "        if TestDepthManager._instance is not None:\n",
    "            raise Exception(\"This is a singleton class. Use 'getInstance()'.\")\n",
    "        self.depth = 0  # Starting depth\n",
    "        self.depth_record = {}\n",
    "\n",
    "    @classmethod\n",
    "    def set_max_depth(cls, depth):\n",
    "        if depth > cls._min_depth:\n",
    "            cls._max_depth = depth\n",
    "        else:\n",
    "            print(\"Maximum depth cannot be smaller than the min depth! \\n The default max_depth is\", cls._max_depth)\n",
    "    \n",
    "    def reset_depth(self):\n",
    "        self.depth = 0\n",
    "    \n",
    "    # def reset_depth_record(self):\n",
    "    #     self.depth_record = {}\n",
    "    \n",
    "def depth_control(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        dm = TestDepthManager.getInstance()\n",
    "        if dm.depth == dm._max_depth:\n",
    "            print(\"Max depth reached\")\n",
    "            return None\n",
    "        result = func(*args, **kwargs)\n",
    "        dm.depth += 1  # Increment depth after function call\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "def class_depth_control(cls):\n",
    "    class WrappedClass(cls):  # Create a new class that wraps the original class\n",
    "        def __init__(self, *args, **kwargs):\n",
    "            dm = TestDepthManager.getInstance()\n",
    "            if dm.depth == dm._max_depth:\n",
    "                print(\"Max depth reached\")\n",
    "                return None\n",
    "            super().__init__(*args, **kwargs)\n",
    "            dm.depth += 1\n",
    "    return WrappedClass "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TreeNode:\n",
    "    depth = 0\n",
    "    score = 1 #defaultly assume tree as garbage query\n",
    "   \n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "        self.children = []\n",
    "        # self.level = 0\n",
    "\n",
    "    def add_child(self, node):\n",
    "        \"\"\"Add a TreeNode or value as a child.\"\"\"\n",
    "        # if not isinstance(node, TreeNode):\n",
    "            # node = TreeNode(node)  # Ensure all children are TreeNode instances\n",
    "        self.children.append(node)\n",
    "        # self.level += 1\n",
    "\n",
    "    def __str__(self):\n",
    "        # Use the helper method for generating the string with indentation\n",
    "        return self._str_recursive(level=0)\n",
    "\n",
    "    def _str_recursive(self,level):\n",
    "        # Create the string representation with indentation for current node\n",
    "        ret = \"\\t\" *level + str(self.value) + \"\\n\"  # Indent based on the current level\n",
    "        for child in self.children:\n",
    "            ret += child._str_recursive(level+1)\n",
    "        return ret\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'<TreeNode {self.value}>'\n",
    "    \n",
    "    def get_depth(self):\n",
    "        pass #because it needs later-defined class type\n",
    "\n",
    "    def to_querystr(self):\n",
    "        \"\"\"\n",
    "        convert the generate query tree into query string with ; separation to get ready for querying the Memgraph client\n",
    "        \"\"\"\n",
    "        child_compose = ''\n",
    "        final_query_str = 'MATCH'\n",
    "        for child in self.children:\n",
    "            if child.children:\n",
    "                for grandchild in child.children:\n",
    "                    child_compose = ' '+ str(grandchild.value)\n",
    "            final_query_str += ' ' + str(child.value) + child_compose\n",
    "        final_query_str += ';'\n",
    "        return final_query_str\n",
    "\n",
    "        \n",
    "class Clause(TreeNode):\n",
    "    def __init__(self, value, children=None):\n",
    "        super().__init__(value)\n",
    "        self.children = children if children is not None else []\n",
    "\n",
    "    def __str__(self):\n",
    "        if not self.children:\n",
    "            return str(self.value)\n",
    "        if self.value == \"RETURN\":\n",
    "            return f\"{self.value} {', '.join(str(child) for child in self.children)}\"\n",
    "        return f\"{self.value} {' '.join(str(child) for child in self.children)}\"\n",
    "    \n",
    "class Node(TreeNode):\n",
    "    \"\"\"\n",
    "    When called, will add connector to nodes and make nodes Node type\n",
    "    \"\"\" \n",
    "    def __init__(self, value, children=None):\n",
    "        super().__init__(value)\n",
    "        self.children = children if children is not None else []\n",
    " \n",
    "    def __str__(self):\n",
    "        if not self.children:\n",
    "            return str(self.value)\n",
    "        # if self.value == '-':  \n",
    "        #     return ' '.join(str(child) for child in self.children)\n",
    "        return f\"{self.value}({', '.join(str(child) for child in self.children)})\"\n",
    "\n",
    "# @class_depth_control\n",
    "class Relationship(TreeNode):\n",
    "    \"\"\"\n",
    "    When called, will add connector to relationships and make relationships Relationship type\n",
    "    \"\"\"\n",
    "    def __init__(self, value, hop_only=False):\n",
    "        super().__init__(value)\n",
    "        # self.children = children if children is not None else []\n",
    "        self.hop_only = True if hop_only else False\n",
    "        \n",
    "    @depth_control\n",
    "    def __str__(self):\n",
    "        # if not self.children:\n",
    "            # return str(self.value)\n",
    "        return f\"{self.value}\"\n",
    "        # return f\"{self.value} {' '.join(str(child) for child in self.children)}\"\n",
    "\n",
    "    def calculate_individual_depth(self):\n",
    "        # This method checks for both the presence of a relationship and additional depth from hops\n",
    "        base_depth = 1 if self.hop_only == False else 0 # Start with a depth of 1 for the relationship itself\n",
    "        # Look for hop patterns, each '*' adds one to the depth\n",
    "        hop_matches = re.findall(r'\\*', self.value.value)\n",
    "        return base_depth + len(hop_matches)  # Add one additional depth for each hop pattern\n",
    "\n",
    "def get_depth(self):\n",
    "    # This now maps through children, checking if they are Relationship instances, and sums their depths\n",
    "    updated_depth = 0\n",
    "    for child in self.children:\n",
    "        if isinstance(child, Relationship):\n",
    "            updated_depth += child.calculate_individual_depth()\n",
    "        elif isinstance(child,Condition):\n",
    "            updated_depth += 1\n",
    "        else:\n",
    "            updated_depth = updated_depth\n",
    "    # updated_depth = sum(child.calculate_individual_depth() if isinstance(child, Relationship) else 0 for child in self.children)\n",
    "    return updated_depth\n",
    "TreeNode.get_depth = get_depth\n",
    "\n",
    "\n",
    "class Condition(TreeNode):\n",
    "    def __init__(self, value, children=None):\n",
    "        super().__init__(value)\n",
    "        self.children = children if children is not None else []\n",
    "\n",
    "    def __str__(self):\n",
    "        if not self.children:\n",
    "            return str(self.value)\n",
    "        return f\"{self.value} {' '.join(str(child) for child in self.children)}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QueryManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DepthManagerInUse:\n",
    "\n",
    "class TestQueryManager:\n",
    "    def __init__(self, dm):\n",
    "        self.root = TreeNode(\"ROOT\")  # All parts will be children of root\n",
    "        self.current_node = self.root  # Current node context for adding parts\n",
    "        self.node_labels = []\n",
    "        self.relationships = []\n",
    "        self.grouped_info = {}\n",
    "        self.usable_labels = set()\n",
    "        # self.usable_props = set()\n",
    "\n",
    "        \n",
    "        self.parts = []\n",
    "        self.selected_label_alias = {}\n",
    "\n",
    "        self.query_str = ''\n",
    "        \n",
    "        self.final_depth = 0\n",
    "        # self.depth_manager = DepthManager()\n",
    "        # self.id = self.generate_id()\n",
    "        self.observed_depths = set()\n",
    "        self.dm = dm\n",
    "       \n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_id(input_object):\n",
    "        if isinstance(input_object, TreeNode):\n",
    "            content = str(input_object)  \n",
    "        elif isinstance(input_object, str):\n",
    "            content = input_object\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported object type for ID generation\")\n",
    "        hash_digest = hashlib.sha256(content.encode('utf-8')).digest()\n",
    "        return base64.urlsafe_b64encode(hash_digest).decode('utf-8').rstrip('=')\n",
    "\n",
    "        \n",
    "\n",
    "    def reset_per_gen(self):\n",
    "        self.root = TreeNode(\"ROOT\")\n",
    "        self.current_node = self.root  # Reset the tree for new generation\n",
    "        self.parts = []\n",
    "        self.selected_label_alias = {}\n",
    "        self.usable_labels.clear()\n",
    "        # self.usable_props.clear()\n",
    "        self.final_depth = 0\n",
    "        self.query_str = ''\n",
    "\n",
    "    def import_grouped_info(self, input_group):\n",
    "        if input_group:\n",
    "            if type(input_group) is dict:\n",
    "                self.grouped_info = input_group\n",
    "                self.node_labels = list(self.grouped_info.keys())\n",
    "                # print(\"loaded node_labels:\",self.node_labels)\n",
    "            else:\n",
    "                print(\"input grouped info need to be dictionary type\")\n",
    "        else:\n",
    "            print(\"input grouped info cannot be empty\")\n",
    "    \n",
    "    def import_relationships(self, input_relationships):\n",
    "        if input_relationships:\n",
    "            self.relationships = input_relationships\n",
    "        else:\n",
    "            print(\"relationships cannot be empty\")\n",
    "    \n",
    "    def create_unique_alias(self, label):\n",
    "        \"\"\"Creates a unique alias for a node to prevent label overlap in queries.\"\"\"\n",
    "        base_alias = label.lower()\n",
    "        alias = base_alias\n",
    "        counter = 1\n",
    "        while alias in self.usable_labels:\n",
    "            alias = f\"{base_alias}{counter}\"\n",
    "            counter += 1\n",
    "        return alias\n",
    "    \n",
    "    def extract_alias_label(self, node_value_str):\n",
    "        # Pattern to capture both the alias and the node label\n",
    "        pattern = r\"^\\(([^:]+):([^ {]+)\"\n",
    "        match = re.search(pattern, node_value_str)\n",
    "\n",
    "        if match:\n",
    "            alias = match.group(1).strip()  # Get the alias, strip any whitespace\n",
    "            label = match.group(2).strip()  # Get the label, strip any whitespace\n",
    "            return alias, label\n",
    "        else:\n",
    "            # No match found, return None for both\n",
    "            return None, None\n",
    "\n",
    "    def add_node(self):\n",
    "        \"\"\"Adds a node with random properties selected from grouped_info.\"\"\"\n",
    "        # node_label = ''\n",
    "        if self.node_labels:\n",
    "            node_label = random.choice(self.node_labels) \n",
    "            possible_props = self.grouped_info[node_label]\n",
    "            property_label, properties_list = random.choice(list(possible_props.items()))\n",
    "            alias = self.create_unique_alias(node_label)\n",
    "            self.selected_label_alias[alias]=node_label\n",
    "\n",
    "            property_value = random.choice(properties_list)\n",
    "            properties_str = f'''{property_label}: \"{property_value}\"''' if possible_props else ''\n",
    "            node_value = f\"{node_label} {{{properties_str}}}\"\n",
    "\n",
    "\n",
    "            node = Node(f\"({alias}:{node_value})\")\n",
    "            # self.current_node.add_child(node)\n",
    "\n",
    "            # self.nodes.append(node)\n",
    "            self.usable_labels.add(alias)  # Store label for possible RETURN clause usage\n",
    "            return node \n",
    "        print(\"No node labels available. Please import grouped info first.\")\n",
    "        return None\n",
    "\n",
    "    def add_hop(self):\n",
    "        \"\"\"\n",
    "        Randomly generate hops as condition to relationship based on a customizable possibility;\n",
    "        the default possibility is 0.2\n",
    "        \"\"\"\n",
    "        current_depth = self.dm.depth\n",
    "        hop = random.randint(1,3) #TODO: see if this is reasonable\n",
    "        upper_hop = hop + random.randint(1,5)\n",
    "        exact_hop = f\"*{hop}\"\n",
    "        ceiling_hop = f\"*..{upper_hop}\"\n",
    "        floor_hop = f\"*{hop}..\"\n",
    "        interval_hop = f\"*{hop}..{upper_hop}\"\n",
    "        hop_choices = [exact_hop, ceiling_hop, floor_hop, interval_hop]\n",
    "        if current_depth < self.dm._max_depth: #random.random() > hop_p and \n",
    "            hop_choice = random.choice(hop_choices)\n",
    "            return hop_choice\n",
    "        else:\n",
    "            return ''\n",
    "\n",
    "\n",
    "\n",
    "    def add_relationship(self, bi_dir_p=0.3, rev_dir_p=0.5, hop_only_p=0.2, hop_p=0.2):\n",
    "        \"\"\" \n",
    "        Randomly generate a relationship between two nodes \n",
    "        bi_dir: probability of getting a bidirectional direction\n",
    "        rev_dir_p: probability of getting a reversed direction\n",
    "        hop_only_p: probability of getting only hops without specific relationships\n",
    "        hop_p: probability of getting hops in addition to a specific relationship\n",
    "        \"\"\"\n",
    "        current_depth = self.dm.depth\n",
    "        rel_type = random.choice(self.relationships)\n",
    "        if random.random() < bi_dir_p:\n",
    "            direction1 = \"-\"\n",
    "            direction2 = \"-\"\n",
    "        if current_depth>=3 and random.random() > rev_dir_p: \n",
    "            direction1 = \"<-\"\n",
    "            direction2 = \"-\"\n",
    "        else:\n",
    "            direction1 = \"-\" \n",
    "            direction2 = \"->\"\n",
    "        # if random.random() > hop_p:\n",
    "        \n",
    "        hop_result = Relationship(self.add_hop()) if random.random() > hop_p else ''\n",
    "        if random.random() > hop_only_p and hop_result:\n",
    "            relationship = Node(f\"{direction1} [{hop_result}] {direction2}\")\n",
    "            return Relationship(relationship, hop_only=True)\n",
    "        else:\n",
    "            relationship = Node(f\"{direction1} [:{rel_type}{hop_result}] {direction2}\")\n",
    "            return Relationship(relationship)\n",
    "        # self.current_node.add_child(relationship)\n",
    "        # return relationship\n",
    "        # return Relationship(relationship, hop_only)\n",
    "        \n",
    "    @depth_control\n",
    "    def add_condition(self, children_source, where_p=0.5, for_ea=False):\n",
    "        \"\"\"\n",
    "        Randomly generate WHERE clause based on a customizable possibility;\n",
    "        Will add to a random node as its child (no)\n",
    "        the default possibility where_p is 0.5;\n",
    "        currently only accepts and will only generate values that are str type properties\n",
    "        children_source: list of children. If from tree_population, should be tree.children, or other list type.\n",
    "        \"\"\"\n",
    "        if for_ea == True:\n",
    "            current_depth = 0 #to make sure as long as WHERE is found in previous query, it will be replaced only by chance and not depth\n",
    "        else:\n",
    "            current_depth = self.dm.depth\n",
    "        if random.random() > where_p and current_depth < self.dm._max_depth:\n",
    "            \n",
    "            np_children = np.array(children_source,dtype=object)\n",
    "            is_node = np.vectorize(lambda x: isinstance(x, Node))\n",
    "            # Apply the function to the numpy array\n",
    "            node_checks = is_node(np_children)\n",
    "            node_idx = np.where(node_checks)\n",
    "            node_children = np_children[node_idx]\n",
    "            random_node = random.choice(node_children)\n",
    "\n",
    "            alias, node_label = self.extract_alias_label(random_node.value)\n",
    "            # alias, node_label = random.choice(list(self.selected_label_alias.items()))\n",
    "            # print(alias, node_label)\n",
    "\n",
    "            # selected_node_label = random.choice(selected_node_labels)\n",
    "            possible_properties = self.grouped_info[node_label.strip()]\n",
    "            if possible_properties:\n",
    "                property_label, properties_list = random.choice(list(possible_properties.items()))\n",
    "                sample_prop_type = properties_list[0]\n",
    "                # value = random.randint(20, 50) if isinstance(sample_prop_type, int) else random.choice(properties_list) \n",
    "                value = random.choice(properties_list) #TODO: generalize to other data type\n",
    "            #TODO: customize the int part\n",
    "\n",
    "                operator = random.choice([\">\", \"<\", \"=\", \"<=\", \">=\"]) if isinstance(sample_prop_type, int) else '='\n",
    "                # grandchild = Condition(\"WHERE\", [Condition(f\"{alias}.{property_label} {operator} '{value}'\")])\n",
    "                # random_node.add_child(grandchild)\n",
    "                # print(\"Added WHERE clause to node\", random_node)\n",
    "                return Condition(\"WHERE\", [Clause(f'''{alias}.{property_label} {operator} \"{value}\"''', [])])\n",
    "            else:\n",
    "                raise ValueError(\"No available properties for the label selected:\", {node_label})\n",
    "        return \n",
    "    \n",
    "    @staticmethod\n",
    "    def is_relationship(part):\n",
    "        \"\"\"\n",
    "        Determine if the given part of a query is a relationship based on containing \"[]\"\n",
    "        Ensures that part is a string before checking.\n",
    "        \"\"\"\n",
    "        # pattern = re.compile(r'\\[(.*?)\\]')\n",
    "        trying = r\"-\\s*\\[:?([A-Za-z0-9_]+)?(\\*\\d*(\\.\\.\\d*)?)?\\]\\s*[-<>]?\"\n",
    "        # Ensure part is a string or bytes-like object\n",
    "        if isinstance(part,str):\n",
    "            # if pattern.search(part):\n",
    "            if re.search(trying, part):\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        else:   \n",
    "            print(\"input has to be str!\")\n",
    "            return None\n",
    "        \n",
    "\n",
    "    \n",
    "    def get_usable_labels(self):\n",
    "        return list(self.usable_labels)\n",
    "    \n",
    "    def add_return(self, return_num=None):\n",
    "\n",
    "        # print(\"selected_label_alias:\", self.selected_label_alias)\n",
    "\n",
    "        selected_alias = list(self.selected_label_alias.keys())\n",
    "        selected_node_labels = list(self.selected_label_alias.values())\n",
    "        \n",
    "        if return_num:\n",
    "            random_k = random.randint(1,return_num)\n",
    "        else:\n",
    "            random_k = random.randint(1,len(selected_alias))\n",
    "           \n",
    "        # print(\"selected_node_labels:\",selected_node_labels)\n",
    "        # choices = random.sample(self.usable_labels, random_k)\n",
    "        random_indices = random.sample(range(len(selected_node_labels)), random_k)\n",
    "        return_list = []\n",
    "        for i in random_indices:\n",
    "            current_alias = selected_alias[i]\n",
    "            current_label = selected_node_labels[i]\n",
    "            # print(\"type of current_label:\", repr(current_label), type(current_label))\n",
    "            # print(\"check if current_label is in self.node_labels\", repr(self.node_labels), current_label in self.node_labels)\n",
    "            current_possible_properties = self.grouped_info[str(current_label).strip()]\n",
    "            if current_possible_properties:\n",
    "                property_label = random.choice(list(current_possible_properties.keys()))\n",
    "                current_return = Clause(f\"{current_alias}.{property_label}\")\n",
    "                return_list.append(current_return)\n",
    "\n",
    "        return Clause(\"RETURN\", return_list)\n",
    "        # return None\n",
    "    \n",
    "    def parts_to_str(self):\n",
    "        \"\"\"\n",
    "        convert the generate query tree into query string with ; separation to get ready for querying the Memgraph client\n",
    "        \"\"\"\n",
    "        final_query_str = 'MATCH'\n",
    "        for part in self.parts:\n",
    "            final_query_str = final_query_str + ' ' + str(part)\n",
    "        final_query_str += ';'\n",
    "        return final_query_str\n",
    "\n",
    "\n",
    "    \n",
    "    ### FOR CROSSOVER RETURN ADJUSTMENT\n",
    "\n",
    "    def collect_alias_labels(self, tree):\n",
    "        \"\"\" Recursively collect labels from the tree that are usable in the RETURN clause. \"\"\"\n",
    "        if isinstance(tree, TreeNode) and isinstance(tree.children, list):\n",
    "            for child in tree.children:\n",
    "                # Extract label from the current node's value and add it to usable labels\n",
    "                child_value = str(child.value)\n",
    "                # print(child_value, type(child_value))\n",
    "                # label = self.extract_node_alias(child_value)\n",
    "                alias, label = self.extract_alias_label(child_value)\n",
    "                if alias and label:\n",
    "                    self.selected_label_alias[alias] = label\n",
    "                    # self.usable_labels.add(label)\n",
    "            # Recursively process each child\n",
    "                # self.collect_labels(child)\n",
    "                \n",
    "    \n",
    "    def adjust_return(self, tree):\n",
    "        \"\"\" Adjust the RETURN clause based on the labels collected from the tree. \"\"\"\n",
    "        if not isinstance(tree, TreeNode):\n",
    "            raise TypeError(\"Expected a tree that is TreeNode instance\")\n",
    "        # Clear existing labels and recollect from the new tree structure\n",
    "        # self.usable_labels.clear() \n",
    "        self.selected_label_alias = {}\n",
    "        self.collect_alias_labels(tree)\n",
    "        \n",
    "        if self.selected_label_alias:\n",
    "            # random_k = random.randint(1, len(self.usable_labels))\n",
    "            # choices = random.sample(self.usable_labels, random_k)\n",
    "            new_return = self.add_return()\n",
    "            \n",
    "            if tree.children and isinstance(tree.children[-1], TreeNode) and \"RETURN\" in str(tree.children[-1].value):\n",
    "                tree.children[-1] = new_return  # Replace the last child with the new RETURN clause\n",
    "            else:\n",
    "                tree.add_child(new_return)  # Add new if no RETURN exists\n",
    "            # print(\"updated return:\",tree.children[-1])\n",
    "            return tree\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    \n",
    "    def generate_query(self, flag=True, return_num=None, part_num=None, hop_p=0.5, where_p=0.5):\n",
    "        self.reset_per_gen()\n",
    "        self.dm.reset_depth()\n",
    "        def alternate_functions(flag):\n",
    "            if flag:\n",
    "                return self.add_node(), not flag\n",
    "            else:\n",
    "                return self.add_relationship(hop_p), not flag\n",
    "        if part_num is None:\n",
    "            part_num = random.randint(1, self.dm._max_depth-2)\n",
    "        # Keep adding nodes and relationships while depth is within limit\n",
    "        for _ in range(part_num+1):\n",
    "            part, flag = alternate_functions(flag)\n",
    "            if part is None:\n",
    "                break\n",
    "            self.parts.append(part)\n",
    "            # self.current_node.add_child(TreeNode(part))\n",
    "            self.current_node.add_child(part)\n",
    "        if self.parts and self.is_relationship(str(self.parts[-1]))==True: #ensure the input part is in string format\n",
    "            final_node = self.add_node()  # Generate a final node\n",
    "            if final_node:\n",
    "                self.parts.append(final_node)\n",
    "                # print(\"final_node added:\", final_node)\n",
    "                # self.current_node.add_child(TreeNode(final_node))\n",
    "                self.current_node.add_child(final_node)\n",
    "        # Optionally add a WHERE clause to a random node if depth is still under max_depth\n",
    "        condition = self.add_condition(self.current_node.children, where_p) \n",
    "        if condition:\n",
    "            self.parts.append(condition)\n",
    "            self.current_node.add_child(condition)\n",
    "           \n",
    "        # Add RETURN clause \n",
    "        ret = self.add_return(return_num)\n",
    "        if ret:\n",
    "            self.parts.append(ret)\n",
    "            self.current_node.add_child(ret)\n",
    "\n",
    "        self.query_str = self.parts_to_str()\n",
    "        self.current_node.depth = self.dm.depth\n",
    "        return self.current_node, self.query_str #return the treenode type and string type of query\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next building workflow:\n",
    "- function to convert resulting tree nodes into working queries (done)\n",
    "- write them into test_queries.txt file (done)\n",
    "- then apply the bash file & convertion function to run the queries & store as readable csv (done)\n",
    "- write a basic scoring/evaluation function on depth (done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitness function basics buildup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EA try\n",
    "- initialize population with cutomizable size and max depth\n",
    "- all queried through mgconsole and only maintained and added to the population if it returns result\n",
    "- fitness function to evaluate initial population\n",
    "\n",
    "Questions\n",
    "- Do I need to make sure all initial queries must work?\n",
    "- generally how customizable we want EA to be? and how do we usually achieve such flexibility in adjusting parameters? through functions or...?\n",
    "- is the current method of initializing population \"wise\" enough?\n",
    "- \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mgclient\n",
    "import threading\n",
    "import concurrent.futures\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "class DatabaseConn:\n",
    "    def __init__(self, host, port):\n",
    "        self.host = host\n",
    "        self.port = port\n",
    "        # Create a thread-local data storage\n",
    "        self.local = threading.local()\n",
    "\n",
    "    def get_connection(self):\n",
    "        \"\"\"\n",
    "        Retrieve or establish a database connection for the current thread.\n",
    "        \"\"\"\n",
    "        if not hasattr(self.local, 'conn'):\n",
    "            self.local.conn = mgclient.connect(host=self.host, port=self.port)\n",
    "            self.local.conn.autocommit = True\n",
    "        return self.local.conn\n",
    "\n",
    "    def execute_query(self, query):\n",
    "        \"\"\"\n",
    "        Execute a given query using a thread-specific database connection.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            conn = self.get_connection()\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute(query)\n",
    "            results = cursor.fetchall()\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to execute query {query}: {str(e)}\")\n",
    "            return e\n",
    "        finally:\n",
    "            if 'cursor' in locals():\n",
    "                cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import mgclient\n",
    "import threading\n",
    "import concurrent.futures\n",
    "# from concurrent.futures import ThreadPoolExecutor\n",
    "# conn = mgclient.connect(host='127.0.0.1', port=7687)\n",
    "# cursor = conn.cursor()\n",
    "\n",
    "class EvolutionaryAlgorithm:\n",
    "    def __init__(self, qm, depth_manager, population_size, max_depth, max_generation, mut_rate=0.05):\n",
    "        self.population_size = population_size\n",
    "        self.max_depth = max_depth\n",
    "        self.tree_population = []\n",
    "        self.str_population = []\n",
    "        self.fitness_scores = {}\n",
    "        self.query_ids = {}\n",
    "        self.observed_depths = set() #TODO: clear this set after each initializED population\n",
    "        self.qm = qm\n",
    "        self.depth_manager = depth_manager\n",
    "        self.depth_manager.set_max_depth(self.max_depth)\n",
    "        # self.database_conn = database_conn\n",
    "        self.generation = 0\n",
    "        self.max_generation = max_generation\n",
    "        self.mut_rate = mut_rate\n",
    "\n",
    "        # self.host = host\n",
    "        # self.port = port\n",
    "\n",
    "        self.valid_queries = [] #store queries with score = 2 across all generations\n",
    "\n",
    "            \n",
    "    def population_to_query_list(self, population):\n",
    "        return [tree.to_querystr() for tree in population]\n",
    "\n",
    "        \n",
    "    def evaluate_population(self, max_workers=5):\n",
    "        \"\"\" Evaluates the entire population and updates fitness scores. \"\"\"\n",
    "        self.generation += 1 \n",
    "        # query_list = self.population_to_query_list(self.tree_population)\n",
    "        successful_indices, failed_indices = self.run_queries_multithreaded(tree_population=self.tree_population, max_workers=max_workers)\n",
    "        for index, tree in enumerate(self.tree_population):\n",
    "            if index in successful_indices:\n",
    "                tree.score += 1\n",
    "                self.valid_queries.append(tree)\n",
    "            elif index in failed_indices:\n",
    "                tree.score -= 1\n",
    "\n",
    "\n",
    "    def initialize_population(self): #TODO: change to generate till size is satisfied\n",
    "        \"\"\" Initializes the population with random depth queries. \"\"\"\n",
    "        for _ in range(self.population_size):\n",
    "            # self.depth_manager.set_max_depth(self.max_depth)\n",
    "            tree, query = self.qm.generate_query()\n",
    "            self.tree_population.append(tree)\n",
    "            self.str_population.append(query)\n",
    "            self.qm.reset_per_gen()\n",
    "\n",
    "    def is_valid_query(self, cursor, query):\n",
    "        \"\"\" Execute the query and check if it returns any results. \"\"\"\n",
    "        try:\n",
    "            cursor.execute(query)\n",
    "            results = cursor.fetchall()\n",
    "            return len(results) > 0  # Return True if there are results\n",
    "        except Exception as e:\n",
    "            print(f\"Query failed: {e}\")\n",
    "            return False  # Query failed or returned no results\n",
    "\n",
    "\n",
    "    def tournament_parent_selection(self, k: int = None):\n",
    "        \"\"\"\n",
    "        Selects the fittest individual from a random sample of the population using a tournament selection approach.\n",
    "\n",
    "        Parameters:\n",
    "        - k (int, optional): The number of individuals to sample for the tournament. Defaults to half the population size.\n",
    "\n",
    "        Returns:\n",
    "        - The fittest individual from the sampled tournament.\n",
    "        \"\"\"\n",
    "        if k is None:\n",
    "            k = self.population_size // 2\n",
    "        if k > len(self.tree_population):\n",
    "            raise ValueError(\"Sample size k cannot be larger than the population size.\")\n",
    "        tournament = random.sample(self.tree_population, k)\n",
    "        fittest = max(tournament, key=lambda individual: self.fitness_scores[self.query_ids[individual]])\n",
    "        return fittest\n",
    "\n",
    "    def select_parents(self, num_pairs, k):\n",
    "        \"\"\"\n",
    "        Selects pairs of parents for reproduction, ensuring parents within a pair would not repeat.\n",
    "\n",
    "        Parameters:\n",
    "        - num_pairs (int): The number of parent pairs to select.\n",
    "\n",
    "        Returns:\n",
    "        - List of tuples, where each tuple contains two parent individuals.\n",
    "        \"\"\"\n",
    "        if num_pairs <= 0:\n",
    "            raise ValueError(\"num_pairs must be a positive integer\")\n",
    "        elif num_pairs * 2 > len(self.tree_population):\n",
    "            raise ValueError(\"Insufficient population to select the requested number of unique pairs\")\n",
    "\n",
    "        parents = []\n",
    "        parent_pairs=[]\n",
    "        # selected_individuals = set()  # Keep track of selected individuals\n",
    "\n",
    "        while len(parents) < num_pairs * 2: #and len(selected_individuals) < len(self.tree_population):\n",
    "            parent1 = self.tournament_parent_selection(k)\n",
    "            parent2 = self.tournament_parent_selection(k)\n",
    "            while parent1 == parent2:\n",
    "                parent2 = self.tournament_parent_selection(k)\n",
    "            parent_pair = (parent1,parent2)\n",
    "            parents.append(parent1)\n",
    "            parents.append(parent2)\n",
    "            parent_pairs.append(parent_pair)\n",
    "        return parent_pairs\n",
    "\n",
    "    # JUL25: implementing mutation\n",
    "    def mutate_query(self, tree):\n",
    "        \"\"\"\n",
    "        Randomly mutate either choice based on mut_rate probability below:\n",
    "        - node label of the query\n",
    "        - WHERE clause\n",
    "        And the returned tree query will also have an adjusted RETURN clause\n",
    "        \"\"\"\n",
    "        mutations = ['node_label','condition']\n",
    "        mut_type = random.choice(mutations)\n",
    "        if type(tree)==TreeNode:\n",
    "            if mut_type == 'condition':\n",
    "                # mutated_condition = self.qm.add_condition(for_ea=True)\n",
    "                for index, element in enumerate(tree.children):\n",
    "                    if 'WHERE' in str(element.value):\n",
    "                        children_source = tree.children[:index] + tree.children[index+1:]\n",
    "                        tree.children[index] = self.qm.add_condition(children_source, for_ea=True)\n",
    "                    else:\n",
    "                        mut_type = 'node_label' #ensure mutation takes place for selected queries without WHERE clause\n",
    "            if mut_type == 'node_label':\n",
    "                # mutated_node = self.qm.add_node()\n",
    "                indices = []\n",
    "                for index, element in enumerate(tree.children):\n",
    "                    if type(element)==Node:\n",
    "                        indices.append(index)\n",
    "                ind = random.choice(indices)\n",
    "                tree.children[ind] = self.qm.add_node()\n",
    "            tree = self.qm.adjust_return(tree)\n",
    "            return tree\n",
    "        else:\n",
    "            raise ValueError(\"The input tree query has to be TreeNode type!\")\n",
    "\n",
    "    def mutation(self):\n",
    "        num_to_mutate = int(len(self.tree_population) * self.mut_rate)\n",
    "        for _ in range(num_to_mutate):\n",
    "            # Randomly pick an individual to mutate\n",
    "            individual_index = random.randint(0, len(self.tree_population) - 1)\n",
    "            print(\"mutated index:\", individual_index)\n",
    "            # Perform mutation on this individual\n",
    "            self.tree_population[individual_index] = self.mutate_query(self.tree_population[individual_index])\n",
    "        return self.tree_population  \n",
    "\n",
    "    def swap(self, tree1, tree2):\n",
    "        if not tree1.children or not tree2.children:\n",
    "            print(\"One of the trees does not have children to perform swapping.\")\n",
    "            return\n",
    "        # Select random subtree indices from both trees\n",
    "        index1 = random.randint(0, len(tree1.children) - 1)\n",
    "        index2 = random.randint(0, len(tree2.children) - 1)\n",
    "\n",
    "        tree1_swap = copy.deepcopy(tree1)\n",
    "        tree2_swap = copy.deepcopy(tree2)\n",
    "\n",
    "        # Swap the subtrees\n",
    "        tree1_swap.children[index1], tree2_swap.children[index2] = \\\n",
    "            tree2_swap.children[index2], tree1_swap.children[index1]\n",
    "        print(\"Swapping completed.\")\n",
    "        return tree1_swap, tree2_swap\n",
    "\n",
    "    def one_point_crossover(self, tree1, tree2):\n",
    "        if not tree1.children or not tree2.children:\n",
    "            print(\"One of the trees does not have children to perform crossover.\")\n",
    "            return\n",
    "        #get indices that are not relationships as possible crossover point\n",
    "        node_indices1 = [index for index, child in enumerate(tree1.children) if type(child)!= Relationship]\n",
    "        node_indices2 = [index for index, child in enumerate(tree2.children) if type(child)!= Relationship]\n",
    "\n",
    "        #check node existence\n",
    "        if not node_indices1 or not node_indices2:\n",
    "            print(\"No nodes available for crossover in one or both trees.\")\n",
    "            return\n",
    "\n",
    "        #select random node indices from the filtered lists\n",
    "        index1 = random.choice(node_indices1[:-1])\n",
    "        index2 = random.choice(node_indices2[:-1])\n",
    "\n",
    "        #exchange the subtrees at these indices\n",
    "        tree1_crossover = copy.deepcopy(tree1)\n",
    "        tree2_crossover = copy.deepcopy(tree2)\n",
    "\n",
    "        tree1_crossover.children[index1:], tree2_crossover.children[index2:] = \\\n",
    "            tree2_crossover.children[index2:], tree1_crossover.children[index1:]\n",
    "        \n",
    "        #adjust RETURN clause based on exchanged trees\n",
    "        tree1_crossover = self.qm.adjust_return(tree1_crossover)\n",
    "        tree2_crossover = self.qm.adjust_return(tree2_crossover)\n",
    "\n",
    "        print(\"Crossover and Return clause adjustment completed.\")\n",
    "        return tree1_crossover, tree2_crossover\n",
    "    \n",
    "\n",
    "    def output_top_queries(self, top_n):\n",
    "        \"\"\"\n",
    "        Outputs the top N queries from the current population based on fitness scores,\n",
    "        considering depth diversity and query diversity.\n",
    "        \n",
    "        Parameters:\n",
    "        - top_n (int): Number of top queries to return.\n",
    "        \n",
    "        Returns:\n",
    "        - list: Top N queries as per the defined criteria.\n",
    "        \"\"\"\n",
    "        # Sort the population based on fitness scores\n",
    "\n",
    "        sorted_population = sorted(self.tree_population, key=lambda x: self.fitness_scores[self.query_ids[x]], reverse=True)\n",
    "        # Implement logic to ensure diversity if needed, example placeholder:\n",
    "        # diverse_population = self.ensure_diversity(sorted_population, top_n)\n",
    "        top_queries_with_scores = [(query, self.fitness_scores[self.query_ids[query]]) for query in sorted_population[:top_n]]\n",
    "        return top_queries_with_scores\n",
    "        # return diverse_population[:top_n]\n",
    "\n",
    "\n",
    "    def reset_ea(self):\n",
    "        # self.depth_manager.reset_depth_record()\n",
    "        self.observed_depths = set()\n",
    "        self.tree_population = []\n",
    "        self.str_population = []\n",
    "        self.fitness_scores = {}\n",
    "        self.query_ids = {}\n",
    "        self.generation=0\n",
    "\n",
    "\n",
    "def execute_query(query):\n",
    "    # Create thread-local storage for database connections\n",
    "    if not hasattr(execute_query, \"conn\"):\n",
    "        execute_query.conn = mgclient.connect(host='127.0.0.1', port=7687)\n",
    "        execute_query.conn.autocommit = True\n",
    "    try:\n",
    "        cursor = execute_query.conn.cursor()\n",
    "        cursor.execute(query)\n",
    "        results = cursor.fetchall()\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to execute query {query}: {str(e)}\")\n",
    "        return None\n",
    "    finally:\n",
    "        cursor.close()\n",
    "\n",
    "def run_queries_multithreaded(query_list, max_workers=5):\n",
    "    # Create a ThreadPoolExecutor to manage multiple threads\n",
    "    with threading.Thread.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Map futures to their corresponding query indices\n",
    "        future_to_index = {executor.submit(execute_query, query): i for i, query in enumerate(query_list)}\n",
    "\n",
    "        # Create a list to store the indices of successful queries\n",
    "        successful_indices = []\n",
    "        failed_indices = []\n",
    "\n",
    "        # Collect results as they complete\n",
    "        for future in concurrent.futures.as_completed(future_to_index):\n",
    "            index = future_to_index[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "                if result:\n",
    "                    print(f\"Query at index {index} returned results.\")\n",
    "                    successful_indices.append(index)\n",
    "                else:\n",
    "                    print(f\"Query at index {index} returned no results.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Query execution failed for index {index}: {str(e)}\")\n",
    "                failed_indices.append(index)\n",
    "\n",
    "        return successful_indices, failed_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mgclient\n",
    "from multiprocessing import Pool, current_process\n",
    "import os\n",
    "from mgclient_multiproc import execute_query_withtimeout\n",
    "\n",
    "# Function to execute queries using multiprocessing\n",
    "def execute_queries_in_parallel(query_list):\n",
    "    with Pool(processes=os.cpu_count()) as pool:\n",
    "        results = pool.map(execute_query_withtimeout, query_list)\n",
    "    # Filter None results (failed or no-result queries)\n",
    "    return [result for result in results if result is not None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# databaseTest = DatabaseConn(host='127.0.0.1',port=7687)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation: 0\n"
     ]
    }
   ],
   "source": [
    "dmTest = TestDepthManager.getInstance()\n",
    "qmTest = TestQueryManager(dm=dmTest)\n",
    "qmTest.import_grouped_info(alzkb_nested_dict)  \n",
    "qmTest.import_relationships(relationships)  \n",
    "\n",
    "\n",
    "ea = EvolutionaryAlgorithm(qm=qmTest, depth_manager=dmTest, population_size=50,max_depth=4, max_generation=3)\n",
    "# ea.test_initialize_population() \n",
    "print(\"generation:\",ea.generation)\n",
    "ea.initialize_population() \n",
    "# for index, query in enumerate(ea.str_population):\n",
    "#     print(index, query)\n",
    "# ea.mutation()\n",
    "query_list = ea.population_to_query_list(ea.tree_population)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Memgraph and Execute Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First half: connect to memgraph console mgconsole\n",
    "- Need docker installation following notion notes; paste them here later\n",
    "- But the bash file will handle the querying so no actual terminal operation needed from the user side"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second half: query execution\n",
    "- because of the csv format glich of memgraph, the bash file **single_query_run.s**h** first execute each individual generated query in the **test_queries.txt**\n",
    "- the results are stored in the outputs folder in fake csv format\n",
    "- a function converting and aggregating the results into readable, interpretable csv is then applied to obtain a **aggregated_results.csv**\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1. write 10 queries into a txt file\\n2. execute\\n3. write another 10 queries into txt, execute ... till all are executed -- DONE\\n4. grep results from aggregated csv with indices. --DONE\\n    if \"no_result\", no change;\\n    if error, corresponding query.score = 0\\n    if return results, corresponding query.score = 2 \\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "1. write 10 queries into a txt file\n",
    "2. execute\n",
    "3. write another 10 queries into txt, execute ... till all are executed -- DONE\n",
    "4. grep results from aggregated csv with indices. --DONE\n",
    "    if \"no_result\", no change;\n",
    "    if error, corresponding query.score = 0\n",
    "    if return results, corresponding query.score = 2 \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./test_batch/batch2.txt', './test_batch/batch1.txt', './test_batch/batch0.txt']\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import subprocess\n",
    "from multiprocessing import Pool\n",
    "from test_multiprocessing import create_batch, process_batch, aggregate_text_files, merge_csv_files, get_indices\n",
    "########################################\n",
    "#TESTING WITH MULTIPROCESSING\n",
    "# def process_batches(batch_folder):\n",
    "#     # Find all batch files\n",
    "#     batch_files = [os.path.join(batch_folder, f) for f in os.listdir(batch_folder) if f.startswith('batch') and f.endswith('.txt')]\n",
    "#     # Ensure the script is executable\n",
    "#     subprocess.run(['chmod', '+x', 'test_query_run.sh'])\n",
    "    \n",
    "#     # Process each batch in parallel\n",
    "#     with Pool(processes=os.cpu_count()) as pool:\n",
    "#         pool.map(execute_script, batch_files)\n",
    "\n",
    "#     # Aggregate results\n",
    "#     folder_path = './outputs'\n",
    "#     output_file = './aggregates/results.csv'\n",
    "#     aggregate_text_files(folder_path, output_file)\n",
    "\n",
    "# # Merge CSV files if needed\n",
    "# merge_csv_files('aggregates', 'multiproc_merged_output.csv')\n",
    "\n",
    "# # Get indices of successful and failed queries\n",
    "# get_indices('multiproc_merged_output.csv')\n",
    "# process_batch('./input_batch/batch0.txt')\n",
    "\n",
    "batch_folder = './test_batch'\n",
    "batch_files = [os.path.join(batch_folder, f) for f in os.listdir(batch_folder) if f.startswith('batch')]\n",
    "print(batch_files)\n",
    "# with Pool(processes=2) as pool:\n",
    "    # batch_files = [os.path.join(batch_folder, f) for f in os.listdir(batch_folder) if f.startswith('batch')]\n",
    "    # pool.map(process_batch, batch_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running bash file\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mprocess_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_files\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/KGCypherGenerator/test_multiprocessing.py:198\u001b[0m, in \u001b[0;36mprocess_batch\u001b[0;34m(batch_file)\u001b[0m\n\u001b[1;32m    196\u001b[0m source_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./outputs\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 198\u001b[0m     clean_directory(source_folder)\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrunning bash file\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    200\u001b[0m     subprocess\u001b[38;5;241m.\u001b[39mrun([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./test_query_run.sh\u001b[39m\u001b[38;5;124m'\u001b[39m, batch_file], check\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/subprocess.py:507\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Popen(\u001b[38;5;241m*\u001b[39mpopenargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 507\u001b[0m         stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommunicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    508\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutExpired \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    509\u001b[0m         process\u001b[38;5;241m.\u001b[39mkill()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/subprocess.py:1126\u001b[0m, in \u001b[0;36mPopen.communicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1124\u001b[0m         stderr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1125\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1126\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/subprocess.py:1189\u001b[0m, in \u001b[0;36mPopen.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1187\u001b[0m     endtime \u001b[38;5;241m=\u001b[39m _time() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1191\u001b[0m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m     \u001b[38;5;66;03m# The first keyboard interrupt waits briefly for the child to\u001b[39;00m\n\u001b[1;32m   1193\u001b[0m     \u001b[38;5;66;03m# exit under the common assumption that it also received the ^C\u001b[39;00m\n\u001b[1;32m   1194\u001b[0m     \u001b[38;5;66;03m# generated SIGINT and will exit rapidly.\u001b[39;00m\n\u001b[1;32m   1195\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/subprocess.py:1917\u001b[0m, in \u001b[0;36mPopen._wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1915\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1916\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# Another thread waited.\u001b[39;00m\n\u001b[0;32m-> 1917\u001b[0m (pid, sts) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1918\u001b[0m \u001b[38;5;66;03m# Check the pid and loop as waitpid has been known to\u001b[39;00m\n\u001b[1;32m   1919\u001b[0m \u001b[38;5;66;03m# return 0 even without WNOHANG in odd situations.\u001b[39;00m\n\u001b[1;32m   1920\u001b[0m \u001b[38;5;66;03m# http://bugs.python.org/issue14396.\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pid \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/subprocess.py:1875\u001b[0m, in \u001b[0;36mPopen._try_wait\u001b[0;34m(self, wait_flags)\u001b[0m\n\u001b[1;32m   1873\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"All callers to this function MUST hold self._waitpid_lock.\"\"\"\u001b[39;00m\n\u001b[1;32m   1874\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1875\u001b[0m     (pid, sts) \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwaitpid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait_flags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1876\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mChildProcessError\u001b[39;00m:\n\u001b[1;32m   1877\u001b[0m     \u001b[38;5;66;03m# This happens if SIGCLD is set to be ignored or waiting\u001b[39;00m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;66;03m# for child processes has otherwise been disabled for our\u001b[39;00m\n\u001b[1;32m   1879\u001b[0m     \u001b[38;5;66;03m# process.  This child is dead, we can't get the status.\u001b[39;00m\n\u001b[1;32m   1880\u001b[0m     pid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "process_batch(batch_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod +x save_only_working.sh\n",
    "!./save_only_working.sh ./input_batch/batch0.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation: 0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'EvolutionaryAlgorithm' object has no attribute 'run_queries_multithreaded'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration:\u001b[39m\u001b[38;5;124m\"\u001b[39m,ea\u001b[38;5;241m.\u001b[39mgeneration)\n\u001b[1;32m      4\u001b[0m ea\u001b[38;5;241m.\u001b[39minitialize_population() \n\u001b[0;32m----> 5\u001b[0m \u001b[43mea\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_population\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration:\u001b[39m\u001b[38;5;124m\"\u001b[39m,ea\u001b[38;5;241m.\u001b[39mgeneration)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitness Scores:\u001b[39m\u001b[38;5;124m\"\u001b[39m, ea\u001b[38;5;241m.\u001b[39mfitness_scores)\n",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36mEvolutionaryAlgorithm.evaluate_population\u001b[0;34m(self, max_workers)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# query_list = self.population_to_query_list(self.tree_population)\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m successful_indices, failed_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_queries_multithreaded\u001b[49m(tree_population\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree_population, max_workers\u001b[38;5;241m=\u001b[39mmax_workers)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, tree \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree_population):\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m successful_indices:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'EvolutionaryAlgorithm' object has no attribute 'run_queries_multithreaded'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed query: MATCH (gene:Gene {commonName: \"-\"}) - [*3..] -> (pathway:Pathway {commonName: \"Cargo trafficking to the periciliary membrane\"}) WHERE gene.geneSymbol = \"LOC129999684\" RETURN gene.geneSymbol\n",
      "Client received query exception: Transaction was asked to abort because of transaction timeout.\n"
     ]
    }
   ],
   "source": [
    "ea = EvolutionaryAlgorithm(qm=qmTest, depth_manager=dmTest, population_size=10,max_depth=10, max_generation=3)\n",
    "# print(ea.depth_manager._max_depth)\n",
    "print(\"generation:\",ea.generation)\n",
    "ea.initialize_population() \n",
    "ea.evaluate_population()\n",
    "print(\"generation:\",ea.generation)\n",
    "print(\"Fitness Scores:\", ea.fitness_scores)\n",
    "\n",
    "parent_pairs = ea.select_parents(num_pairs=5,k=3) #assume 5 pairs of parents --> maintain 10 total in next gen\n",
    "offspring_list = []\n",
    "for parent1, parent2 in parent_pairs:\n",
    "    # print(\"parent:\",parent1.children[-1])\n",
    "    # print(parent2.children[-1])\n",
    "    offspring1, offspring2 = ea.one_point_crossover(parent1, parent2)\n",
    "    # print(\"offspring:\", offspring1.children[-1])\n",
    "    # print(offspring2.children[-1])\n",
    "    offspring_list.extend([offspring1, offspring2])\n",
    "\n",
    "ea.tree_population = offspring_list\n",
    "ea.evaluate_population()\n",
    "print(\"generation:\",ea.generation)\n",
    "print(\"Fitness Scores:\", ea.fitness_scores)\n",
    "# ea.reset_ea()\n",
    "\n",
    "\n",
    "# print(ea.depth_manager.depth_record)\n",
    "result = ea.output_top_queries(top_n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_path = 'output_queries.txt'\n",
    "with open(output_file_path, 'w') as file:\n",
    "    for tuple in result:\n",
    "        tree = tuple[0]\n",
    "        querystr = tree.to_querystr()\n",
    "        file.write(querystr + '\\n')\n",
    "print(\"All queries have been written to\", output_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod +x single_query_run.sh\n",
    "!./single_query_run.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing performance from Claude suggestions\n",
    "- asynchronous processing\n",
    "- caching?\n",
    "- execution time limits (timeout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import mgclient\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "class ConnectionPool:\n",
    "    def __init__(self, host, port, max_connections=10):\n",
    "        self.host = host\n",
    "        self.port = port\n",
    "        self.max_connections = max_connections\n",
    "        self.pool = asyncio.Queue()\n",
    "        self.active_connections = 0\n",
    "        self.lock = asyncio.Lock()\n",
    "        self.executor = ThreadPoolExecutor(max_workers=max_connections)\n",
    "\n",
    "    async def get_connection(self):\n",
    "        async with self.lock:\n",
    "            if self.pool.empty() and self.active_connections < self.max_connections:\n",
    "                # Create a new connection if we haven't reached the max\n",
    "                loop = asyncio.get_event_loop()\n",
    "                connection = await loop.run_in_executor(self.executor, self._create_connection)\n",
    "                self.active_connections += 1\n",
    "            else:\n",
    "                # Wait for an available connection from the pool\n",
    "                connection = await self.pool.get()\n",
    "        return connection\n",
    "\n",
    "    def _create_connection(self):\n",
    "        return mgclient.connect(host=self.host, port=self.port)\n",
    "\n",
    "    async def return_connection(self, connection):\n",
    "        await self.pool.put(connection)\n",
    "\n",
    "    async def close_all(self):\n",
    "        while not self.pool.empty():\n",
    "            connection = await self.pool.get()\n",
    "            await asyncio.get_event_loop().run_in_executor(self.executor, connection.close)\n",
    "        self.active_connections = 0\n",
    "        self.executor.shutdown()\n",
    "\n",
    "# Create a global connection pool\n",
    "pool = ConnectionPool(host='127.0.0.1', port=7687, max_connections=10)\n",
    "\n",
    "async def get_connection():\n",
    "    return await pool.get_connection()\n",
    "\n",
    "async def return_connection(connection):\n",
    "    await pool.return_connection(connection)\n",
    "\n",
    "# In your main function or wherever you set up your application:\n",
    "async def setup():\n",
    "    global pool\n",
    "    pool = ConnectionPool(host='127.0.0.1', port=7687, max_connections=10)\n",
    "\n",
    "# Don't forget to close connections when shutting down:\n",
    "async def cleanup():\n",
    "    await pool.close_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import concurrent.futures\n",
    "from cachetools import TTLCache\n",
    "\n",
    "# Initialize cache\n",
    "cache = TTLCache(maxsize=1000, ttl=3600)\n",
    "\n",
    "# Connection pool\n",
    "connection_pool = []\n",
    "\n",
    "async def execute_query(query):\n",
    "    if query in cache:\n",
    "        return cache[query]\n",
    "    \n",
    "    connection = await get_connection()\n",
    "    cursor = connection.cursor()\n",
    "            #     conn = self.get_connection()\n",
    "            # cursor = conn.cursor()\n",
    "            # cursor.execute(query)\n",
    "            # results = cursor.fetchall()\n",
    "    try:\n",
    "        loop = asyncio.get_event_loop()\n",
    "        print(\"waiting for result\")\n",
    "        result = await loop.run_in_executor(None, cursor.execute, query)\n",
    "        print(\"got the result\")\n",
    "        cache[query] = result\n",
    "        return result\n",
    "    finally:\n",
    "        cursor.close()\n",
    "        await return_connection(connection)\n",
    "\n",
    "async def batch_execute_queries(queries, batch_size):\n",
    "    total_queries = len(queries)\n",
    "    results = []\n",
    "    for i in range(0, total_queries, batch_size):\n",
    "        batch = queries[i:i+batch_size]\n",
    "        batch_tasks = [execute_query(query) for query in batch]\n",
    "        batch_results = await asyncio.gather(*batch_tasks)\n",
    "        results.extend(batch_results)\n",
    "    return results\n",
    "\n",
    "# def preprocess_queries(queries):\n",
    "#     # Implement logic to filter out obviously invalid queries\n",
    "#     return [q for q in queries if EvolutionaryAlgorithm.is_valid_query(q)]\n",
    "\n",
    "async def evolve_queries(initial_population, generations, batch_size):\n",
    "    print(\"evolving...\")\n",
    "    population = initial_population\n",
    "    for _ in range(generations):\n",
    "        # Preprocess to filter out obviously invalid queries\n",
    "        # population = preprocess_queries(population)\n",
    "        \n",
    "        # Execute queries in batches\n",
    "        results = await batch_execute_queries(population, batch_size)\n",
    "        \n",
    "        # Select best queries based on results\n",
    "        # best_queries = select_best_queries(population, results)\n",
    "        \n",
    "        # Generate new population\n",
    "        # population = generate_new_population(best_queries)\n",
    "\n",
    "    \n",
    "    # return population\n",
    "    return results\n",
    "\n",
    "# Run the evolutionary process\n",
    "def run_async_main():\n",
    "    asyncio.run(async_main())\n",
    "\n",
    "async def async_main():\n",
    "    await setup()  # Set up the connection pool\n",
    "    try:\n",
    "        ea = EvolutionaryAlgorithm(qm=qmTest, depth_manager=dmTest, population_size=10, max_depth=10, max_generation=3)\n",
    "        ea.initialize_population() \n",
    "        initial_population = ea.population_to_query_list(ea.tree_population)\n",
    "        final_population = await evolve_queries(initial_population, generations=1, batch_size=10)  # Add batch_size parameter\n",
    "        print(\"Final selected queries:\", final_population)\n",
    "    finally:\n",
    "        await cleanup()  # Close all connections\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     run_async_main()\n",
    "await async_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
